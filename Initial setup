{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doodle-Med/Python-based-framework-for-training-AI-models-using-probabilistic-embeddings-and-pairwise-data/blob/main/Cell_1_Imports_and_Initial_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1"
      ],
      "metadata": {
        "id": "l2Hhk28-R3E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: Title, Abstract, Installs, Imports, Setup ---\n",
        "\n",
        "# # Unified Multi-Geometry Latent World Model (Pairwise Training & Dynamic Interactive Datasets)\n",
        "#\n",
        "# **Abstract:** This notebook implements and demonstrates an advanced probabilistic world model operating\n",
        "# in a multi-geometry latent space. It's designed for flexible pairwise training across various modalities\n",
        "# (images, text, audio, etc.) using an interactive GUI (via ipywidgets) for dynamic configuration of datasets.\n",
        "# The model embeds multi-modal data into a product manifold P = M_1 x ... x M_k, where each M_i can be\n",
        "# Euclidean, Spherical, Hyperbolic, Complex, Quaternion, or a Probability Simplex.\n",
        "# Modality-specific encoders map inputs to parameters of distributions (e.g., Gaussian N(μ_i, Σ_i))\n",
        "# within these geometric spaces. The training loop alternates between configured modality pairs,\n",
        "# aligning them using contrastive losses based on geometric distances (e.g., 2-Wasserstein)\n",
        "# within the shared mixture space. The system includes a flexible data loading pipeline, modular\n",
        "# geometry heads, encoders, and a training framework supporting various regularization techniques\n",
        "# like entropy maximization for mixture weights and Optimal Transport.\n",
        "\n",
        "import time # For timestamping print statements\n",
        "CURRENT_TIME_START_CELL1 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL1))}] Cell 1: Initializing Environment & Dependencies...\")\n",
        "\n",
        "# --- Install/Update Dependencies ---\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Installing/Updating core dependencies (including ipywidgets)...\")\n",
        "!pip install -q -U \"torch>=2.3.0\" \"torchvision>=0.18.0\" \"torchaudio>=2.3.0\" \\\n",
        "    \"transformers>=4.40.0\" \"sentence-transformers>=2.7.0\" \\\n",
        "    \"datasets>=2.19.0\" git+https://github.com/geoopt/geoopt.git \\\n",
        "    \"geomloss>=0.2.5\" \"einops>=0.7.0\" \"Pillow>=10.2.0\" \"matplotlib>=3.8.0\" \\\n",
        "    \"seaborn>=0.13.0\" \"gradio>=4.20.0\" \"scipy>=1.11.0\" \\\n",
        "    \"huggingface_hub>=0.20.0\" \\\n",
        "    \"wandb>=0.16.0\" \"tensorboard>=2.16.0\" \"ipywidgets>=7.6.0\" --no-cache-dir # Ensure ipywidgets is reasonably new\n",
        "\n",
        "# For Colab, ipywidgets usually works out of the box.\n",
        "# For local Jupyter, you might need: !jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Dependency installation/update process finished.\")\n",
        "\n",
        "# --- Standard & Library Imports ---\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Importing libraries...\")\n",
        "import math, random, os, functools, json, warnings, sys, copy\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any, Callable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torchvision.models as vision_models\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor, WhisperProcessor, WhisperModel\n",
        "import datasets\n",
        "from datasets import load_dataset, VerificationMode, Audio\n",
        "from datasets.exceptions import DatasetNotFoundError # For GUI in Cell 3 and data loading in Cell 4\n",
        "\n",
        "import torchaudio\n",
        "import scipy\n",
        "try:\n",
        "    import geoopt\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Successfully imported geoopt (version: {geoopt.__version__ if hasattr(geoopt, '__version__') else 'dev'}).\")\n",
        "except ImportError as e:\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] CRITICAL ERROR: FAILED to import geoopt.\")\n",
        "    raise e\n",
        "import geomloss\n",
        "from einops import rearrange, reduce, repeat\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import wandb\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from huggingface_hub import dataset_info as hf_dataset_info\n",
        "\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] All libraries imported successfully.\")\n",
        "\n",
        "# --- Check Versions & Device ---\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] --- Versions & Device Check ---\")\n",
        "print(f\"    Python version: {sys.version.split(' ')[0]}\")\n",
        "print(f\"    PyTorch version: {torch.__version__}\")\n",
        "print(f\"    Torchvision version: {torchvision.__version__}\")\n",
        "if hasattr(torchaudio, '__version__'): print(f\"    Torchaudio version: {torchaudio.__version__}\")\n",
        "else: print(\"    Torchaudio version: Not available.\")\n",
        "if hasattr(transformers, '__version__'): print(f\"    Transformers version: {transformers.__version__}\")\n",
        "else: print(\"    Transformers version: Not available.\")\n",
        "if hasattr(datasets, '__version__'): print(f\"    Datasets version: {datasets.__version__}\")\n",
        "else: print(\"    Datasets version: Not available.\")\n",
        "if 'geoopt' in sys.modules and hasattr(geoopt, '__version__'): print(f\"    Geoopt version: {geoopt.__version__}\")\n",
        "else: print(f\"    Geoopt version: (dev install - no __version__ attribute)\")\n",
        "if hasattr(gr, '__version__'): print(f\"    Gradio version: {gr.__version__}\")\n",
        "else: print(f\"    Gradio version: Not available.\")\n",
        "if hasattr(scipy, '__version__'): print(f\"    Scipy version: {scipy.__version__}\")\n",
        "else: print(f\"    Scipy version: Not available.\")\n",
        "if hasattr(widgets, '__version__'): print(f\"    IPyWidgets version: {widgets.__version__}\")\n",
        "else: print(f\"    IPyWidgets version: Not available.\")\n",
        "try:\n",
        "    from huggingface_hub import __version__ as hf_hub_version\n",
        "    print(f\"    huggingface_hub version: {hf_hub_version}\")\n",
        "except ImportError:\n",
        "    print(f\"    huggingface_hub version: Not available.\")\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if torch.cuda.is_available() and DEVICE.type == 'cuda':\n",
        "    print(f\"    CUDA Available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"        CUDA Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "elif DEVICE.type == 'cpu':\n",
        "    print(f\"    CUDA not available or not selected. Using CPU.\")\n",
        "else:\n",
        "    print(f\"    Using device: {DEVICE}\")\n",
        "\n",
        "# --- Script Version Info ---\n",
        "VERSION_INFO_STRING = \"3.5_FullPipeline_Corrected_AllCells\"\n",
        "print(f\"    Script Version: {VERSION_INFO_STRING}\")\n",
        "\n",
        "# --- Utility: Seed Everything ---\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"    Global random seed set to {seed}.\")\n",
        "\n",
        "INITIAL_SEED = 42\n",
        "seed_everything(INITIAL_SEED)\n",
        "\n",
        "# --- Suppress Common Warnings ---\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*LazyModules.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*represents the Weights.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "print(f\"    Common warnings suppressed.\")\n",
        "\n",
        "CURRENT_TIME_END_CELL1 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL1))}] Cell 1 setup complete. Duration: {CURRENT_TIME_END_CELL1 - CURRENT_TIME_START_CELL1:.2f}s\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-15 11:59:33] Cell 1: Initializing Environment & Dependencies...\n",
            "[2025-05-15 11:59:33] Installing/Updating core dependencies (including ipywidgets)...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[2025-05-15 12:02:42] Dependency installation/update process finished.\n",
            "[2025-05-15 12:02:42] Importing libraries...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e420b79da2b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvision_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roi_align\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_roi_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"rois must have shape as Tensor[K, 5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mget_meta_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "WMND_brmJtcg",
        "outputId": "7e667f0d-9935-4b3c-95aa-4592fc5a7fd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2"
      ],
      "metadata": {
        "id": "nlxBSKPeRWs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Geometry Library (Corrected SphereHead) ---\n",
        "# Defines geometry heads, helper functions (sqrtm, W2), and MixtureEmbedding class.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys # geoopt check uses sys\n",
        "import time\n",
        "# geoopt should have been successfully imported in Cell 1 or here\n",
        "if 'geoopt' not in sys.modules:\n",
        "    try:\n",
        "        import geoopt\n",
        "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] INFO: geoopt successfully imported in Cell 2.\")\n",
        "    except ImportError:\n",
        "        print(f\"CRITICAL ERROR in Cell 2: geoopt library not found or failed to import. This library is essential for non-Euclidean geometries. Please install it (e.g., pip install geoopt) and restart the kernel.\")\n",
        "        raise\n",
        "\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any\n",
        "import traceback # Added for more detailed error logging in functions\n",
        "\n",
        "# --- Configuration for this Cell ---\n",
        "CELL2_INFO_PREFIX = \"[Cell 2]\"\n",
        "DEBUG_CELL2 = True # For verbose logging within this cell's components (matches your Cell 5 flag)\n",
        "\n",
        "CURRENT_TIME_START_CELL2 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL2))}] {CELL2_INFO_PREFIX} Defining Geometry Library components...\")\n",
        "\n",
        "# --- Common Projection Dimension ---\n",
        "if 'config' in globals() and hasattr(config, 'common_proj_dim'):\n",
        "    COMMON_PROJECTION_DIM = config.common_proj_dim\n",
        "    print(f\"    {CELL2_INFO_PREFIX} Using COMMON_PROJECTION_DIM from global 'config': {COMMON_PROJECTION_DIM}\")\n",
        "elif 'COMMON_PROJECTION_DIM' not in globals():\n",
        "    COMMON_PROJECTION_DIM = 128\n",
        "    print(f\"    {CELL2_INFO_PREFIX} COMMON_PROJECTION_DIM not found globally or in config, fallback to default: {COMMON_PROJECTION_DIM}\")\n",
        "else:\n",
        "    print(f\"    {CELL2_INFO_PREFIX} Using existing global COMMON_PROJECTION_DIM: {COMMON_PROJECTION_DIM}\")\n",
        "\n",
        "\n",
        "# --- Numerically Stable Matrix Square Root ---\n",
        "def sqrtm_stable(matrix: torch.Tensor, retries: int = 5, eps_factor: float = 1e-6) -> torch.Tensor:\n",
        "    if matrix.dim() < 2 or matrix.shape[-1] != matrix.shape[-2]:\n",
        "        if DEBUG_CELL2: print(f\"ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] Input matrix must be at least 2D and square. Got shape: {matrix.shape}\")\n",
        "        raise ValueError(f\"Input matrix must be at least 2D and square. Got shape: {matrix.shape}\")\n",
        "\n",
        "    current_matrix_to_process = matrix\n",
        "    for i in range(retries + 1):\n",
        "        try:\n",
        "            if i == 0:\n",
        "                matrix_for_eigh = 0.5 * (current_matrix_to_process + current_matrix_to_process.mH)\n",
        "            else:\n",
        "                eps_shift_increase = eps_factor * (10**(i-1))\n",
        "                identity_shift = torch.eye(current_matrix_to_process.shape[-1], device=current_matrix_to_process.device, dtype=current_matrix_to_process.dtype) * eps_shift_increase\n",
        "                shifted_matrix = current_matrix_to_process + identity_shift\n",
        "                matrix_for_eigh = 0.5 * (shifted_matrix + shifted_matrix.mH)\n",
        "                if DEBUG_CELL2 and i==1 : print(f\"    DEBUG: {CELL2_INFO_PREFIX}[sqrtm_stable] Retry {i}, added eps_shift: {eps_shift_increase:.1e}\")\n",
        "\n",
        "            eigval, eigvec = torch.linalg.eigh(matrix_for_eigh)\n",
        "            eigval_clamped = torch.clamp(eigval, min=1e-8)\n",
        "\n",
        "            if torch.isnan(eigval_clamped).any() or torch.isinf(eigval_clamped).any():\n",
        "                if DEBUG_CELL2: print(f\"    WARNING: {CELL2_INFO_PREFIX}[sqrtm_stable] NaN/Inf in clamped eigenvalues on attempt {i}. Matrix for eigh min/max: {matrix_for_eigh.min():.2e}/{matrix_for_eigh.max():.2e}\")\n",
        "                if i == retries:\n",
        "                    print(f\"    ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] NaN/Inf eigenvalues on final retry. Input matrix might be severely ill-conditioned.\")\n",
        "                    return torch.zeros_like(matrix)\n",
        "                continue\n",
        "\n",
        "            sqrt_eigval = torch.sqrt(eigval_clamped)\n",
        "            sqrt_matrix = eigvec @ torch.diag_embed(sqrt_eigval) @ eigvec.mH\n",
        "\n",
        "            if torch.isfinite(sqrt_matrix).all():\n",
        "                if DEBUG_CELL2 and i > 0: print(f\"    INFO: {CELL2_INFO_PREFIX}[sqrtm_stable] Succeeded on retry {i}.\")\n",
        "                return sqrt_matrix\n",
        "            else:\n",
        "                if DEBUG_CELL2: print(f\"    WARNING: {CELL2_INFO_PREFIX}[sqrtm_stable] sqrt_matrix contains NaN/Inf on attempt {i}.\")\n",
        "                if i == retries:\n",
        "                    print(f\"    ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] sqrt_matrix has NaNs/Infs on final retry. Returning zeros.\")\n",
        "                    return torch.zeros_like(matrix)\n",
        "\n",
        "        except torch.linalg.LinAlgError as e:\n",
        "            if DEBUG_CELL2: print(f\"    WARNING: {CELL2_INFO_PREFIX}[sqrtm_stable] LinAlgError on attempt {i}: {e}\")\n",
        "            if i == retries:\n",
        "                print(f\"    ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] LinAlgError on final retry ({e}). Returning zeros.\")\n",
        "                return torch.zeros_like(matrix)\n",
        "        except Exception as e_other:\n",
        "            if DEBUG_CELL2: print(f\"    WARNING: {CELL2_INFO_PREFIX}[sqrtm_stable] Unexpected error on attempt {i}: {type(e_other).__name__} - {e_other}\")\n",
        "            if i == retries:\n",
        "                print(f\"    ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] Unexpected error on final retry ({e_other}). Returning zeros.\")\n",
        "                return torch.zeros_like(matrix)\n",
        "\n",
        "    print(f\"    ERROR: {CELL2_INFO_PREFIX}[sqrtm_stable] Loop completed without returning a valid matrix. Fallback to zeros.\")\n",
        "    return torch.zeros_like(matrix)\n",
        "\n",
        "# --- Gaussian Wasserstein Distance (Vectorized for Pairwise) ---\n",
        "def gaussian_wasserstein_distance_pairwise(\n",
        "    mu1: torch.Tensor, sigma1: torch.Tensor,\n",
        "    mu2: torch.Tensor, sigma2: torch.Tensor,\n",
        "    stability_eps: float = 1e-6,\n",
        "    component_name_for_log: str = \"W2_Pairwise\"\n",
        ") -> torch.Tensor:\n",
        "    func_prefix_w2 = f\"{CELL2_INFO_PREFIX}[{component_name_for_log}]\"\n",
        "    if not (mu1.ndim >= 2 and sigma1.ndim >= 3 and mu2.ndim >= 2 and sigma2.ndim >= 3):\n",
        "        bs_i = mu1.shape[0] if mu1.ndim >=1 else 0; bs_j = mu2.shape[0] if mu2.ndim >=1 else 0\n",
        "        dev = mu1.device if mu1.numel() > 0 else (sigma1.device if sigma1.numel() > 0 else torch.device('cpu'))\n",
        "        dt = mu1.dtype if mu1.numel() > 0 else torch.float32\n",
        "        if DEBUG_CELL2: print(f\"ERROR: {func_prefix_w2} Incorrect input dims. mu1:{mu1.ndim}, sigma1:{sigma1.ndim}, mu2:{mu2.ndim}, sigma2:{sigma2.ndim}. Returning large dists.\")\n",
        "        return torch.full((bs_i, bs_j), 1e6, device=dev, dtype=dt)\n",
        "\n",
        "    N, D1 = mu1.shape[0], mu1.shape[-1]; M, D2 = mu2.shape[0], mu2.shape[-1]\n",
        "    if D1 != D2: raise ValueError(f\"{func_prefix_w2} Mean dimensions mismatch D1={D1}, D2={D2}.\")\n",
        "    D = D1\n",
        "    if sigma1.shape[0]!=N or sigma1.shape[1:]!=(D,D) or sigma2.shape[0]!=M or sigma2.shape[1:]!=(D,D):\n",
        "        raise ValueError(f\"{func_prefix_w2} Covariance shape mismatch. mu1:{mu1.shape}, sigma1:{sigma1.shape}, mu2:{mu2.shape}, sigma2:{sigma2.shape}\")\n",
        "\n",
        "    mu_diff_sq = torch.sum((mu1.unsqueeze(1) - mu2.unsqueeze(0))**2, dim=-1)\n",
        "    trace_sigma1 = torch.diagonal(sigma1, dim1=-2, dim2=-1).sum(-1)\n",
        "    trace_sigma2 = torch.diagonal(sigma2, dim1=-2, dim2=-1).sum(-1)\n",
        "    trace_sum_term = trace_sigma1.unsqueeze(1) + trace_sigma2.unsqueeze(0)\n",
        "\n",
        "    sqrt_sigma2_all_m = sqrtm_stable(sigma2)\n",
        "    if not torch.isfinite(sqrt_sigma2_all_m).all():\n",
        "        if DEBUG_CELL2: print(f\"WARNING: {func_prefix_w2} sqrt_sigma2_all_m non-finite. W2 dist may be large.\")\n",
        "        return torch.full_like(mu_diff_sq, 1e6)\n",
        "\n",
        "    cross_trace_term_matrix = torch.zeros(N, M, device=mu1.device, dtype=mu1.dtype)\n",
        "    for i in range(N):\n",
        "        sigma1_i_current = sigma1[i]\n",
        "        if DEBUG_CELL2 and not torch.isfinite(sigma1_i_current).all(): continue\n",
        "        for j in range(M):\n",
        "            sqrt_sigma2_j_current = sqrt_sigma2_all_m[j]\n",
        "            if DEBUG_CELL2 and not torch.isfinite(sqrt_sigma2_j_current).all(): continue\n",
        "            try:\n",
        "                prod_term_ij = sqrt_sigma2_j_current @ sigma1_i_current @ sqrt_sigma2_j_current\n",
        "                eps_identity_prod = torch.eye(D, device=prod_term_ij.device, dtype=prod_term_ij.dtype) * stability_eps\n",
        "                sqrt_of_prod_term_ij = sqrtm_stable(prod_term_ij + eps_identity_prod)\n",
        "                current_trace_val = torch.trace(sqrt_of_prod_term_ij)\n",
        "                if torch.isfinite(current_trace_val): cross_trace_term_matrix[i, j] = current_trace_val\n",
        "            except Exception as e_ct:\n",
        "                if DEBUG_CELL2: print(f\"    DEBUG: {func_prefix_w2} Exc in cross-term i={i},j={j}: {e_ct}\")\n",
        "\n",
        "    w2_squared = mu_diff_sq + trace_sum_term - 2 * cross_trace_term_matrix\n",
        "    w2_squared_clamped = torch.clamp(w2_squared, min=0.0)\n",
        "    is_finite_w2 = torch.isfinite(w2_squared_clamped)\n",
        "    if not is_finite_w2.all() and DEBUG_CELL2: print(f\"WARNING: {func_prefix_w2} Non-finite W2 results. Replacing.\")\n",
        "    return torch.where(is_finite_w2, w2_squared_clamped, torch.tensor(1e6, device=w2_squared_clamped.device, dtype=w2_squared_clamped.dtype))\n",
        "\n",
        "# --- Geometry Head Classes ---\n",
        "class EuclidHead(nn.Module):\n",
        "    def __init__(self,d_in:int,d_lat:int):super().__init__();self.d_lat=d_lat;self.mu_net=nn.Linear(d_in,d_lat);self.cov_factor_net=nn.Linear(d_in,d_lat*d_lat);self.mean_projector=nn.Linear(d_lat,COMMON_PROJECTION_DIM);self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[EuclidHead] Input 'e' NaN/Inf.\")\n",
        "        mu=self.mu_net(e);cov_factor=self.cov_factor_net(e).view(-1,self.d_lat,self.d_lat);cov=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.d_lat,device=e.device,dtype=e.dtype).unsqueeze(0);mu_proj=self.mean_projector(mu)\n",
        "        if DEBUG_CELL2:\n",
        "            if not torch.isfinite(mu).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[EuclidHead] 'mu' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[EuclidHead] 'cov' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[EuclidHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu,cov,mu_proj\n",
        "\n",
        "class SphereHead(nn.Module):\n",
        "    def __init__(self,d_in:int,d_lat:int):\n",
        "        super().__init__()\n",
        "        self.d_lat=d_lat\n",
        "        self.ambient_dim=d_lat+1\n",
        "        # *** CORRECTED LINE: Removed ndim argument ***\n",
        "        self.manifold=geoopt.manifolds.Sphere()\n",
        "        self.mu_net=nn.Linear(d_in,self.ambient_dim)\n",
        "        self.cov_factor_net=nn.Linear(d_in,d_lat*d_lat)\n",
        "        self.mean_projector=nn.Linear(self.ambient_dim,COMMON_PROJECTION_DIM)\n",
        "        self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SphereHead] Input 'e' NaN/Inf.\")\n",
        "        mu_ambient=self.mu_net(e)\n",
        "        mu_on_sphere=self.manifold.projx(mu_ambient)\n",
        "        mu_on_sphere = torch.where(torch.isfinite(mu_on_sphere), mu_on_sphere, torch.zeros_like(mu_on_sphere)) # Add NaN/Inf guard\n",
        "        cov_factor=self.cov_factor_net(e).view(-1,self.d_lat,self.d_lat)\n",
        "        cov_tangent=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.d_lat,device=e.device,dtype=e.dtype).unsqueeze(0)\n",
        "        mu_proj=self.mean_projector(mu_on_sphere)\n",
        "        if DEBUG_CELL2:\n",
        "            if not torch.isfinite(mu_on_sphere).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SphereHead] 'mu_on_sphere' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov_tangent).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SphereHead] 'cov_tangent' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SphereHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu_on_sphere,cov_tangent,mu_proj\n",
        "\n",
        "class HyperbolicHead(nn.Module):\n",
        "    def __init__(self,d_in:int,d_lat:int):\n",
        "        super().__init__();self.d_lat=d_lat\n",
        "        self.manifold=geoopt.manifolds.PoincareBall(c=1.0)\n",
        "        self.mu_net=nn.Linear(d_in,d_lat)\n",
        "        self.cov_factor_net=nn.Linear(d_in,d_lat*d_lat)\n",
        "        self.mean_projector=nn.Linear(d_lat,COMMON_PROJECTION_DIM)\n",
        "        self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[HyperbolicHead] Input 'e' NaN/Inf.\")\n",
        "        mu_raw=self.mu_net(e);mu_on_ball=self.manifold.projx(mu_raw)\n",
        "        mu_on_ball=torch.where(torch.isfinite(mu_on_ball),mu_on_ball,torch.zeros_like(mu_on_ball))\n",
        "        cov_factor=self.cov_factor_net(e).view(-1,self.d_lat,self.d_lat)\n",
        "        cov_tangent=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.d_lat,device=e.device,dtype=e.dtype).unsqueeze(0)\n",
        "        mu_proj=self.mean_projector(mu_on_ball)\n",
        "        if DEBUG_CELL2:\n",
        "            if not torch.isfinite(mu_on_ball).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[HyperbolicHead] 'mu_on_ball' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov_tangent).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[HyperbolicHead] 'cov_tangent' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[HyperbolicHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu_on_ball,cov_tangent,mu_proj\n",
        "\n",
        "class ComplexHead(nn.Module): # (As per your code, added NaN/Inf checks)\n",
        "    def __init__(self,d_in:int,d_lat:int):super().__init__();self.real_dim=2*d_lat;self.mu_net=nn.Linear(d_in,self.real_dim);self.cov_factor_net=nn.Linear(d_in,self.real_dim*self.real_dim);self.mean_projector=nn.Linear(self.real_dim,COMMON_PROJECTION_DIM);self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[ComplexHead] Input 'e' NaN/Inf.\")\n",
        "        mu=self.mu_net(e);cov_factor=self.cov_factor_net(e).view(-1,self.real_dim,self.real_dim);cov=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.real_dim,device=e.device).unsqueeze(0);mu_proj=self.mean_projector(mu)\n",
        "        if DEBUG_CELL2: # Check outputs\n",
        "            if not torch.isfinite(mu).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[ComplexHead] 'mu' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[ComplexHead] 'cov' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[ComplexHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu,cov,mu_proj\n",
        "\n",
        "class QuaternionHead(nn.Module): # (As per your code, added NaN/Inf checks)\n",
        "    def __init__(self,d_in:int,d_lat:int):super().__init__();self.real_dim=4*d_lat;self.mu_net=nn.Linear(d_in,self.real_dim);self.cov_factor_net=nn.Linear(d_in,self.real_dim*self.real_dim);self.mean_projector=nn.Linear(self.real_dim,COMMON_PROJECTION_DIM);self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[QuaternionHead] Input 'e' NaN/Inf.\")\n",
        "        mu=self.mu_net(e);cov_factor=self.cov_factor_net(e).view(-1,self.real_dim,self.real_dim);cov=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.real_dim,device=e.device).unsqueeze(0);mu_proj=self.mean_projector(mu)\n",
        "        if DEBUG_CELL2: # Check outputs\n",
        "            if not torch.isfinite(mu).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[QuaternionHead] 'mu' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[QuaternionHead] 'cov' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[QuaternionHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu,cov,mu_proj\n",
        "\n",
        "class SimplexHead(nn.Module): # (As per your code, added NaN/Inf checks)\n",
        "    def __init__(self,d_in:int,d_lat:int):super().__init__();self.d_lat=d_lat;self.tangent_dim=d_lat-1 if d_lat > 1 else 1;self.logits_net=nn.Linear(d_in,d_lat);self.cov_factor_net=nn.Linear(d_in,self.tangent_dim*self.tangent_dim);self.mean_projector=nn.Linear(d_lat,COMMON_PROJECTION_DIM);self.epsilon=1e-5\n",
        "    def forward(self,e):\n",
        "        if DEBUG_CELL2 and not torch.isfinite(e).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SimplexHead] Input 'e' NaN/Inf.\")\n",
        "        logits=self.logits_net(e);mu_on_simplex=F.softmax(logits,dim=-1);cov_factor=self.cov_factor_net(e).view(-1,self.tangent_dim,self.tangent_dim);cov_tangent=torch.bmm(cov_factor,cov_factor.transpose(1,2))+self.epsilon*torch.eye(self.tangent_dim,device=e.device).unsqueeze(0);mu_proj=self.mean_projector(mu_on_simplex)\n",
        "        if DEBUG_CELL2: # Check outputs\n",
        "            if not torch.isfinite(mu_on_simplex).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SimplexHead] 'mu_on_simplex' NaN/Inf.\")\n",
        "            if not torch.isfinite(cov_tangent).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SimplexHead] 'cov_tangent' NaN/Inf.\")\n",
        "            if not torch.isfinite(mu_proj).all(): print(f\"WARNING: {CELL2_INFO_PREFIX}[SimplexHead] 'mu_proj' NaN/Inf.\")\n",
        "        return mu_on_simplex,cov_tangent,mu_proj\n",
        "\n",
        "GEOM_HEAD_MAP = {'E':EuclidHead,'S':SphereHead,'H':HyperbolicHead,'C':ComplexHead,'Q':QuaternionHead,'Delta':SimplexHead}\n",
        "print(f\"    {CELL2_INFO_PREFIX} Defined Geometry Heads: {list(GEOM_HEAD_MAP.keys())}\")\n",
        "\n",
        "class MixtureEmbedding(nn.Module):\n",
        "    def __init__(self,d_enc:int,geometry_dims:Dict[str,int],temperature:float,geometries_to_use:List[str],kernel_function:str=\"w2_or_projeuclid\"):\n",
        "        super().__init__()\n",
        "        self.class_prefix = f\"{CELL2_INFO_PREFIX}[MixtureEmbedding]\"\n",
        "        self.d_enc=d_enc; self.geometry_dims_config=geometry_dims\n",
        "        self.geometries_to_use=[g for g in geometries_to_use if g in GEOM_HEAD_MAP and g in geometry_dims]\n",
        "        if not self.geometries_to_use: raise ValueError(f\"{self.class_prefix} No valid/configured geometries specified.\")\n",
        "        self.temperature=nn.Parameter(torch.tensor(max(float(temperature),1e-6)))\n",
        "        self.geom_keys=self.geometries_to_use; self.num_geoms=len(self.geom_keys)\n",
        "        if self.num_geoms == 0: raise ValueError(f\"{self.class_prefix} No active geometries. Cannot initialize.\")\n",
        "        self.heads=nn.ModuleDict({k:GEOM_HEAD_MAP[k](d_in=d_enc,d_lat=geometry_dims[k]) for k in self.geom_keys})\n",
        "        self.gamma_net=nn.Linear(d_enc,self.num_geoms); self._encoder=nn.Identity(); self.kernel_function=kernel_function.lower()\n",
        "        print(f\"    {self.class_prefix} Initialized. Active: {self.geom_keys}, Input d_enc: {d_enc}, DistMetric: '{self.kernel_function}', InitTemp: {self.temperature.item():.4f}\")\n",
        "\n",
        "    def set_encoder(self,encoder_module:nn.Module):\n",
        "        if isinstance(encoder_module, nn.Module): self._encoder=encoder_module; print(f\"{self.class_prefix} Shared encoder set: {type(encoder_module).__name__}\")\n",
        "        else: self._encoder=nn.Identity(); print(f\"WARNING: {self.class_prefix} Invalid encoder set. Using Identity.\")\n",
        "\n",
        "    def forward(self,x: torch.Tensor) -> Tuple[Dict[str, Tuple[torch.Tensor, ...]], torch.Tensor]:\n",
        "        if not isinstance(self._encoder,nn.Identity):\n",
        "            x_encoded=self._encoder(x)\n",
        "            if x_encoded is None: print(f\"ERROR: {self.class_prefix} Shared encoder returned None.\"); return {k:self._empty_measure_tuple(x) for k in self.geom_keys}, torch.empty(0, self.num_geoms, device=x.device)\n",
        "            x = x_encoded\n",
        "        if DEBUG_CELL2 and not torch.isfinite(x).all(): print(f\"WARNING: {self.class_prefix} Input 'x' to gamma/heads NaN/Inf. Shape: {x.shape}\")\n",
        "\n",
        "        current_temp = self.temperature.data.clamp(min=1e-6, max=1.0) # Clamp temperature during use for stability\n",
        "        gamma_logits=self.gamma_net(x); gamma=F.softmax(gamma_logits / current_temp, dim=-1)\n",
        "\n",
        "        measures: Dict[str, Tuple[torch.Tensor, ...]] = {}\n",
        "        for k_head in self.geom_keys:\n",
        "            try:\n",
        "                head_output = self.heads[k_head](x)\n",
        "                if isinstance(head_output, tuple) and len(head_output) == 3 and all(isinstance(t, torch.Tensor) for t in head_output):\n",
        "                    measures[k_head] = head_output\n",
        "                    if DEBUG_CELL2 and not all(torch.isfinite(t).all() for t in head_output): print(f\"WARNING: {self.class_prefix} Head '{k_head}' output NaN/Inf.\")\n",
        "                else:\n",
        "                    print(f\"ERROR: {self.class_prefix} Head '{k_head}' bad output: {type(head_output)}. Skipping.\"); measures[k_head] = self._empty_measure_tuple(x)\n",
        "            except Exception as e_head_fwd:\n",
        "                print(f\"ERROR: {self.class_prefix} Error in head '{k_head}': {e_head_fwd}\\n{traceback.format_exc()}\"); measures[k_head] = self._empty_measure_tuple(x)\n",
        "        if DEBUG_CELL2 and not torch.isfinite(gamma).all(): print(f\"WARNING: {self.class_prefix} Output 'gamma' NaN/Inf.\")\n",
        "        return measures,gamma\n",
        "\n",
        "    def _empty_measure_tuple(self, x_ref: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Helper to return empty/placeholder tensors for a failed geometry head.\"\"\"\n",
        "        # Placeholder shapes, adjust if more specific error shapes are needed\n",
        "        d_placeholder = self.geometry_dims_config.get(self.geom_keys[0] if self.geom_keys else 'E', 16) # Default to a common dim\n",
        "        bs = x_ref.shape[0] if x_ref is not None and x_ref.dim() > 0 else 0\n",
        "        return (torch.empty(bs, d_placeholder, device=x_ref.device),\n",
        "                torch.empty(bs, d_placeholder, d_placeholder, device=x_ref.device),\n",
        "                torch.empty(bs, COMMON_PROJECTION_DIM, device=x_ref.device))\n",
        "\n",
        "\n",
        "    def compute_entropy(self, measures:Optional[Dict]=None, gamma:Optional[torch.Tensor]=None):\n",
        "        if gamma is None or gamma.numel() == 0:return torch.tensor(0.0,device=self.temperature.device,dtype=self.temperature.dtype)\n",
        "        entropy = -torch.sum(gamma.clamp(min=1e-9)*torch.log(gamma.clamp(min=1e-9)),dim=-1)\n",
        "        return entropy.mean()\n",
        "\n",
        "    def compute_pairwise_distances(self, measures_i,gamma_i,measures_j,gamma_j,metric:Optional[str]=None):\n",
        "        func_prefix_dist = f\"{self.class_prefix}[compute_pairwise_distances]\"\n",
        "        current_metric_strategy = metric if metric is not None else self.kernel_function\n",
        "        dev_default = self.temperature.device; dtype_default = self.temperature.dtype\n",
        "\n",
        "        if not(measures_i and measures_j and gamma_i is not None and gamma_i.numel()>0 and gamma_j is not None and gamma_j.numel()>0):\n",
        "            bs_i=gamma_i.shape[0] if gamma_i is not None and gamma_i.numel()>0 else 0;bs_j=gamma_j.shape[0] if gamma_j is not None and gamma_j.numel()>0 else 0\n",
        "            if DEBUG_CELL2: print(f\"DEBUG: {func_prefix_dist} Invalid inputs. Returning zero dist matrix ({bs_i}x{bs_j}).\")\n",
        "            return torch.zeros(bs_i,bs_j,device=dev_default,dtype=dtype_default)\n",
        "\n",
        "        bs_i,num_geoms_i=gamma_i.shape; bs_j,num_geoms_j=gamma_j.shape\n",
        "        dev,dt=gamma_i.device,gamma_i.dtype\n",
        "        if num_geoms_i!=self.num_geoms or num_geoms_j!=self.num_geoms:\n",
        "            print(f\"ERROR: {func_prefix_dist} Gamma dim mismatch. Gamma_i:{gamma_i.shape}, Gamma_j:{gamma_j.shape}, Expected_geoms:{self.num_geoms}\"); return torch.full((bs_i,bs_j),1e6,device=dev,dtype=dt)\n",
        "\n",
        "        total_dist_matrix=torch.zeros(bs_i,bs_j,device=dev,dtype=dt)\n",
        "        try: gamma_prod_across_geoms=torch.einsum('ik,jk->ijk',gamma_i,gamma_j)\n",
        "        except Exception as e_einsum: print(f\"ERROR: {func_prefix_dist} Einsum failed: {e_einsum}\"); return torch.full((bs_i,bs_j),1e6,device=dev,dtype=dt)\n",
        "\n",
        "        for k_idx,geom_key_k in enumerate(self.geom_keys):\n",
        "            if geom_key_k not in measures_i or geom_key_k not in measures_j or \\\n",
        "               not (isinstance(measures_i[geom_key_k], tuple) and len(measures_i[geom_key_k]) == 3) or \\\n",
        "               not (isinstance(measures_j[geom_key_k], tuple) and len(measures_j[geom_key_k]) == 3):\n",
        "                if DEBUG_CELL2: print(f\"DEBUG: {func_prefix_dist} Geom '{geom_key_k}' missing or malformed in measures. Skipping.\"); continue\n",
        "\n",
        "            try:\n",
        "                mu_i_k,cov_i_k,proj_i_k=measures_i[geom_key_k]; mu_j_k,cov_j_k,proj_j_k=measures_j[geom_key_k]\n",
        "                if not all(isinstance(t, torch.Tensor) for t in [mu_i_k,cov_i_k,proj_i_k, mu_j_k,cov_j_k,proj_j_k]):\n",
        "                    if DEBUG_CELL2: print(f\"WARNING: {func_prefix_dist} Not all components for '{geom_key_k}' are tensors. Skipping.\"); continue\n",
        "\n",
        "                current_geom_pairwise_dist_ij = torch.zeros(bs_i,bs_j,device=dev,dtype=dt)\n",
        "                use_w2 = (current_metric_strategy == \"w2\" or \"w2\" in current_metric_strategy) and geom_key_k in ['E','C','Q']\n",
        "\n",
        "                if use_w2:\n",
        "                    if mu_i_k.ndim >=2 and mu_j_k.ndim >=2 and cov_i_k.ndim >=3 and cov_j_k.ndim >=3 and mu_i_k.shape[0]==bs_i and mu_j_k.shape[0]==bs_j:\n",
        "                         current_geom_pairwise_dist_ij = gaussian_wasserstein_distance_pairwise(mu_i_k,cov_i_k,mu_j_k,cov_j_k, component_name_for_log=f\"W2_Geom_{geom_key_k}\")\n",
        "                    elif DEBUG_CELL2: print(f\"DEBUG: {func_prefix_dist} Skipping W2 for '{geom_key_k}' due to incompatible mu/cov shapes/batch_size.\")\n",
        "                else:\n",
        "                    if proj_i_k.ndim >=2 and proj_j_k.ndim >=2 and proj_i_k.shape[0]==bs_i and proj_j_k.shape[0]==bs_j:\n",
        "                        current_geom_pairwise_dist_ij = torch.cdist(proj_i_k,proj_j_k,p=2).pow(2)\n",
        "                    elif DEBUG_CELL2: print(f\"DEBUG: {func_prefix_dist} Skipping cdist for '{geom_key_k}' due to incompatible proj shapes/batch_size.\")\n",
        "\n",
        "                if not torch.isfinite(current_geom_pairwise_dist_ij).all():\n",
        "                    if DEBUG_CELL2: print(f\"WARNING: {func_prefix_dist} Non-finite dists for '{geom_key_k}'. Replacing with 1e6.\")\n",
        "                    current_geom_pairwise_dist_ij = torch.where(torch.isfinite(current_geom_pairwise_dist_ij),current_geom_pairwise_dist_ij,torch.tensor(1e6,device=dev,dtype=dt))\n",
        "                total_dist_matrix += gamma_prod_across_geoms[:,:,k_idx] * current_geom_pairwise_dist_ij\n",
        "            except TypeError as te_unpack:\n",
        "                print(f\"ERROR: {func_prefix_dist} TypeError for geom '{geom_key_k}': {te_unpack}. Skipping.\")\n",
        "            except Exception as e_geom_dist:\n",
        "                print(f\"ERROR: {func_prefix_dist} Error for geom '{geom_key_k}': {e_geom_dist}\")\n",
        "                if DEBUG_CELL2:\n",
        "                    print(traceback.format_exc())\n",
        "\n",
        "        total_dist_matrix_clamped = torch.clamp(total_dist_matrix, min=0.0)\n",
        "        if not torch.isfinite(total_dist_matrix_clamped).all():\n",
        "            if DEBUG_CELL2:\n",
        "                print(f\"WARNING: {func_prefix_dist} Final total_dist_matrix NaN/Inf. Replacing with 1e6.\")\n",
        "            return torch.where(torch.isfinite(total_dist_matrix_clamped),total_dist_matrix_clamped,torch.tensor(1e6,device=dev,dtype=dt))\n",
        "        return total_dist_matrix_clamped\n",
        "\n",
        "CURRENT_TIME_END_CELL2 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL2))}] {CELL2_INFO_PREFIX} Geometry Library components defined. Duration: {CURRENT_TIME_END_CELL2 - CURRENT_TIME_START_CELL2:.2f}s\")"
      ],
      "metadata": {
        "id": "XcEEgg4-Mpt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# section 3"
      ],
      "metadata": {
        "id": "VrEoxLJhRY4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 3: Configuration (Fully Code-Based, Multi-Dataset Active, Streaming-Aware) ---\n",
        "# Defines ModelConfig, DatasetPairConfig, instantiates global 'config',\n",
        "# and uses a direct Python list to define 'config.active_training_pairs'\n",
        "# with examples for Image-Text, Text-Code, and Audio-Text (gated) datasets active by default.\n",
        "# GUI has been REMOVED.\n",
        "\n",
        "import time\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import traceback # For detailed error logging\n",
        "\n",
        "# Imports for dataset info fetching (used in optional helper function)\n",
        "from huggingface_hub import dataset_info as hf_dataset_info, HfFolder, notebook_login\n",
        "from datasets.exceptions import DatasetNotFoundError\n",
        "import datasets # For type checking\n",
        "\n",
        "CELL_INFO_PREFIX = \"[Cell 3]\"\n",
        "DEBUG_CELL3 = True\n",
        "\n",
        "# --- Colab Widget Manager (Informational, as GUI is removed) ---\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL_INFO_PREFIX} Note: Colab environment detected. GUI has been removed for code-based configuration.\")\n",
        "\n",
        "# Ensure COMMON_PROJECTION_DIM is available (should be from Cell 2)\n",
        "if 'COMMON_PROJECTION_DIM' not in globals():\n",
        "    COMMON_PROJECTION_DIM = 128 # Default if not set by Cell 2\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))}] WARNING: {CELL_INFO_PREFIX} COMMON_PROJECTION_DIM not found globally, fallback: {COMMON_PROJECTION_DIM}\")\n",
        "\n",
        "CURRENT_TIME_START_CELL3 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL3))}] {CELL_INFO_PREFIX} Defining Configuration (with advanced model options) & Code-Based Dataset Setup...\")\n",
        "\n",
        "# --- Dataclass Definitions ---\n",
        "@dataclass\n",
        "class DatasetPairConfig:\n",
        "    \"\"\"Configuration for a single dataset pair to be used in training.\"\"\"\n",
        "    dataset_id: str\n",
        "    modality1_key: str\n",
        "    modality1_type: str\n",
        "    modality2_key: str\n",
        "    modality2_type: str\n",
        "    split: str = 'train' # Actual data split (e.g., 'train', 'test', 'validation', or 'split_0')\n",
        "    dataset_config_name: Optional[str] = None # For HF datasets with named configurations\n",
        "    data_files_glob_pattern: Optional[str] = None # For WebDatasets using file patterns\n",
        "    streaming: Optional[bool] = None # To control streaming for this specific pair\n",
        "\n",
        "    image_key_override: Optional[str] = None\n",
        "    caption_key_override: Optional[str] = None\n",
        "    audio_key_override: Optional[str] = None\n",
        "    attempt_fallback_to_conceptual_captions: bool = False\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Global configuration for models, training, data, and logging.\"\"\"\n",
        "    # Core Embedding & Mixture Model Parameters\n",
        "    d_enc: int = 512\n",
        "    common_proj_dim: int = COMMON_PROJECTION_DIM\n",
        "    geometry_dims: Dict[str, int] = field(default_factory=lambda: {'E': 64, 'S': 32, 'H': 16})\n",
        "    geometries_to_use: List[str] = field(default_factory=lambda: ['E', 'S', 'H'])\n",
        "    mixture_temp: float = 0.1\n",
        "    mixture_kernel_function: str = \"w2_or_projeuclid\"\n",
        "\n",
        "    # Unimodal Encoder Configurations\n",
        "    image_encoder_name: str = \"resnet50\"; image_pretrained: bool = True\n",
        "    image_encoder_projection_layers: int = 1; image_encoder_projection_hidden_dim: Optional[int] = None\n",
        "    image_encoder_projection_activation: str = 'relu'; image_encoder_use_layernorm: bool = True\n",
        "\n",
        "    text_encoder_name: str = 'sentence-transformers/all-MiniLM-L12-v2'; text_freeze_transformer: bool = True\n",
        "    text_encoder_pooling_strategy: str = 'mean'; text_encoder_projection_layers: int = 1\n",
        "    text_encoder_projection_hidden_dim: Optional[int] = None; text_encoder_projection_activation: str = 'relu'\n",
        "    text_encoder_use_layernorm: bool = True\n",
        "\n",
        "    audio_encoder_name: Optional[str] = None; audio_freeze_encoder: bool = True\n",
        "    audio_encoder_projection_layers: int = 1; audio_encoder_projection_hidden_dim: Optional[int] = None\n",
        "    audio_encoder_projection_activation: str = 'relu'; audio_encoder_use_layernorm: bool = True\n",
        "\n",
        "    # PairModel Fusion Configuration\n",
        "    pair_fusion_method: str = 'concat'; pair_fusion_mlp_hidden_layers: int = 1; pair_fusion_mlp_activation: str = 'relu'\n",
        "\n",
        "    # Loss Function Parameters\n",
        "    lambda_kl_contrastive: float = 1.0\n",
        "    lambda_ent: float = 0.05\n",
        "    lambda_reg: float = 1e-3\n",
        "    lambda_sb: float = 0.1\n",
        "    main_loss_reg_type: str = 'mean_norm'\n",
        "    ot_cost_function: str = \"sqeuclidean\"; ot_blur: float = 0.05; ot_scaling: float = 0.9; ot_debias: bool = True; ot_backend: str = \"tensorized\"\n",
        "\n",
        "    # Optimizer & Scheduler Parameters\n",
        "    optimizer_type: str = \"AdamW\"; lr: float = 2e-4; weight_decay: float = 1e-4\n",
        "    use_lr_scheduler: bool = True; scheduler_type: str = \"cosine_with_warmup\"; scheduler_warmup_epochs: int = 1; scheduler_num_cycles: float = 0.5; scheduler_patience: int = 3; scheduler_factor: float = 0.1\n",
        "\n",
        "    # Training Loop Parameters\n",
        "    num_epochs: int = 5; batch_size: int = 16; gradient_accumulation_steps: int = 10\n",
        "    use_amp: bool = False; # Set to False for easier debugging initially\n",
        "    gradient_clip_val: Optional[float] = 1.0\n",
        "\n",
        "    # Validation & Logging\n",
        "    val_check_interval_epochs: int = 1\n",
        "    num_val_test_samples_cap: Optional[int] = 1024\n",
        "    log_steps: int = 10\n",
        "    use_wandb: bool = False; wandb_project: str = \"MultiModal_AllPairs_V_Final\";\n",
        "\n",
        "    # Data Parameters\n",
        "    active_training_pairs: Dict[str, DatasetPairConfig] = field(default_factory=dict)\n",
        "    img_size: int = 224; max_text_len: int = 77;\n",
        "    num_samples: Optional[int] = 32 # Small num_samples for testing with multiple datasets\n",
        "    num_workers: int = 0\n",
        "    image_key: str = 'image'; caption_key: str = 'caption'; audio_key: str = 'audio'\n",
        "    audio_target_sr: int = 16000; audio_max_duration_s: int = 30\n",
        "\n",
        "    # Checkpointing\n",
        "    checkpoint_dir: str = './checkpoints_all_pairs_final'; save_every_n_epochs: int = 1\n",
        "    load_from_checkpoint: Optional[str] = None; checkpoint_load_strict: bool = False\n",
        "\n",
        "    # Misc\n",
        "    seed: int = 42\n",
        "    max_steps_per_epoch_override: Optional[int] = None\n",
        "    default_steps_for_iterable_epoch: int = 50\n",
        "    share_gradio_demo: bool = False\n",
        "    debug_mode: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        print(f\"INFO: {CELL_INFO_PREFIX} Running ModelConfig.__post_init__ validation and setup...\")\n",
        "        if self.geometries_to_use is None: self.geometries_to_use = []\n",
        "        if self.geometry_dims is None: self.geometry_dims = {}\n",
        "        original_geoms_to_use = list(self.geometries_to_use); self.geometries_to_use = [g for g in self.geometries_to_use if g in self.geometry_dims]\n",
        "        if len(self.geometries_to_use) != len(original_geoms_to_use): print(f\"    INFO: Filtered geometries_to_use. Original: {original_geoms_to_use}, Validated: {self.geometries_to_use}\")\n",
        "        if not self.geometries_to_use:\n",
        "            default_geom_options = [g for g in ['E', 'S', 'H'] if g in self.geometry_dims]\n",
        "            if default_geom_options: self.geometries_to_use = [default_geom_options[0]]; print(f\"    WARNING: 'geometries_to_use' defaulted to ['{self.geometries_to_use[0]}']\")\n",
        "            else: print(f\"    CRITICAL WARNING: 'geometries_to_use' is empty. MixtureEmbedding might not function.\")\n",
        "        for modality_prefix in ['image', 'text', 'audio']:\n",
        "            proj_layers_attr = f\"{modality_prefix}_encoder_projection_layers\"; num_layers = getattr(self, proj_layers_attr, 1)\n",
        "            if num_layers < 1: print(f\"    WARNING: {proj_layers_attr} ({num_layers}) < 1. Setting to 1.\"); setattr(self, proj_layers_attr, 1)\n",
        "        if self.pair_fusion_method == 'mlp_on_concat' and self.pair_fusion_mlp_hidden_layers < 1:\n",
        "                 print(f\"    WARNING: pair_fusion_mlp_hidden_layers ({self.pair_fusion_mlp_hidden_layers}) for 'mlp_on_concat' < 1. Setting to 1.\"); self.pair_fusion_mlp_hidden_layers = 1\n",
        "        if 'google.colab' in sys.modules and self.num_workers > 0: print(f\"    INFO: Colab detected, setting num_workers=0.\"); self.num_workers = 0\n",
        "        if self.active_training_pairs is None: self.active_training_pairs = {}\n",
        "        self._update_audio_encoder_default()\n",
        "        print(f\"    --- Key Architectural Choices from Config ---\")\n",
        "        print(f\"        Image Enc Proj: Layers={self.image_encoder_projection_layers}, UseLN={self.image_encoder_use_layernorm}\")\n",
        "        print(f\"        Text Enc Proj: Layers={self.text_encoder_projection_layers}, UseLN={self.text_encoder_use_layernorm},Pool='{self.text_encoder_pooling_strategy}'\")\n",
        "        if self.audio_encoder_name: print(f\"        Audio Enc Proj: Layers={self.audio_encoder_projection_layers},UseLN={self.audio_encoder_use_layernorm}\")\n",
        "        else: print(f\"        Audio Encoder: Not configured.\")\n",
        "        print(f\"        Pair Fusion: '{self.pair_fusion_method}'\" + (f\" (MLP HiddenL: {self.pair_fusion_mlp_hidden_layers})\" if self.pair_fusion_method == 'mlp_on_concat' else \"\"))\n",
        "        print(f\"    --- End Key Architectural Choices ---\")\n",
        "\n",
        "    def _update_audio_encoder_default(self):\n",
        "        audio_pair_active = any(isinstance(p, DatasetPairConfig) and (p.modality1_type == 'audio' or p.modality2_type == 'audio') for p in self.active_training_pairs.values())\n",
        "        if audio_pair_active and (self.audio_encoder_name is None or self.audio_encoder_name.strip() == \"\"):\n",
        "            self.audio_encoder_name = \"openai/whisper-tiny\"; print(f\"    INFO: _update_audio_encoder_default: Auto-set config.audio_encoder_name to '{self.audio_encoder_name}'.\")\n",
        "\n",
        "# --- Instantiate Global Config ---\n",
        "config = ModelConfig(\n",
        "    num_epochs=10, batch_size=2, num_samples=8, log_steps=1, max_steps_per_epoch_override=4,\n",
        "    use_amp=False,\n",
        "    checkpoint_dir=f'./checkpoints_cell3_full_active_{time.strftime(\"%Y%m%d%H%M\")}',\n",
        "    image_key='image', caption_key='caption_0', audio_key='audio'\n",
        ")\n",
        "print(f\"    {CELL_INFO_PREFIX} Initial 'config' object created. 'active_training_pairs' will be populated next.\")\n",
        "\n",
        "# --- Code-Based Configuration for `active_training_pairs` ---\n",
        "print(f\"\\n{CELL_INFO_PREFIX} --- Defining 'config.active_training_pairs' via Python Code ---\")\n",
        "print(f\"    Instructions: Review/edit 'active_pairs_definitions' to set your datasets.\")\n",
        "print(f\"    IMPORTANT FOR GATED DATASETS: Ensure Hugging Face Hub authentication and terms acceptance.\")\n",
        "\n",
        "active_pairs_definitions = [\n",
        "    {\n",
        "        \"pair_id_key\": \"IT_Flickr8k_jxie_Train\",\n",
        "        \"config_data\": DatasetPairConfig(\n",
        "            dataset_id='jxie/flickr8k',\n",
        "            modality1_key='image', modality1_type='image',\n",
        "            modality2_key='caption_0', modality2_type='text',\n",
        "            split='train', streaming=False\n",
        "        ),\n",
        "        \"notes\": \"Image-Text: jxie/flickr8k (train). Uses 'caption_0'.\"\n",
        "    },\n",
        "    {\n",
        "        \"pair_id_key\": \"IT_Flickr8k_jxie_Val\",\n",
        "        \"config_data\": DatasetPairConfig(\n",
        "            dataset_id='jxie/flickr8k',\n",
        "            modality1_key='image', modality1_type='image',\n",
        "            modality2_key='caption_0', modality2_type='text',\n",
        "            split='validation', # Or 'test' - use get_hf_dataset_suggestions to confirm split name\n",
        "            streaming=False\n",
        "        ),\n",
        "        \"notes\": \"Image-Text: jxie/flickr8k (validation). Uses 'caption_0'.\"\n",
        "    },\n",
        "    {\n",
        "        \"pair_id_key\": \"CodeReasoning_S0_Train\",\n",
        "        \"config_data\": DatasetPairConfig(\n",
        "            dataset_id='nvidia/OpenCodeReasoning',\n",
        "            dataset_config_name='split_0',\n",
        "            modality1_key='input', modality1_type='text',\n",
        "            modality2_key='solution', modality2_type='text',\n",
        "            split='split_0',\n",
        "            streaming=True\n",
        "        ),\n",
        "        \"notes\": \"Text-Code: nvidia/OpenCodeReasoning ('split_0'). Streaming enabled.\"\n",
        "    },\n",
        "    # { # Example for a validation split of OpenCodeReasoning if it existed or if you want to use a subset of split_1\n",
        "    #     \"pair_id_key\": \"CodeReasoning_S1_Val\",\n",
        "    #     \"config_data\": DatasetPairConfig(\n",
        "    #         dataset_id='nvidia/OpenCodeReasoning',\n",
        "    #         dataset_config_name='split_1',\n",
        "    #         modality1_key='input', modality1_type='text',\n",
        "    #         modality2_key='solution', modality2_type='text',\n",
        "    #         split='split_1',\n",
        "    #         streaming=True\n",
        "    #     ),\n",
        "    #     \"notes\": \"Text-Code: nvidia/OpenCodeReasoning ('split_1') as validation example. Streaming enabled.\"\n",
        "    # },\n",
        "    {\n",
        "        \"pair_id_key\": \"AT_Emilia_EN_Train\", # GATED DATASET\n",
        "        \"config_data\": DatasetPairConfig(\n",
        "            dataset_id='amphion/Emilia-Dataset',\n",
        "            dataset_config_name='EN', # Signal to Cell 4 to attempt to load English\n",
        "            data_files_glob_pattern=\"Emilia/EN/**/*.tar\", # For Cell 4 to construct data_files if needed\n",
        "            modality1_key='mp3',\n",
        "            modality1_type='audio',\n",
        "            modality2_key='json.text',\n",
        "            modality2_type='text',\n",
        "            split='train',\n",
        "            streaming=True\n",
        "        ),\n",
        "        \"notes\": \"Audio-Text: amphion/Emilia-Dataset (English). GATED & LARGE. Streaming enabled.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "config.active_training_pairs = {}\n",
        "for pair_def in active_pairs_definitions:\n",
        "    if pair_def.get(\"config_data\") and isinstance(pair_def[\"config_data\"], DatasetPairConfig):\n",
        "        config.active_training_pairs[pair_def[\"pair_id_key\"]] = pair_def[\"config_data\"]\n",
        "    else: print(f\"WARNING: {CELL_INFO_PREFIX} Skipping invalid entry: {pair_def.get('pair_id_key', 'Unknown')}\")\n",
        "\n",
        "print(f\"\\n{CELL_INFO_PREFIX} --- Final 'config.active_training_pairs' (Programmatically Set) ---\")\n",
        "if config.active_training_pairs:\n",
        "    for name, pair_conf in config.active_training_pairs.items():\n",
        "        details = asdict(pair_conf); print(f\"    Pair ID: '{name}'\")\n",
        "        for k, v in details.items(): print(f\"        {k}: {repr(v)}\")\n",
        "else: print(\"    INFO: No active training pairs defined. Edit 'active_pairs_definitions'.\")\n",
        "\n",
        "config.__post_init__()\n",
        "\n",
        "def get_hf_dataset_suggestions(dataset_id_str: str, config_name_str: Optional[str] = None, trust_remote_code_ds: bool = True):\n",
        "    # (Full helper function as provided in the previous response)\n",
        "    print(f\"\\n--- Exploring Dataset: '{dataset_id_str}'{f' (Config: {config_name_str})' if config_name_str else ''} ---\")\n",
        "    if not dataset_id_str: print(f\"ERROR: {CELL_INFO_PREFIX} Dataset ID is required.\"); return\n",
        "    html_output = \"\"; display_handle_id = f\"disp_info_{random.randint(1000,9999)}\"\n",
        "    try:\n",
        "        token = os.getenv(\"HF_TOKEN\") or HfFolder.get_token()\n",
        "        if not token: html_output += \"<p style='color:orange;'><i>Note: HF_TOKEN not set & not logged in. Access to gated/private datasets may fail. Consider `huggingface-cli login` or `from huggingface_hub import notebook_login; notebook_login()`.</i></p>\"\n",
        "        display(HTML(html_output + f\"<p id='{display_handle_id}'><i>Fetching info...</i></p>\"), display_id=display_handle_id)\n",
        "        info = hf_dataset_info(dataset_id_str, name=config_name_str, token=token, trust_remote_code=trust_remote_code)\n",
        "        html_output += f\"<p style='color:green;font-weight:bold;'>Successfully fetched info for '<b>{dataset_id_str}</b>'{f' (config: {config_name_str})' if config_name_str else ''}.</p>\"\n",
        "        if hasattr(info, 'description') and info.description: html_output += f\"<b>Description:</b> {info.description[:300]}...<br/>\"\n",
        "        available_configs = []; available_splits = []\n",
        "        if hasattr(info, 'config_names') and info.config_names: available_configs = sorted(info.config_names); html_output += f\"<p><b>Available Dataset Configurations:</b> {available_configs}</p>\"\n",
        "        if not config_name_str and available_configs: html_output += f\"<p><i>Hint: Re-run with `config_name_str='{available_configs[0]}'` to see its specific splits/features.</i></p>\"\n",
        "        info_to_show = info\n",
        "        if config_name_str and hasattr(info, 'config_splits') and isinstance(info.config_splits, dict) and config_name_str in info.config_splits :\n",
        "             try: info_to_show = hf_dataset_info(dataset_id_str, name=config_name_str, token=token, trust_remote_code=trust_remote_code)\n",
        "             except: pass\n",
        "        if hasattr(info_to_show, 'splits') and info_to_show.splits and isinstance(info_to_show.splits, dict): available_splits = sorted([str(k) for k in info_to_show.splits.keys()])\n",
        "        if not available_splits: available_splits = ['train']\n",
        "        html_output += f\"<p><b>Suggested Splits (for '{config_name_str or 'default'}'):</b> {available_splits}</p>\"\n",
        "        features_dict = getattr(info_to_show, 'features', {})\n",
        "        feature_keys = list(features_dict.keys()) if isinstance(features_dict, dict) else []\n",
        "        html_output += f\"<p><b>Features (for '{config_name_str or 'default'}', first available split '{available_splits[0] if available_splits else 'N/A'}'):</b></p>\"\n",
        "        if feature_keys: html_output += \"<ul>\"; html_output += \"\".join([f\"<li><b>'{fk}'</b>: {type(fv).__name__} (<code>{str(fv)[:150]}</code>)</li>\" for fk,fv in features_dict.items()]); html_output += \"</ul>\"\n",
        "        else: html_output += \"<p>Could not determine features for this config/split.</p>\"\n",
        "        display(HTML(html_output), display_id=display_handle_id, update=True)\n",
        "    except DatasetNotFoundError:\n",
        "        err_msg = f\"<p style='color:red;font-weight:bold;'>ERROR: Dataset '{dataset_id_str}'{f' (config {config_name_str})' if config_name_str else ''} not found or requires access. Check ID, config name, and HF Hub authentication (accept terms on dataset page & ensure login/token).</p>\"\n",
        "        display(HTML(html_output + err_msg), display_id=display_handle_id, update=True) if 'display_handle_id' in locals() else print(err_msg.replace(\"<p style='color:red;font-weight:bold;'>\",\"\").replace(\"</p>\",\"\"))\n",
        "    except Exception as e:\n",
        "        err_msg = f\"<p style='color:red;font-weight:bold;'>ERROR: Unexpected error for '{dataset_id_str}': {type(e).__name__} - {e}</p><pre>{traceback.format_exc()}</pre>\"\n",
        "        display(HTML(html_output + err_msg), display_id=display_handle_id, update=True) if 'display_handle_id' in locals() else print(err_msg.replace(\"<p style='color:red;font-weight:bold;'>\",\"\").replace(\"</p>\",\"\").replace(\"<pre>\",\"\\n\").replace(\"</pre>\",\"\"))\n",
        "    print(\"\\n--- End of Dataset Exploration ---\")\n",
        "\n",
        "# --- Seed Everything ---\n",
        "if 'seed_everything' not in globals():\n",
        "    print(f\"INFO: {CELL_INFO_PREFIX} 'seed_everything' function not found. Defining.\")\n",
        "    def seed_everything(seed_val=42):\n",
        "        random.seed(seed_val); os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
        "        np.random.seed(seed_val); torch.manual_seed(seed_val)\n",
        "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed_val)\n",
        "        print(f\"    INFO: {CELL_INFO_PREFIX} Seed set to {seed_val}.\")\n",
        "seed_everything(config.seed)\n",
        "\n",
        "CURRENT_TIME_END_CELL3 = time.time()\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL3))}] {CELL_INFO_PREFIX} Code-Based Configuration setup complete. Duration: {CURRENT_TIME_END_CELL3 - CURRENT_TIME_START_CELL3:.2f}s\")\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] ==> REVIEW/MODIFY 'active_pairs_definitions' in THIS CELL. Then, run Cell 4 (DataLoaders).\")\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] ==> To explore datasets, call 'get_hf_dataset_suggestions(\\\"dataset_id\\\")' in a new cell.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "T-ardrxuMm_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4"
      ],
      "metadata": {
        "id": "uxzVQC1yRQxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 4: Data Handling (Pairwise Training - Dynamic, Robust, Granular, Streaming-Aware, Multi-Split) ---\n",
        "# Defines dataset classes (Map-style and Iterable-style), transformations, collation,\n",
        "# and DataLoaders for modality pairs, using 'active_training_pairs' from Cell 3.\n",
        "# Handles train/val/test splits, audio column casting, nested key access,\n",
        "# and robust loading for diverse dataset structures, including streaming for large datasets.\n",
        "# CORRECTED: Changed `if dataloader_instance:` to `if dataloader_instance is not None:` in main loop.\n",
        "# RE-VERIFIED: DataLoader instantiation for IterableStylePairDataset uses minimal explicit arguments.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset as PyTorchIterableDataset\n",
        "import torchvision.transforms as T\n",
        "from transformers import WhisperProcessor\n",
        "from datasets import load_dataset, Audio as DatasetsAudioModule, Features, Value, Sequence, IterableDataset as HFIterableDataset\n",
        "from datasets.exceptions import DatasetNotFoundError\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any, Callable\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "import io\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as AT\n",
        "import librosa\n",
        "\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "CELL4_INFO_PREFIX = \"[Cell 4]\"\n",
        "DEBUG_CELL4 = getattr(config, 'debug_mode', True) if 'config' in globals() else True\n",
        "\n",
        "# --- CRITICAL CHECK: Ensure config and DEVICE are available ---\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL4_INFO_PREFIX} Verifying prerequisites...\")\n",
        "if 'config' not in globals(): raise NameError(f\"FATAL: {CELL4_INFO_PREFIX} 'config' not defined. Run Cell 3.\")\n",
        "if 'DEVICE' not in globals(): raise NameError(f\"FATAL: {CELL4_INFO_PREFIX} 'DEVICE' not defined. Run Cell 1.\")\n",
        "if 'DatasetPairConfig' not in globals(): raise NameError(f\"FATAL: {CELL4_INFO_PREFIX} 'DatasetPairConfig' class not defined. Run Cell 3.\")\n",
        "if not hasattr(config, 'active_training_pairs'): raise AttributeError(f\"FATAL: {CELL4_INFO_PREFIX} 'config' has no 'active_training_pairs'. Define in Cell 3.\")\n",
        "for attr_check in ['num_samples', 'seed', 'audio_target_sr', 'audio_max_duration_s', 'img_size', 'num_workers', 'batch_size']:\n",
        "    if not hasattr(config, attr_check):\n",
        "        print(f\"WARNING: {CELL4_INFO_PREFIX} config.{attr_check} not found. Using a default or expect error if not set in ModelConfig.\")\n",
        "        if attr_check == 'num_samples': setattr(config, attr_check, None)\n",
        "        elif attr_check == 'seed': setattr(config, attr_check, 42)\n",
        "        elif attr_check == 'audio_target_sr': setattr(config, attr_check, 16000)\n",
        "        elif attr_check == 'audio_max_duration_s': setattr(config, attr_check, 30)\n",
        "        elif attr_check == 'img_size': setattr(config, attr_check, 224)\n",
        "        elif attr_check == 'num_workers': setattr(config, attr_check, 0)\n",
        "        elif attr_check == 'batch_size': setattr(config, attr_check, 2)\n",
        "print(f\"    {CELL4_INFO_PREFIX} Prerequisites verified.\")\n",
        "\n",
        "CURRENT_TIME_START_CELL4 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL4))}] {CELL4_INFO_PREFIX} Defining Data Handling Logic (Streaming Aware)...\")\n",
        "print(f\"    {CELL4_INFO_PREFIX} Global Config: img_size={config.img_size}, num_samples={config.num_samples}, audio_SR={config.audio_target_sr}\")\n",
        "try:\n",
        "    _hf_token_check = HfFolder.get_token(); print(f\"    {CELL4_INFO_PREFIX} HF Token via HfFolder: {'Available' if _hf_token_check else 'Not Found'}\")\n",
        "    if os.getenv(\"HF_TOKEN\"): print(f\"    {CELL4_INFO_PREFIX} HF_TOKEN env var: SET.\")\n",
        "    elif _hf_token_check is None : print(f\"    {CELL4_INFO_PREFIX} Note: No global HF token by HfFolder & HF_TOKEN env var not set. Gated dataset access requires login/token.\")\n",
        "except Exception as e_token_check: print(f\"    WARNING: {CELL4_INFO_PREFIX} Could not check Hugging Face token: {e_token_check}.\")\n",
        "\n",
        "def _get_nested_item(data_dict: Dict, key_path: str, default_val: Any = None) -> Any:\n",
        "    keys = key_path.split('.'); val = data_dict\n",
        "    try:\n",
        "        for key in keys:\n",
        "            if isinstance(val, dict): val = val[key]\n",
        "            else: return default_val\n",
        "        return val\n",
        "    except (KeyError, TypeError, IndexError): return default_val\n",
        "print(f\"    {CELL4_INFO_PREFIX} _get_nested_item helper defined.\")\n",
        "\n",
        "def get_image_transforms(img_size: int = 224, is_train: bool = True) -> T.Compose:\n",
        "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    if is_train: return T.Compose([T.RandomResizedCrop(img_size, scale=(0.75, 1.0), ratio=(0.75, 1.333), interpolation=T.InterpolationMode.BICUBIC), T.RandomHorizontalFlip(), T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), T.ToTensor(), normalize])\n",
        "    else: return T.Compose([T.Resize(img_size + 32 if img_size < 256 else img_size, interpolation=T.InterpolationMode.BICUBIC), T.CenterCrop(img_size), T.ToTensor(), normalize])\n",
        "print(f\"    {CELL4_INFO_PREFIX} get_image_transforms defined.\")\n",
        "\n",
        "def get_audio_processor(processor_config_name: Optional[str], target_sr: int = 16000, max_duration_s: int = 30) -> Optional[Callable[[Any, Optional[int]], Optional[torch.Tensor]]]:\n",
        "    func_prefix_ap = f\"{CELL4_INFO_PREFIX}[get_audio_processor]\"\n",
        "    if not processor_config_name: print(f\"INFO: {func_prefix_ap} Audio processing disabled.\"); return None\n",
        "    try:\n",
        "        processor = WhisperProcessor.from_pretrained(processor_config_name)\n",
        "        print(f\"    INFO: {func_prefix_ap} WhisperProcessor for '{processor_config_name}' loaded (Target SR: {target_sr}Hz, Max Dur: {max_duration_s}s).\")\n",
        "        def process_audio_fn(audio_input: Any, input_sampling_rate: Optional[int] = None) -> Optional[torch.Tensor]:\n",
        "            raw_audio_waveform: Optional[np.ndarray] = None; current_sr: Optional[int] = input_sampling_rate\n",
        "            if isinstance(audio_input, str):\n",
        "                try: waveform, sr_load = torchaudio.load(audio_input); raw_audio_waveform = waveform.numpy(); current_sr = sr_load if current_sr is None else current_sr\n",
        "                except Exception as e: print(f\"ERROR: {func_prefix_ap} Load audio '{audio_input}': {e}\"); return None\n",
        "            elif isinstance(audio_input, dict) and 'array' in audio_input and 'sampling_rate' in audio_input: raw_audio_waveform = np.array(audio_input['array'],dtype=np.float32); current_sr = audio_input['sampling_rate'] if current_sr is None else current_sr\n",
        "            elif isinstance(audio_input, dict) and 'bytes' in audio_input:\n",
        "                 try: waveform, sr_load = torchaudio.load(io.BytesIO(audio_input['bytes'])); raw_audio_waveform = waveform.numpy(); current_sr = sr_load if current_sr is None else current_sr\n",
        "                 except Exception as e: print(f\"ERROR: {func_prefix_ap} Load audio from bytes: {e}\"); return None\n",
        "            elif isinstance(audio_input, (np.ndarray, torch.Tensor)): raw_audio_waveform = audio_input.numpy().astype(np.float32) if isinstance(audio_input, torch.Tensor) else audio_input.astype(np.float32)\n",
        "            else: print(f\"ERROR: {func_prefix_ap} Unsup. audio type: {type(audio_input)}\"); return None\n",
        "            if raw_audio_waveform is None: return None\n",
        "            if current_sr is None: current_sr = target_sr; print(f\"WARNING: {func_prefix_ap} Input SR unknown, assuming {target_sr}Hz.\")\n",
        "            if raw_audio_waveform.ndim > 1: raw_audio_waveform = np.mean(raw_audio_waveform, axis=0) if raw_audio_waveform.shape[0] < raw_audio_waveform.shape[1] and raw_audio_waveform.shape[0] <=2 else np.mean(raw_audio_waveform, axis=-1)\n",
        "            if raw_audio_waveform.ndim > 1 and raw_audio_waveform.shape[0]==1: raw_audio_waveform = raw_audio_waveform.squeeze(0)\n",
        "            elif raw_audio_waveform.ndim > 1 : print(f\"WARNING: {func_prefix_ap} Audio not 1D after mono: {raw_audio_waveform.shape}.\"); raw_audio_waveform = raw_audio_waveform.flatten()[:int(target_sr*max_duration_s*1.1)]\n",
        "            if current_sr != target_sr:\n",
        "                try: raw_audio_waveform = librosa.resample(raw_audio_waveform, orig_sr=current_sr, target_sr=target_sr)\n",
        "                except Exception as e: print(f\"ERROR: {func_prefix_ap} Resample failed: {e}\"); return None\n",
        "            max_samples = int(max_duration_s * target_sr); current_samples = raw_audio_waveform.shape[-1]\n",
        "            if current_samples > max_samples: raw_audio_waveform = raw_audio_waveform[..., :max_samples]\n",
        "            elif current_samples < max_samples: raw_audio_waveform = np.pad(raw_audio_waveform, (0, max_samples - current_samples), mode='constant')\n",
        "            try:\n",
        "                processed = processor(raw_audio_waveform, sampling_rate=target_sr, return_tensors=\"pt\"); features = processed.input_features\n",
        "                return features.squeeze(0) if features.ndim == 3 and features.shape[0] == 1 else features\n",
        "            except Exception as e: print(f\"ERROR: {func_prefix_ap} WhisperProcessor failed: {e}\\n{traceback.format_exc()}\"); return None\n",
        "        return process_audio_fn\n",
        "    except Exception as e: print(f\"ERROR: {func_prefix_ap} Failed to load processor '{processor_config_name}': {e}\\n{traceback.format_exc()}\"); return None\n",
        "print(f\"    {CELL4_INFO_PREFIX} get_audio_processor defined.\")\n",
        "\n",
        "def _process_modality_data(data_item: Any, modality_type: str, modality_key: str, image_transform: Optional[Callable], audio_processor_fn: Optional[Callable], target_sr_for_audio_dict: int):\n",
        "    func_prefix_pmd = f\"{CELL4_INFO_PREFIX}[_process_modality_data:{modality_key}]\"\n",
        "    if data_item is None: return None\n",
        "    try:\n",
        "        if modality_type == 'image':\n",
        "            img = data_item\n",
        "            if not isinstance(img, Image.Image):\n",
        "                if isinstance(img, str): img = Image.open(img) if os.path.exists(img) else None\n",
        "                elif isinstance(img, bytes): img = Image.open(io.BytesIO(img))\n",
        "                elif isinstance(img, (torch.Tensor, np.ndarray)): img = T.ToPILImage()(img.squeeze(0)) if isinstance(img, torch.Tensor) and img.ndim == 4 else (T.ToPILImage()(img) if isinstance(img, torch.Tensor) else Image.fromarray(img))\n",
        "                else: print(f\"WARNING: {func_prefix_pmd} Unhandled image type: {type(img)}\"); return None\n",
        "            if img is None: print(f\"WARNING: {func_prefix_pmd} Image data became None for key '{modality_key}'.\"); return None\n",
        "            if img.mode != 'RGB': img = img.convert('RGB')\n",
        "            return image_transform(img) if image_transform else img\n",
        "        elif modality_type == 'text':\n",
        "            if isinstance(data_item, list): return next((s for s in data_item if isinstance(s, str) and s.strip()),\"\").strip()\n",
        "            return str(data_item).strip()\n",
        "        elif modality_type == 'audio':\n",
        "            if audio_processor_fn:\n",
        "                if isinstance(data_item, dict) and 'array' in data_item and 'sampling_rate' in data_item: return audio_processor_fn(data_item, data_item['sampling_rate'])\n",
        "                elif isinstance(data_item, dict) and 'bytes' in data_item: return audio_processor_fn(data_item, target_sr_for_audio_dict)\n",
        "                elif isinstance(data_item, (str, np.ndarray, torch.Tensor)): return audio_processor_fn(data_item, None)\n",
        "                else: print(f\"WARNING: {func_prefix_pmd} Unhandled audio type: {type(data_item)}\"); return None\n",
        "            return data_item\n",
        "        return data_item\n",
        "    except UnidentifiedImageError:\n",
        "        print(f\"ERROR: {func_prefix_pmd} UnidentifiedImageError for '{modality_key}'.\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: {func_prefix_pmd} FileNotFoundError for '{modality_key}' (path: {data_item if isinstance(data_item, str) else 'N/A'}).\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {func_prefix_pmd} Processing '{modality_type}' failed: {e}\")\n",
        "        if DEBUG_CELL4:\n",
        "            print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "print(f\"    {CELL4_INFO_PREFIX} _process_modality_data helper defined.\")\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, hf_dataset: datasets.Dataset, pair_config: DatasetPairConfig, global_config: ModelConfig, image_transform: Optional[Callable] = None, audio_processor_fn: Optional[Callable] = None):\n",
        "        self.hf_dataset, self.p_cfg, self.g_cfg = hf_dataset, pair_config, global_config\n",
        "        self.image_transform, self.audio_processor_fn = image_transform, audio_processor_fn\n",
        "        self.m1k, self.m1t = self.p_cfg.modality1_key, self.p_cfg.modality1_type.lower()\n",
        "        self.m2k, self.m2t = self.p_cfg.modality2_key, self.p_cfg.modality2_type.lower()\n",
        "        if hasattr(self.hf_dataset, 'features') and self.hf_dataset.features:\n",
        "            dataset_features = self.hf_dataset.features\n",
        "            if self.m1k.split('.')[0] not in dataset_features: raise ValueError(f\"M1 key '{self.m1k}' (top: {self.m1k.split('.')[0]}) not in features: {list(dataset_features.keys())} for {self.p_cfg.dataset_id}\")\n",
        "            if self.m2k.split('.')[0] not in dataset_features: raise ValueError(f\"M2 key '{self.m2k}' (top: {self.m2k.split('.')[0]}) not in features: {list(dataset_features.keys())} for {self.p_cfg.dataset_id}\")\n",
        "        print(f\"    {CELL4_INFO_PREFIX}[PairDataset] Initialized for {self.p_cfg.dataset_id} (Split: {self.p_cfg.split}, Config: {self.p_cfg.dataset_config_name or 'default'}): {self.m1t}('{self.m1k}')<->{self.m2t}('{self.m2k}'). Items: {len(self.hf_dataset)}\")\n",
        "    def __len__(self): return len(self.hf_dataset)\n",
        "    def __getitem__(self, idx: int) -> Optional[Tuple[Any, Any]]:\n",
        "        try:\n",
        "            item_dict = self.hf_dataset[idx]\n",
        "            if not isinstance(item_dict, dict): return None\n",
        "            raw_data1, raw_data2 = _get_nested_item(item_dict, self.m1k), _get_nested_item(item_dict, self.m2k)\n",
        "            if raw_data1 is None and self.m1t != \"placeholder\": return None\n",
        "            if raw_data2 is None and self.m2t != \"placeholder\": return None\n",
        "            processed_data1 = _process_modality_data(raw_data1, self.m1t, self.m1k, self.image_transform, self.audio_processor_fn, self.g_cfg.audio_target_sr)\n",
        "            processed_data2 = _process_modality_data(raw_data2, self.m2t, self.m2k, self.image_transform, self.audio_processor_fn, self.g_cfg.audio_target_sr)\n",
        "            if (processed_data1 is None and self.m1t != \"placeholder\") or \\\n",
        "               (processed_data2 is None and self.m2t != \"placeholder\"): return None\n",
        "            return (processed_data1 if processed_data1 is not None else \"N/A_PLACEHOLDER_M1\",\n",
        "                    processed_data2 if processed_data2 is not None else \"N/A_PLACEHOLDER_M2\")\n",
        "        except IndexError:\n",
        "            print(f\"ERROR: {CELL4_INFO_PREFIX}[PairDataset] IndexError for {idx}. Len: {len(self.hf_dataset)}.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {CELL4_INFO_PREFIX}[PairDataset] __getitem__ idx {idx} for {self.p_cfg.dataset_id}: {e}\")\n",
        "            if DEBUG_CELL4:\n",
        "                print(traceback.format_exc())\n",
        "            return None\n",
        "\n",
        "print(f\"    {CELL4_INFO_PREFIX} PairDataset (Map-style) class defined.\")\n",
        "\n",
        "class IterableStylePairDataset(PyTorchIterableDataset):\n",
        "    def __init__(self, hf_iterable_dataset: HFIterableDataset, pair_config: DatasetPairConfig,\n",
        "                 global_config: ModelConfig, image_transform: Optional[Callable] = None,\n",
        "                 audio_processor_fn: Optional[Callable] = None):\n",
        "        self.hf_iterable_dataset = hf_iterable_dataset\n",
        "        self.p_cfg = pair_config; self.g_cfg = global_config\n",
        "        self.image_transform = image_transform; self.audio_processor_fn = audio_processor_fn\n",
        "        self.m1k, self.m1t = self.p_cfg.modality1_key, self.p_cfg.modality1_type.lower()\n",
        "        self.m2k, self.m2t = self.p_cfg.modality2_key, self.p_cfg.modality2_type.lower()\n",
        "        print(f\"    {CELL4_INFO_PREFIX}[IterablePairDataset] Initialized for {self.p_cfg.dataset_id} (Split: {self.p_cfg.split}, Config: {self.p_cfg.dataset_config_name or 'default'}): Streaming mode.\")\n",
        "    def __iter__(self):\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        iterable_ds_to_use = self.hf_iterable_dataset\n",
        "        if worker_info is not None:\n",
        "            if hasattr(iterable_ds_to_use, 'shard'):\n",
        "                 iterable_ds_to_use = iterable_ds_to_use.shard(num_shards=worker_info.num_workers, index=worker_info.id)\n",
        "                 if DEBUG_CELL4: print(f\"DEBUG: {CELL4_INFO_PREFIX}[IterablePairDataset] Worker {worker_info.id}/{worker_info.num_workers} processing its shard.\")\n",
        "            elif DEBUG_CELL4: print(f\"DEBUG: {CELL4_INFO_PREFIX}[IterablePairDataset] Worker {worker_info.id}/{worker_info.num_workers}. Dataset does not have .shard().\")\n",
        "        for item_dict in iterable_ds_to_use:\n",
        "            try:\n",
        "                if not isinstance(item_dict, dict): continue\n",
        "                raw_data1 = _get_nested_item(item_dict, self.m1k); raw_data2 = _get_nested_item(item_dict, self.m2k)\n",
        "                if (raw_data1 is None and self.m1t != \"placeholder\") or (raw_data2 is None and self.m2t != \"placeholder\"): continue\n",
        "                processed_data1 = _process_modality_data(raw_data1, self.m1t, self.m1k, self.image_transform, self.audio_processor_fn, self.g_cfg.audio_target_sr)\n",
        "                processed_data2 = _process_modality_data(raw_data2, self.m2t, self.m2k, self.image_transform, self.audio_processor_fn, self.g_cfg.audio_target_sr)\n",
        "                if (processed_data1 is not None or self.m1t == \"placeholder\") and (processed_data2 is not None or self.m2t == \"placeholder\"):\n",
        "                    yield (processed_data1 if processed_data1 is not None else \"N/A_PLACEHOLDER_M1\", processed_data2 if processed_data2 is not None else \"N/A_PLACEHOLDER_M2\")\n",
        "            except Exception as e_iter:\n",
        "                print(f\"ERROR: {CELL4_INFO_PREFIX}[IterablePairDataset] Iteration error for {self.p_cfg.dataset_id}: {e_iter}\")\n",
        "                if DEBUG_CELL4:\n",
        "                    print(traceback.format_exc())\n",
        "                continue\n",
        "\n",
        "print(f\"    {CELL4_INFO_PREFIX} IterableStylePairDataset class defined.\")\n",
        "\n",
        "def pairwise_collate_fn(batch: List[Optional[Tuple[Any, Any]]]) -> Optional[Tuple[Any, Any]]:\n",
        "    valid_items = [item for item in batch if item is not None and isinstance(item, tuple) and len(item) == 2]\n",
        "    if not valid_items:\n",
        "        if DEBUG_CELL4: print(f\"DEBUG: {CELL4_INFO_PREFIX}[collate_fn] Batch has no valid items. Returning None.\")\n",
        "        return None\n",
        "    m1_items, m2_items = zip(*valid_items)\n",
        "    def _collate_mod(items_list):\n",
        "        if not items_list: return None\n",
        "        first_item = items_list[0]\n",
        "        if isinstance(first_item, torch.Tensor):\n",
        "            if all(isinstance(it, torch.Tensor) and hasattr(it, 'shape') and it.shape == first_item.shape for it in items_list):\n",
        "                try: return torch.stack(items_list)\n",
        "                except Exception as e_stack:\n",
        "                     if DEBUG_CELL4: print(f\"DEBUG: {CELL4_INFO_PREFIX}[collate_fn] torch.stack failed (shape: {first_item.shape}): {e_stack}. Ret. list.\")\n",
        "                     return list(items_list)\n",
        "            else:\n",
        "                if DEBUG_CELL4: print(f\"DEBUG: {CELL4_INFO_PREFIX}[collate_fn] Tensors have varying shapes. Returning as list.\")\n",
        "                return list(items_list)\n",
        "        return list(items_list)\n",
        "    collated_m1, collated_m2 = _collate_mod(m1_items), _collate_mod(m2_items)\n",
        "    if collated_m1 is None or collated_m2 is None: return None\n",
        "    return collated_m1, collated_m2\n",
        "print(f\"    {CELL4_INFO_PREFIX} pairwise_collate_fn defined.\")\n",
        "\n",
        "def get_dataloader_for_pair(\n",
        "    pair_id_key: str, p_cfg: DatasetPairConfig, g_cfg: ModelConfig,\n",
        "    image_transform_train: Callable, image_transform_eval: Callable,\n",
        "    audio_processor_fn: Optional[Callable]\n",
        ") -> Optional[DataLoader]:\n",
        "    func_prefix_gdl = f\"{CELL4_INFO_PREFIX}[get_dataloader:{pair_id_key}]\"\n",
        "    print(f\"INFO: {func_prefix_gdl} Creating DataLoader...\")\n",
        "    print(f\"    Dataset ID: '{p_cfg.dataset_id}', Config Name: '{p_cfg.dataset_config_name}', Split: '{p_cfg.split}'\")\n",
        "    if p_cfg.data_files_glob_pattern: print(f\"    Data Files Glob: '{p_cfg.data_files_glob_pattern}'\")\n",
        "\n",
        "    hf_dataset_obj = None;\n",
        "    is_streaming = getattr(p_cfg, 'streaming', False)\n",
        "    if p_cfg.dataset_id == 'amphion/Emilia-Dataset' and not is_streaming:\n",
        "        is_streaming = True; print(f\"    INFO: {func_prefix_gdl} Forcing streaming=True for 'amphion/Emilia-Dataset'.\")\n",
        "\n",
        "    try:\n",
        "        token = os.getenv(\"HF_TOKEN\") or HfFolder.get_token()\n",
        "        load_args = {\"path\": p_cfg.dataset_id, \"name\": p_cfg.dataset_config_name,\n",
        "                     \"split\": p_cfg.split, \"token\": token,\n",
        "                     \"trust_remote_code\": True, \"streaming\": is_streaming}\n",
        "        if p_cfg.data_files_glob_pattern: load_args[\"data_files\"] = {p_cfg.split: p_cfg.data_files_glob_pattern}\n",
        "        load_args_clean = {k:v for k,v in load_args.items() if v is not None or k == \"token\"}\n",
        "        if load_args_clean.get(\"token\") is None: load_args_clean.pop(\"token\", None)\n",
        "        print(f\"    INFO: {func_prefix_gdl} Calling load_dataset with: {load_args_clean}\")\n",
        "        hf_dataset_obj = load_dataset(**load_args_clean)\n",
        "\n",
        "        if isinstance(hf_dataset_obj, datasets.DatasetDict):\n",
        "            target_key_for_ds = p_cfg.split\n",
        "            if target_key_for_ds not in hf_dataset_obj:\n",
        "                target_key_for_ds = p_cfg.dataset_config_name if p_cfg.dataset_config_name and p_cfg.dataset_config_name in hf_dataset_obj else None\n",
        "            if target_key_for_ds and target_key_for_ds in hf_dataset_obj:\n",
        "                hf_dataset_obj = hf_dataset_obj[target_key_for_ds]\n",
        "                print(f\"    INFO: {func_prefix_gdl} Accessed dataset using key '{target_key_for_ds}' from DatasetDict.\")\n",
        "            else: print(f\"ERROR: {func_prefix_gdl} Loaded DatasetDict but key '{p_cfg.split}' or config '{p_cfg.dataset_config_name}' not found. Keys: {list(hf_dataset_obj.keys())}.\"); return None\n",
        "        ds_type_str = \"Streamed HFIterableDataset\" if isinstance(hf_dataset_obj, HFIterableDataset) else \"Map-style Dataset\"\n",
        "        print(f\"    SUCCESS: {func_prefix_gdl} Loaded {ds_type_str} '{p_cfg.dataset_id}'.\")\n",
        "    except DatasetNotFoundError as e: print(f\"ERROR: {func_prefix_gdl} DatasetNotFound: {e}. Check ID, config ('{p_cfg.dataset_config_name}'), split ('{p_cfg.split}'), and auth for gated datasets.\"); return None\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR: {func_prefix_gdl} ValueError loading '{p_cfg.dataset_id}' (split '{p_cfg.split}'): {e}\");\n",
        "        if \"split\" in str(e).lower() or \"config\" in str(e).lower(): print(f\"    INFO: {func_prefix_gdl} Tip: Use `get_hf_dataset_suggestions` in Cell 3 to check available splits/configs.\");\n",
        "        return None\n",
        "    except Exception as e: print(f\"ERROR: {func_prefix_gdl} Failed to load '{p_cfg.dataset_id}': {e}\\n{traceback.format_exc()}\"); return None\n",
        "    if hf_dataset_obj is None: print(f\"ERROR: {func_prefix_gdl} hf_dataset_obj is None after load attempt.\"); return None\n",
        "\n",
        "    is_map_style_loaded = isinstance(hf_dataset_obj, datasets.Dataset) and not isinstance(hf_dataset_obj, HFIterableDataset)\n",
        "    is_hf_iterable_loaded = isinstance(hf_dataset_obj, HFIterableDataset) # Explicit check for HF IterableDataset\n",
        "\n",
        "    if is_map_style_loaded and hasattr(hf_dataset_obj, 'features') and hf_dataset_obj.features:\n",
        "        new_features_to_cast = {}\n",
        "        for mod_key_cfg, mod_type_cfg in [(p_cfg.modality1_key, p_cfg.modality1_type), (p_cfg.modality2_key, p_cfg.modality2_type)]:\n",
        "            top_key = mod_key_cfg.split('.')[0]\n",
        "            if mod_type_cfg.lower() == 'audio' and top_key in hf_dataset_obj.features and \\\n",
        "               not isinstance(hf_dataset_obj.features[top_key], DatasetsAudioModule):\n",
        "                new_features_to_cast[top_key] = DatasetsAudioModule(sampling_rate=g_cfg.audio_target_sr)\n",
        "        if new_features_to_cast:\n",
        "            print(f\"    INFO: {func_prefix_gdl} Attempting to cast to datasets.Audio: {list(new_features_to_cast.keys())}\")\n",
        "            try:\n",
        "                current_features = hf_dataset_obj.features.copy()\n",
        "                for col, feat_type in new_features_to_cast.items(): current_features[col] = feat_type\n",
        "                hf_dataset_obj = hf_dataset_obj.cast(Features(current_features))\n",
        "                print(f\"    INFO: {func_prefix_gdl} Successfully cast audio columns.\")\n",
        "            except Exception as e_cast: print(f\"    WARNING: {func_prefix_gdl} Failed to cast audio columns: {e_cast}.\")\n",
        "\n",
        "    num_s_effective = g_cfg.num_samples\n",
        "    is_train_split_for_sampling = 'train' in p_cfg.split.lower()\n",
        "    if not is_train_split_for_sampling and hasattr(g_cfg, 'num_val_test_samples_cap') and g_cfg.num_val_test_samples_cap is not None:\n",
        "        if num_s_effective is None: num_s_effective = g_cfg.num_val_test_samples_cap\n",
        "        else: num_s_effective = min(num_s_effective, g_cfg.num_val_test_samples_cap)\n",
        "        if num_s_effective is not None: print(f\"    INFO: {func_prefix_gdl} Capping val/test samples at {num_s_effective}.\")\n",
        "\n",
        "    if num_s_effective is not None and num_s_effective > 0:\n",
        "        if is_hf_iterable_loaded:\n",
        "            hf_dataset_obj = hf_dataset_obj.take(num_s_effective); print(f\"    INFO: {func_prefix_gdl} IterableDataset will stream up to {num_s_effective} samples.\")\n",
        "        elif is_map_style_loaded :\n",
        "            current_len = len(hf_dataset_obj)\n",
        "            if current_len == 0: print(f\"WARNING: {func_prefix_gdl} Loaded map-style dataset is empty.\"); return None\n",
        "            actual_samples_to_select = min(num_s_effective, current_len)\n",
        "            if actual_samples_to_select < current_len :\n",
        "                if is_train_split_for_sampling: hf_dataset_obj = hf_dataset_obj.shuffle(seed=g_cfg.seed)\n",
        "                hf_dataset_obj = hf_dataset_obj.select(range(actual_samples_to_select))\n",
        "                print(f\"    INFO: {func_prefix_gdl} Selected {actual_samples_to_select} samples from {current_len} (shuffled: {is_train_split_for_sampling}).\")\n",
        "            else: print(f\"    INFO: {func_prefix_gdl} Using all {current_len} available samples.\")\n",
        "\n",
        "    try:\n",
        "        current_image_transform = image_transform_train if 'train' in p_cfg.split.lower() else image_transform_eval\n",
        "\n",
        "        pytorch_dataset: Union[PairDataset, IterableStylePairDataset]\n",
        "\n",
        "        if is_map_style_loaded:\n",
        "            pytorch_dataset = PairDataset(hf_dataset_obj, p_cfg, g_cfg, current_image_transform, audio_processor_fn)\n",
        "        elif is_hf_iterable_loaded:\n",
        "            pytorch_dataset = IterableStylePairDataset(hf_dataset_obj, p_cfg, g_cfg, current_image_transform, audio_processor_fn)\n",
        "        else:\n",
        "            print(f\"ERROR: {func_prefix_gdl} Unrecognized HF dataset type for PyTorch wrapper: {type(hf_dataset_obj)}\"); return None\n",
        "\n",
        "        bs = g_cfg.batch_size\n",
        "\n",
        "        dataloader_args = {\n",
        "            \"dataset\": pytorch_dataset,\n",
        "            \"batch_size\": bs,\n",
        "            \"collate_fn\": pairwise_collate_fn,\n",
        "            \"num_workers\": g_cfg.num_workers,\n",
        "            \"pin_memory\": False\n",
        "        }\n",
        "\n",
        "        if isinstance(pytorch_dataset, PairDataset): # Map-style dataset\n",
        "            if hasattr(pytorch_dataset, '__len__') and len(pytorch_dataset) < bs:\n",
        "                bs = max(1, len(pytorch_dataset)); print(f\"    INFO: {func_prefix_gdl} Batch size adjusted to {bs}.\")\n",
        "                dataloader_args[\"batch_size\"] = bs\n",
        "            if bs <=0: bs = 1; print(f\"    WARNING: {func_prefix_gdl} Batch size was <=0, set to 1.\"); dataloader_args[\"batch_size\"] = bs\n",
        "\n",
        "            dataloader_args[\"shuffle\"] = 'train' in p_cfg.split.lower()\n",
        "            if hasattr(pytorch_dataset, '__len__'):\n",
        "                 dataloader_args[\"drop_last\"] = dataloader_args[\"shuffle\"] and (len(pytorch_dataset) // bs > 0)\n",
        "            else:\n",
        "                 dataloader_args[\"drop_last\"] = False\n",
        "            if dataloader_args[\"num_workers\"] > 0 and torch.cuda.is_available():\n",
        "                dataloader_args[\"pin_memory\"] = True\n",
        "\n",
        "        elif isinstance(pytorch_dataset, IterableStylePairDataset): # Iterable-style dataset\n",
        "            dataloader_args[\"shuffle\"] = False\n",
        "            dataloader_args[\"sampler\"] = None\n",
        "            dataloader_args[\"batch_sampler\"] = None\n",
        "            dataloader_args[\"drop_last\"] = False\n",
        "            if dataloader_args[\"num_workers\"] > 0:\n",
        "                print(f\"    WARNING: {func_prefix_gdl} num_workers > 0 for IterableDataset. Setting to 0 for stability.\")\n",
        "                dataloader_args[\"num_workers\"] = 0\n",
        "            dataloader_args[\"pin_memory\"] = False # Typically False for num_workers=0\n",
        "        else:\n",
        "            print(f\"ERROR: {func_prefix_gdl} pytorch_dataset is of an unexpected type: {type(pytorch_dataset)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        dataloader = DataLoader(**dataloader_args)\n",
        "        print(f\"    SUCCESS: {func_prefix_gdl} DataLoader created. BatchSz={dataloader_args['batch_size']}, Shuffle={dataloader_args.get('shuffle', False)}, Workers={dataloader_args['num_workers']}, DropLast={dataloader_args.get('drop_last', False)}.\")\n",
        "        return dataloader\n",
        "    except ValueError as ve: print(f\"ERROR: {func_prefix_gdl} ValueError creating PyTorch Dataset '{pair_id_key}': {ve}\\n{traceback.format_exc()}\"); return None\n",
        "    except Exception as e_ds: print(f\"ERROR: {func_prefix_gdl} Creating PyTorch Dataset '{pair_id_key}': {e_ds}\\n{traceback.format_exc()}\"); return None\n",
        "print(f\"    {CELL4_INFO_PREFIX} get_dataloader_for_pair function defined.\")\n",
        "\n",
        "# --- Initialize Global Transformations and Processors ---\n",
        "print(f\"\\n{CELL4_INFO_PREFIX} Initializing global image transforms and audio processor...\")\n",
        "main_image_transform_train = get_image_transforms(img_size=config.img_size, is_train=True)\n",
        "main_image_transform_eval = get_image_transforms(img_size=config.img_size, is_train=False)\n",
        "print(f\"    {CELL4_INFO_PREFIX} Main image transforms (train/eval) initialized (img_size={config.img_size}).\")\n",
        "main_audio_processor_fn = get_audio_processor(config.audio_encoder_name, target_sr=config.audio_target_sr, max_duration_s=config.audio_max_duration_s)\n",
        "if main_audio_processor_fn: print(f\"    {CELL4_INFO_PREFIX} Main audio processor for '{config.audio_encoder_name}' configured.\")\n",
        "else: print(f\"    INFO: {CELL4_INFO_PREFIX} Main audio processor not configured.\")\n",
        "\n",
        "# --- Instantiate DataLoaders (Train, Val, Test) ---\n",
        "print(f\"\\n{CELL4_INFO_PREFIX} Instantiating DataLoaders from 'config.active_training_pairs':\")\n",
        "train_loaders: Dict[str, DataLoader] = {}; val_loaders: Dict[str, DataLoader] = {}; test_loaders: Dict[str, DataLoader] = {}\n",
        "if config.active_training_pairs and isinstance(config.active_training_pairs, dict):\n",
        "    print(f\"    Found {len(config.active_training_pairs)} pair configuration(s) to process.\")\n",
        "    for pair_id_key, p_cfg_instance in config.active_training_pairs.items():\n",
        "        print(f\"\\n    --- Processing Pair: '{pair_id_key}' (Dataset: '{p_cfg_instance.dataset_id}', Config: '{p_cfg_instance.dataset_config_name}', Split: '{p_cfg_instance.split}') ---\")\n",
        "        if not isinstance(p_cfg_instance, DatasetPairConfig): print(f\"    WARNING: Item '{pair_id_key}' not a DatasetPairConfig. Skipping.\"); continue\n",
        "        current_audio_processor = main_audio_processor_fn if p_cfg_instance.modality1_type == 'audio' or p_cfg_instance.modality2_type == 'audio' else None\n",
        "        if (p_cfg_instance.modality1_type == 'audio' or p_cfg_instance.modality2_type == 'audio') and not current_audio_processor:\n",
        "             print(f\"    WARNING: Pair '{pair_id_key}' needs audio, but 'main_audio_processor_fn' is None.\")\n",
        "\n",
        "        dataloader_instance = get_dataloader_for_pair(\n",
        "            pair_id_key, p_cfg_instance, config,\n",
        "            main_image_transform_train, main_image_transform_eval,\n",
        "            current_audio_processor\n",
        "        )\n",
        "        # *** CORRECTED TRUTHINESS CHECK ***\n",
        "        if dataloader_instance is not None:\n",
        "            split_lower = p_cfg_instance.split.lower()\n",
        "            target_loader_dict, dict_name = None, \"\"\n",
        "            if any(s_name in split_lower for s_name in ['train', 'training']): target_loader_dict, dict_name = train_loaders, \"train_loaders\"\n",
        "            elif any(s_name in split_lower for s_name in ['val', 'dev', 'validation']): target_loader_dict, dict_name = val_loaders, \"val_loaders\"\n",
        "            elif any(s_name in split_lower for s_name in ['test', 'testing', 'eval', 'evaluation']): target_loader_dict, dict_name = test_loaders, \"test_loaders\"\n",
        "            else:\n",
        "                print(f\"    WARNING: Unrecognized split pattern '{p_cfg_instance.split}' for '{pair_id_key}'. Adding to 'train_loaders' by default.\")\n",
        "                target_loader_dict, dict_name = train_loaders, \"train_loaders (defaulted)\"\n",
        "\n",
        "            if target_loader_dict is not None:\n",
        "                 target_loader_dict[pair_id_key] = dataloader_instance; print(f\"    SUCCESS: DataLoader for '{pair_id_key}' (split: '{p_cfg_instance.split}') added to '{dict_name}'.\")\n",
        "        else: print(f\"    FAILURE: Could not create DataLoader for '{pair_id_key}' (split: '{p_cfg_instance.split}'). This pair will be skipped.\")\n",
        "else: print(f\"    INFO: 'config.active_training_pairs' is empty. No DataLoaders created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(f\"\\n{CELL4_INFO_PREFIX} --- DataLoaders Summary ---\")\n",
        "for name, l_dict in [(\"train_loaders\", train_loaders), (\"val_loaders\", val_loaders), (\"test_loaders\", test_loaders)]:\n",
        "    if l_dict: print(f\"    {name} created for: {list(l_dict.keys())}\")\n",
        "    else: print(f\"    {name} is empty.\")\n",
        "\n",
        "if not train_loaders and not val_loaders and not test_loaders: print(f\"\\nCRITICAL WARNING: {CELL4_INFO_PREFIX} NO DataLoaders created.\")\n",
        "else: print(f\"    {CELL4_INFO_PREFIX} DataLoaders instantiation phase complete.\")\n",
        "\n",
        "CURRENT_TIME_END_CELL4 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL4))}] {CELL4_INFO_PREFIX} Data Handling definition and instantiation complete. Duration: {CURRENT_TIME_END_CELL4 - CURRENT_TIME_START_CELL4:.2f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4NOvPJQvMjVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5"
      ],
      "metadata": {
        "id": "nh4hX6p_Roke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 5: Models (Pair-Specific Encoders and PairModels with Two Mixture Embeddings) ---\n",
        "# Defines modality-specific encoders with enhanced configurability (projection heads, pooling, LayerNorm),\n",
        "# a PairModel class with configurable fusion methods, and instantiates shared components.\n",
        "# This version includes intensified NaN/Inf/Value range checks for raw_feat1 & raw_feat2 in PairModel.forward.\n",
        "# Based on the logs, this cell seems to be producing stable raw_feat1 and raw_feat2.\n",
        "# The SinkhornOTLoss error is likely downstream or an internal issue in SinkhornOTLoss with valid-looking inputs.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as vision_models\n",
        "from transformers import AutoTokenizer, AutoModel, WhisperModel # AutoTokenizer not used if text processed raw to TextEncoder\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import traceback\n",
        "import math\n",
        "\n",
        "# --- Configuration for this Cell ---\n",
        "DEBUG_CELL5 = True\n",
        "CELL_INFO_PREFIX = \"[Cell 5]\"\n",
        "\n",
        "# --- Helper Function for Parameter Counting ---\n",
        "def count_parameters(model: nn.Module) -> Tuple[int, int]:\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "# --- Critical Pre-run Checks ---\n",
        "print(f\"{CELL_INFO_PREFIX} Performing pre-run checks...\")\n",
        "if 'config' not in globals(): raise NameError(f\"FATAL: {CELL_INFO_PREFIX} 'config' not defined.\")\n",
        "if 'DEVICE' not in globals(): raise NameError(f\"FATAL: {CELL_INFO_PREFIX} 'DEVICE' not defined.\")\n",
        "if 'MixtureEmbedding' not in globals(): raise NameError(f\"FATAL: {CELL_INFO_PREFIX} 'MixtureEmbedding' class not defined.\")\n",
        "if not hasattr(config, 'd_enc') or config.d_enc <= 0: raise ValueError(f\"FATAL: config.d_enc invalid.\")\n",
        "print(f\"{CELL_INFO_PREFIX} Pre-run checks passed.\")\n",
        "\n",
        "CURRENT_TIME_START_CELL5 = time.time()\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL5))}] {CELL_INFO_PREFIX} Initializing Encoders, PairModels, and Mixture Embeddings...\")\n",
        "print(f\"    {CELL_INFO_PREFIX} Using DEVICE: {DEVICE}\")\n",
        "print(f\"    {CELL_INFO_PREFIX} Debug Mode: {'ON' if DEBUG_CELL5 else 'OFF'}\")\n",
        "print(f\"    {CELL_INFO_PREFIX} Target Individual Encoder Output Dim (config.d_enc): {config.d_enc}\")\n",
        "print(f\"    {CELL_INFO_PREFIX} Target Fused Features Input Dim (2 * config.d_enc): {2 * config.d_enc}\")\n",
        "\n",
        "# --- 1. Modality-Specific Encoders ---\n",
        "ENCODER_PREFIX = f\"{CELL_INFO_PREFIX}[Encoder]\"\n",
        "\n",
        "def _build_mlp_projection_with_norm(\n",
        "    input_dim: int, output_dim: int, num_layers: int = 1,\n",
        "    hidden_dim: Optional[int] = None, activation_fn_str: str = 'relu',\n",
        "    use_layernorm: bool = False, modality_name_for_log: str = \"\"\n",
        ") -> nn.Sequential:\n",
        "    # (This function is the same as the last robust version)\n",
        "    func_prefix_mlp = f\"{ENCODER_PREFIX}[_build_mlp_projection:{modality_name_for_log}]\"\n",
        "    if num_layers < 1: raise ValueError(f\"{func_prefix_mlp} Number of projection layers must be at least 1.\")\n",
        "    act_fn_module = nn.ReLU if activation_fn_str.lower() == 'relu' else nn.GELU if activation_fn_str.lower() == 'gelu' else nn.Identity\n",
        "    if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Activation: {act_fn_module.__name__}\")\n",
        "    layers = []; current_dim = input_dim\n",
        "    if num_layers == 1:\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "        if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Added Linear: {current_dim} -> {output_dim}\")\n",
        "    else:\n",
        "        h_dim = hidden_dim if hidden_dim and hidden_dim > 0 else output_dim\n",
        "        layers.append(nn.Linear(current_dim, h_dim)); layers.append(act_fn_module())\n",
        "        if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Added Linear: {current_dim} -> {h_dim}, Activation: {act_fn_module.__name__}\")\n",
        "        current_dim = h_dim\n",
        "        for i in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(current_dim, h_dim)); layers.append(act_fn_module())\n",
        "            if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Added Hidden Linear {i+1}: {current_dim} -> {h_dim}, Activation: {act_fn_module.__name__}\")\n",
        "            current_dim = h_dim\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "        if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Added Output Linear: {current_dim} -> {output_dim}\")\n",
        "    if use_layernorm:\n",
        "        layers.append(nn.LayerNorm(output_dim))\n",
        "        if DEBUG_CELL5: print(f\"    {func_prefix_mlp} Added LayerNorm for dim: {output_dim}\")\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def build_image_encoder(name: str = \"resnet50\", d_out: int = 512, pretrained: bool = True,\n",
        "                        num_projection_layers: int = 1, projection_hidden_dim: Optional[int] = None,\n",
        "                        projection_activation: str = 'relu', use_layernorm: bool = False) -> nn.Module:\n",
        "    # (This function is the same as the last robust version)\n",
        "    func_prefix = f\"{ENCODER_PREFIX}[build_image_encoder]\"; print(f\"{func_prefix} Building: Name='{name}', Target_d_out={d_out}, Pretrained={pretrained}, UseLayerNorm={use_layernorm}\"); print(f\"    Projection Config: Layers={num_projection_layers}, HiddenDim={projection_hidden_dim}, Activation='{projection_activation}'\")\n",
        "    try:\n",
        "        encoder_base: Optional[nn.Module] = None; native_out_features: Optional[int] = None\n",
        "        if name.lower() == \"resnet50\": weights = vision_models.ResNet50_Weights.DEFAULT if pretrained else None; encoder_base = vision_models.resnet50(weights=weights); native_out_features = encoder_base.fc.in_features; encoder_base.fc = nn.Identity()\n",
        "        elif name.lower() == \"vit_b_16\": weights = vision_models.ViT_B_16_Weights.DEFAULT if pretrained else None; encoder_base = vision_models.vit_b_16(weights=weights); native_out_features = encoder_base.heads.head.in_features; encoder_base.heads.head = nn.Identity()\n",
        "        else: raise ValueError(f\"{func_prefix} Unsupported image encoder name: {name}\")\n",
        "        if native_out_features is None or encoder_base is None: raise RuntimeError(f\"Base encoder init failed for {name}\")\n",
        "        projection_head = _build_mlp_projection_with_norm(native_out_features, d_out, num_projection_layers, projection_hidden_dim, projection_activation, use_layernorm, modality_name_for_log=f\"Image-{name}\")\n",
        "        full_encoder = nn.Sequential(encoder_base, projection_head); total_p, trainable_p = count_parameters(full_encoder)\n",
        "        print(f\"{func_prefix} '{name}' built. NativeOut={native_out_features} -> ProjHead -> {d_out}. Params: {total_p/1e6:.2f}M (T: {trainable_p/1e6:.2f}M)\"); return full_encoder\n",
        "    except Exception as e: print(f\"ERROR: {func_prefix} Failed for '{name}': {e}\\n{traceback.format_exc()}\"); raise\n",
        "print(f\"{ENCODER_PREFIX} build_image_encoder function defined (with LayerNorm option).\")\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    # (This class is the same as the last robust version)\n",
        "    def __init__(self, name: str = 'sentence-transformers/all-MiniLM-L12-v2', d_out: int = 512, freeze: bool = True, max_text_len: int = 77, pooling_strategy: str = 'mean', num_projection_layers: int = 1, projection_hidden_dim: Optional[int] = None, projection_activation: str = 'relu', use_layernorm: bool = False):\n",
        "        super().__init__(); self.class_prefix = f\"{ENCODER_PREFIX}[TextEncoder:{name.split('/')[-1]}]\"; self.name = name; self.d_out = d_out; self.freeze_transformer = freeze; self.max_text_len = max_text_len; self.pooling_strategy = pooling_strategy.lower(); print(f\"{self.class_prefix} Initializing: Target_d_out={self.d_out}, Freeze={self.freeze_transformer}, Pooling='{self.pooling_strategy}', UseLayerNorm={use_layernorm}\"); print(f\"    Projection Config: Layers={num_projection_layers}, HiddenDim={projection_hidden_dim}, Activation='{projection_activation}'\")\n",
        "        try: self.tokenizer = AutoTokenizer.from_pretrained(self.name); self.transformer = AutoModel.from_pretrained(self.name)\n",
        "        except Exception as e: print(f\"ERROR: {self.class_prefix} Load failed for '{self.name}': {e}\\n{traceback.format_exc()}\"); raise\n",
        "        if self.freeze_transformer:\n",
        "            for param in self.transformer.parameters(): param.requires_grad = False\n",
        "            self.transformer.eval()\n",
        "        transformer_native_out_dim = self.transformer.config.hidden_size\n",
        "        self.projection_layer = _build_mlp_projection_with_norm(transformer_native_out_dim, self.d_out, num_projection_layers, projection_hidden_dim, projection_activation, use_layernorm, modality_name_for_log=f\"Text-{name.split('/')[-1]}\")\n",
        "        total_p, trainable_p = count_parameters(self); print(f\"{self.class_prefix} Projection: {transformer_native_out_dim} -> ProjHead -> {self.d_out}. Params: {total_p/1e6:.2f}M (T: {trainable_p/1e6:.2f}M)\")\n",
        "    def forward(self, texts: List[str]) -> Optional[torch.Tensor]:\n",
        "        if not texts or not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "            if DEBUG_CELL5: print(f\"DEBUG: {self.class_prefix} forward(): Invalid input 'texts'. Returning None.\")\n",
        "            return None\n",
        "        current_device = next(self.parameters()).device\n",
        "        if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Input texts (count: {len(texts)}), Device: {current_device}\")\n",
        "        try:\n",
        "            inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=self.max_text_len).to(current_device)\n",
        "            if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Tokenized input_ids shape: {inputs['input_ids'].shape if 'input_ids' in inputs else 'N/A'}\")\n",
        "            with torch.set_grad_enabled(not self.freeze_transformer): model_outputs = self.transformer(**inputs)\n",
        "            last_hidden_state = model_outputs.last_hidden_state\n",
        "            pooled_embeddings: Optional[torch.Tensor] = None\n",
        "            if self.pooling_strategy == 'mean':\n",
        "                attention_mask = inputs['attention_mask'].unsqueeze(-1).to(last_hidden_state.dtype)\n",
        "                summed_embeddings = (last_hidden_state * attention_mask).sum(dim=1); summed_mask = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "                pooled_embeddings = summed_embeddings / summed_mask\n",
        "            elif self.pooling_strategy == 'cls': pooled_embeddings = last_hidden_state[:, 0]\n",
        "            if pooled_embeddings is None: print(f\"ERROR: {self.class_prefix} forward(): Pooled embeddings None after strategy '{self.pooling_strategy}'.\"); return None\n",
        "            if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Pooled ({self.pooling_strategy}) shape: {pooled_embeddings.shape}\")\n",
        "            projected_embeddings = self.projection_layer(pooled_embeddings)\n",
        "            if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Projected shape: {projected_embeddings.shape}\")\n",
        "            return projected_embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {self.class_prefix} forward(): Failed: {e}\")\n",
        "            if DEBUG_CELL5: print(traceback.format_exc())\n",
        "            return None\n",
        "\n",
        "print(f\"{ENCODER_PREFIX} TextEncoder class defined (with LayerNorm option & pooling).\")\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    # (This class is the same as the last robust version)\n",
        "    def __init__(self, name: Optional[str] = \"openai/whisper-base\", d_out: int = 512, freeze: bool = True, num_projection_layers: int = 1, projection_hidden_dim: Optional[int] = None, projection_activation: str = 'relu', use_layernorm: bool = False):\n",
        "        super().__init__(); self.class_prefix = f\"{ENCODER_PREFIX}[AudioEncoder:{name.split('/')[-1] if name else 'Placeholder'}]\"; self.name = name if name else \"placeholder_audio_encoder\"; self.d_out = d_out; self.freeze_encoder = freeze; print(f\"{self.class_prefix} Initializing: Target_d_out={d_out}, Freeze={self.freeze_encoder}, UseLayerNorm={use_layernorm}\"); print(f\"    Projection Config: Layers={num_projection_layers}, HiddenDim={projection_hidden_dim}, Activation='{projection_activation}'\")\n",
        "        base_model_native_out_dim = d_out; self.base_model: nn.Module = nn.Identity()\n",
        "        if self.name and self.name != \"placeholder_audio_encoder\":\n",
        "            try:\n",
        "                if \"whisper\" in self.name.lower(): self.base_model = WhisperModel.from_pretrained(self.name); base_model_native_out_dim = self.base_model.config.hidden_size\n",
        "                if self.freeze_encoder and hasattr(self.base_model, 'parameters'):\n",
        "                    for param in self.base_model.parameters(): param.requires_grad = False\n",
        "                    if hasattr(self.base_model, 'eval'): self.base_model.eval()\n",
        "            except Exception as e: print(f\"ERROR: {self.class_prefix} Load failed for '{self.name}': {e}\\n{traceback.format_exc()}\"); self.base_model = nn.Identity(); base_model_native_out_dim = 768\n",
        "        self.projection_layer = _build_mlp_projection_with_norm(base_model_native_out_dim, self.d_out, num_projection_layers, projection_hidden_dim, projection_activation, use_layernorm, modality_name_for_log=f\"Audio-{name.split('/')[-1] if name else 'Placeholder'}\")\n",
        "        total_p, trainable_p = count_parameters(self); print(f\"{self.class_prefix} Projection: {base_model_native_out_dim} -> ProjHead -> {self.d_out}. Params: {total_p/1e6:.2f}M (T: {trainable_p/1e6:.2f}M)\")\n",
        "    def forward(self, audio_features: torch.Tensor) -> Optional[torch.Tensor]:\n",
        "        # (Forward pass same as last robust version)\n",
        "        if audio_features is None or audio_features.numel() == 0: return None\n",
        "        current_device = next(self.parameters()).device\n",
        "        try:\n",
        "            if not isinstance(audio_features, torch.Tensor): raise ValueError(\"AudioEncoder expects torch.Tensor\")\n",
        "            audio_features = audio_features.to(current_device)\n",
        "        except Exception as e_dev: print(f\"ERROR: {self.class_prefix} forward(): Bad audio_features: {e_dev}\"); return None\n",
        "        if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Input audio_features shape: {audio_features.shape}\")\n",
        "        try:\n",
        "            x = audio_features\n",
        "            if isinstance(self.base_model, WhisperModel):\n",
        "                with torch.set_grad_enabled(not self.freeze_encoder):\n",
        "                    if x.dim() == 2: x = x.unsqueeze(0)\n",
        "                    num_mel_bins = self.base_model.config.num_mel_bins\n",
        "                    if x.dim() == 3:\n",
        "                        if x.shape[1] != num_mel_bins and x.shape[2] == num_mel_bins: x = x.permute(0, 2, 1)\n",
        "                    if x.shape[1] != num_mel_bins: print(f\"WARNING: {self.class_prefix} forward(): Whisper input shape {x.shape} incompatible with num_mel_bins ({num_mel_bins}). Returning zeros.\"); return torch.zeros(x.shape[0] if x.dim() > 0 else 1, self.d_out, device=current_device)\n",
        "                    encoder_outputs = self.base_model.encoder(input_features=x); x = encoder_outputs.last_hidden_state.mean(dim=1)\n",
        "            elif isinstance(self.base_model, nn.Identity):\n",
        "                proj_input_dim_actual = self.projection_layer[0].in_features if isinstance(self.projection_layer, nn.Sequential) else self.projection_layer.in_features\n",
        "                if x.dim() == 3 and x.shape[-1] != proj_input_dim_actual: x = x.mean(dim=1)\n",
        "                if x.shape[-1] != proj_input_dim_actual: print(f\"WARNING: {self.class_prefix} forward(): nn.Identity feature dim {x.shape[-1]} mismatch with projection input {proj_input_dim_actual}. Returning zeros.\"); return torch.zeros(audio_features.shape[0] if audio_features.dim() > 0 else 1, self.d_out, device=current_device)\n",
        "            else: x = self.base_model(x)\n",
        "            projected_output = self.projection_layer(x)\n",
        "            if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Projected shape: {projected_output.shape}\")\n",
        "            return projected_output\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {self.class_prefix} forward(): Failed: {e}\")\n",
        "            if DEBUG_CELL5: print(traceback.format_exc())\n",
        "            batch_dim_size = audio_features.shape[0] if audio_features is not None and isinstance(audio_features, torch.Tensor) and audio_features.dim() > 0 else 1\n",
        "            return torch.zeros(batch_dim_size, self.d_out, device=current_device)\n",
        "\n",
        "print(f\"{ENCODER_PREFIX} AudioEncoder class defined (with LayerNorm option).\")\n",
        "\n",
        "# --- 2. Pairwise Model with Configurable Fusion & Enhanced NaN/Inf Checks ---\n",
        "PAIRMODEL_PREFIX = f\"{CELL_INFO_PREFIX}[PairModel]\"\n",
        "class PairModel(nn.Module):\n",
        "    # (This class is the same as the last robust version with intensified raw_feat diagnostics)\n",
        "    def __init__(self, mod1_type: str, mod2_type: str, encoder1: nn.Module, encoder2: nn.Module, fused_mixture_embedding: MixtureEmbedding, individual_encoder_d_out: int, fusion_method: str = 'concat', fusion_mlp_hidden_layers: int = 1, fusion_mlp_activation: str = 'relu'):\n",
        "        super().__init__(); self.class_prefix = f\"{PAIRMODEL_PREFIX}({mod1_type}<->{mod2_type})\"; self.modality1_type = mod1_type; self.modality2_type = mod2_type; self.encoder1 = encoder1; self.encoder2 = encoder2; self.fused_mixture_embedding = fused_mixture_embedding; self.individual_d_out = individual_encoder_d_out; self.fusion_method = fusion_method.lower(); print(f\"{self.class_prefix} Initializing with Fusion: '{self.fusion_method}'\")\n",
        "        self.expected_fused_embedding_input_dim = 2 * self.individual_d_out; self.fusion_module: Optional[nn.Module] = None\n",
        "        if self.fusion_method == 'mlp_on_concat':\n",
        "            mlp_input_dim = 2 * self.individual_d_out; mlp_output_dim = self.expected_fused_embedding_input_dim\n",
        "            self.fusion_module = _build_mlp_projection_with_norm(mlp_input_dim, mlp_output_dim, fusion_mlp_hidden_layers, hidden_dim=mlp_input_dim, activation_fn_str=fusion_mlp_activation, use_layernorm=False, modality_name_for_log=f\"FusionMLP-{mod1_type}-{mod2_type}\")\n",
        "            if self.fusion_module: print(f\"{self.class_prefix} Using 'mlp_on_concat' fusion MLP.\")\n",
        "        elif self.fusion_method != 'concat': print(f\"WARNING: {self.class_prefix} Unknown fusion_method '{self.fusion_method}', defaulting to 'concat'.\"); self.fusion_method = 'concat'\n",
        "        total_p_fusion, trainable_p_fusion = count_parameters(self.fusion_module) if self.fusion_module else (0,0); print(f\"{self.class_prefix} Fusion module params: {total_p_fusion/1e3:.1f}K (T: {trainable_p_fusion/1e3:.1f}K)\"); print(f\"{self.class_prefix} Initialization complete.\")\n",
        "    def forward(self, input1: Any, input2: Any) -> Tuple[Optional[Dict[str, Any]], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Inputs for {self.modality1_type}, {self.modality2_type}\")\n",
        "        raw_feat1, raw_feat2, measures_fused, gamma_fused = None, None, None, None\n",
        "        try:\n",
        "            raw_feat1 = self.encoder1(input1); raw_feat2 = self.encoder2(input2)\n",
        "            for i, (feat, name) in enumerate([(raw_feat1, self.modality1_type), (raw_feat2, self.modality2_type)]):\n",
        "                if feat is None or not isinstance(feat, torch.Tensor) or feat.shape[0] == 0: print(f\"CRITICAL WARNING: {self.class_prefix} forward(): Invalid raw_feat{i+1} from '{name}' enc. Type: {type(feat)}, Shape: {feat.shape if isinstance(feat, torch.Tensor) else 'N/A'}. Ret None.\"); return None, None, None, None\n",
        "            if raw_feat1.shape[0] != raw_feat2.shape[0]: print(f\"WARNING: {self.class_prefix} forward(): Batch size mismatch. R1:{raw_feat1.shape}, R2:{raw_feat2.shape}. Ret None.\"); return None, None, None, None\n",
        "            if DEBUG_CELL5:\n",
        "                print(f\"    {self.class_prefix} forward(): --- Raw Feature Diagnostics ---\")\n",
        "                for i, (feat, name) in enumerate([(raw_feat1, self.modality1_type), (raw_feat2, self.modality2_type)]):\n",
        "                    print(f\"        Feature: raw_feat{i+1} ('{name}'), Shape: {feat.shape}, Dtype: {feat.dtype}, Device: {feat.device}\")\n",
        "                    has_nans = torch.isnan(feat).any().item(); has_infs = torch.isinf(feat).any().item()\n",
        "                    if has_nans: print(f\"        CRITICAL WARNING: raw_feat{i+1} ('{name}') CONTAINS NaNs! Sum_NaNs: {torch.isnan(feat).sum().item()}\")\n",
        "                    if has_infs: print(f\"        CRITICAL WARNING: raw_feat{i+1} ('{name}') CONTAINS Infs! Sum_Infs: {torch.isinf(feat).sum().item()}\")\n",
        "                    if not has_nans and not has_infs: print(f\"        Stats: Min={feat.min().item():.3e}, Max={feat.max().item():.3e}, Mean={feat.mean().item():.3e}, Std={feat.std().item():.3e}\")\n",
        "                    else: print(f\"        Stats: Not calculable due to non-finite values.\")\n",
        "                    if feat.shape[-1] != self.individual_d_out: print(f\"        WARNING: {name} feat dim {feat.shape[-1]} != expected {self.individual_d_out}\")\n",
        "                print(f\"    {self.class_prefix} forward(): --- End Raw Feature Diagnostics ---\")\n",
        "            concatenated_features = torch.cat([raw_feat1, raw_feat2], dim=-1)\n",
        "            fused_for_mixture: torch.Tensor\n",
        "            if self.fusion_method == 'mlp_on_concat' and self.fusion_module is not None: fused_for_mixture = self.fusion_module(concatenated_features)\n",
        "            else: fused_for_mixture = concatenated_features\n",
        "            if DEBUG_CELL5: print(f\"{self.class_prefix} forward(): Features for fused_ME (method: {self.fusion_method}) shape: {fused_for_mixture.shape}\")\n",
        "            if fused_for_mixture.shape[-1] != self.expected_fused_embedding_input_dim: print(f\"CRITICAL WARNING: {self.class_prefix} forward(): Fused feature dim ({fused_for_mixture.shape[-1]}) != ME expected ({self.expected_fused_embedding_input_dim}).\")\n",
        "            measures_fused, gamma_fused = self.fused_mixture_embedding(fused_for_mixture)\n",
        "            if DEBUG_CELL5: # (Debug block for measures_fused, gamma_fused as before)\n",
        "                if measures_fused is not None:\n",
        "                    print(f\"{self.class_prefix} forward(): ---- Fused Measures ----\")\n",
        "                    for k_fm, v_fm in measures_fused.items():\n",
        "                        if isinstance(v_fm, torch.Tensor): print(f\"{self.class_prefix} forward():   Fused measure '{k_fm}' (Tensor) shape: {v_fm.shape}\")\n",
        "                        elif isinstance(v_fm, tuple):\n",
        "                            print(f\"{self.class_prefix} forward():   Fused measure '{k_fm}' (Tuple contents):\")\n",
        "                            for i_fm, item_fm in enumerate(v_fm):\n",
        "                                if isinstance(item_fm, torch.Tensor): print(f\"{self.class_prefix} forward():     Item {i_fm} shape: {item_fm.shape}\")\n",
        "                                else: print(f\"{self.class_prefix} forward():     Item {i_fm} (type: {type(item_fm)}): {str(item_fm)[:100]}\")\n",
        "                        else: print(f\"{self.class_prefix} forward():   Fused measure '{k_fm}' (type: {type(v_fm)}): {str(v_fm)[:100]}\")\n",
        "                else: print(f\"{self.class_prefix} forward(): ---- Fused Measures are None ----\")\n",
        "                if gamma_fused is not None and isinstance(gamma_fused, torch.Tensor): print(f\"{self.class_prefix} forward(): Fused gamma shape: {gamma_fused.shape}\")\n",
        "                elif gamma_fused is not None: print(f\"{self.class_prefix} forward(): Fused gamma type: {type(gamma_fused)}\")\n",
        "                else: print(f\"{self.class_prefix} forward(): Fused gamma is None\")\n",
        "                print(f\"{self.class_prefix} forward(): ---- End Fused Measures ----\")\n",
        "            return measures_fused, gamma_fused, raw_feat1, raw_feat2\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {self.class_prefix} forward(): Failed: {e}\")\n",
        "            if DEBUG_CELL5:\n",
        "                print(traceback.format_exc())\n",
        "            return None, None, None, None\n",
        "\n",
        "print(f\"{PAIRMODEL_PREFIX} PairModel class defined (with configurable fusion and enhanced debug checks).\")\n",
        "\n",
        "# --- 3. Instantiate Shared Encoders and Mixture Embeddings ---\n",
        "INSTANTIATION_PREFIX = f\"{CELL_INFO_PREFIX}[Instantiation]\"; print(f\"\\n{INSTANTIATION_PREFIX} Starting instantiation with new configurable encoder options...\")\n",
        "def get_encoder_extra_config(modality_prefix: str, config_obj: Any) -> Dict[str, Any]:\n",
        "    return {'num_projection_layers': getattr(config_obj, f\"{modality_prefix}_encoder_projection_layers\", 1),\n",
        "            'projection_hidden_dim': getattr(config_obj, f\"{modality_prefix}_encoder_projection_hidden_dim\", None),\n",
        "            'projection_activation': getattr(config_obj, f\"{modality_prefix}_encoder_projection_activation\", 'relu'),\n",
        "            'use_layernorm': getattr(config_obj, f\"{modality_prefix}_encoder_use_layernorm\", True) }\n",
        "img_extra_cfg = get_encoder_extra_config('image', config); print(f\"{INSTANTIATION_PREFIX} Base Image Encoder (Name: {config.image_encoder_name}, Target_d_out: {config.d_enc}, ProjLayers: {img_extra_cfg['num_projection_layers']}, UseLN: {img_extra_cfg['use_layernorm']})...\")\n",
        "base_image_encoder = build_image_encoder(name=config.image_encoder_name, d_out=config.d_enc, pretrained=config.image_pretrained, **img_extra_cfg).to(DEVICE)\n",
        "txt_extra_cfg = get_encoder_extra_config('text', config); txt_pool_strat_cfg = getattr(config, 'text_encoder_pooling_strategy', 'mean'); print(f\"{INSTANTIATION_PREFIX} Base Text Encoder (Name: {config.text_encoder_name}, Target_d_out: {config.d_enc}, ProjLayers: {txt_extra_cfg['num_projection_layers']}, Pooling: '{txt_pool_strat_cfg}', UseLN: {txt_extra_cfg['use_layernorm']})...\")\n",
        "base_text_encoder = TextEncoder(name=config.text_encoder_name, d_out=config.d_enc, freeze=config.text_freeze_transformer, max_text_len=config.max_text_len, pooling_strategy=txt_pool_strat_cfg, **txt_extra_cfg).to(DEVICE)\n",
        "base_audio_encoder: Optional[nn.Module] = None\n",
        "if hasattr(config, 'audio_encoder_name') and config.audio_encoder_name:\n",
        "    aud_extra_cfg = get_encoder_extra_config('audio', config); print(f\"{INSTANTIATION_PREFIX} Base Audio Encoder (Name: {config.audio_encoder_name}, Target_d_out: {config.d_enc}, ProjLayers: {aud_extra_cfg['num_projection_layers']}, UseLN: {aud_extra_cfg['use_layernorm']})...\")\n",
        "    base_audio_encoder = AudioEncoder(name=config.audio_encoder_name, d_out=config.d_enc, freeze=config.audio_freeze_encoder, **aud_extra_cfg).to(DEVICE)\n",
        "else: print(f\"{INSTANTIATION_PREFIX} Base Audio Encoder will NOT be instantiated.\")\n",
        "fused_me_input_dim = 2 * config.d_enc; print(f\"{INSTANTIATION_PREFIX} MixtureEmbedding for FUSED features (Input dim: {fused_me_input_dim})...\")\n",
        "mixture_embedding_for_fused = MixtureEmbedding(d_enc=fused_me_input_dim, geometry_dims=config.geometry_dims, temperature=config.mixture_temp, geometries_to_use=config.geometries_to_use, kernel_function=config.mixture_kernel_function).to(DEVICE)\n",
        "total_p_fme, trainable_p_fme = count_parameters(mixture_embedding_for_fused); print(f\"    {INSTANTIATION_PREFIX} 'mixture_embedding_for_fused' Params: {total_p_fme/1e3:.1f}K (T: {trainable_p_fme/1e3:.1f}K).\")\n",
        "raw_me_input_dim = config.d_enc; print(f\"{INSTANTIATION_PREFIX} MixtureEmbedding for RAW features (Input dim: {raw_me_input_dim})...\")\n",
        "mixture_embedding_for_raw = MixtureEmbedding(d_enc=raw_me_input_dim, geometry_dims=config.geometry_dims, temperature=config.mixture_temp, geometries_to_use=config.geometries_to_use, kernel_function=config.mixture_kernel_function).to(DEVICE)\n",
        "total_p_rme, trainable_p_rme = count_parameters(mixture_embedding_for_raw); print(f\"    {INSTANTIATION_PREFIX} 'mixture_embedding_for_raw' Params: {total_p_rme/1e3:.1f}K (T: {trainable_p_rme/1e3:.1f}K).\")\n",
        "\n",
        "# --- 4. Instantiate PairModels for Active Training Pairs ---\n",
        "# (This section remains the same as your last robust version, using config for fusion method etc.)\n",
        "print(f\"\\n{INSTANTIATION_PREFIX} Instantiating PairModels with configurable fusion...\")\n",
        "models_for_pairs: Dict[str, PairModel] = {}\n",
        "if 'train_loaders' in globals() and isinstance(train_loaders, dict) and train_loaders: # This check is key\n",
        "    for pair_name_key, _ in train_loaders.items():\n",
        "        pair_model_prefix = f\"{INSTANTIATION_PREFIX}[PairModelSetup:{pair_name_key}]\"; print(f\"{pair_model_prefix} Processing...\")\n",
        "        pair_cfg = config.active_training_pairs.get(pair_name_key)\n",
        "        if not pair_cfg: print(f\"WARNING: {pair_model_prefix} No DatasetPairConfig for '{pair_name_key}'. Skipping.\"); continue\n",
        "        print(f\"{pair_model_prefix} Config for '{pair_cfg.modality1_type}<->{pair_cfg.modality2_type}'.\")\n",
        "        encoder_map = {'image': base_image_encoder, 'text': base_text_encoder, 'audio': base_audio_encoder}\n",
        "        selected_encoder1 = encoder_map.get(pair_cfg.modality1_type) if encoder_map.get(pair_cfg.modality1_type) is not None else nn.Identity().to(DEVICE)\n",
        "        selected_encoder2 = encoder_map.get(pair_cfg.modality2_type) if encoder_map.get(pair_cfg.modality2_type) is not None else nn.Identity().to(DEVICE)\n",
        "        print(f\"{pair_model_prefix} Using Enc1 ({type(selected_encoder1).__name__}), Enc2 ({type(selected_encoder2).__name__})\")\n",
        "        pair_fusion_method = getattr(config, f\"{pair_name_key}_fusion_method\", getattr(config, 'pair_fusion_method', 'concat'))\n",
        "        pair_fusion_mlp_layers = getattr(config, f\"{pair_name_key}_fusion_mlp_hidden_layers\", getattr(config, 'pair_fusion_mlp_hidden_layers', 1))\n",
        "        pair_fusion_mlp_act = getattr(config, f\"{pair_name_key}_fusion_mlp_activation\", getattr(config, 'pair_fusion_mlp_activation', 'relu'))\n",
        "        print(f\"    {pair_model_prefix} For pair '{pair_name_key}', Fusion: '{pair_fusion_method}', MLP Hidden Layers: {pair_fusion_mlp_layers}\")\n",
        "        try:\n",
        "            models_for_pairs[pair_name_key] = PairModel(mod1_type=pair_cfg.modality1_type, mod2_type=pair_cfg.modality2_type, encoder1=selected_encoder1, encoder2=selected_encoder2, fused_mixture_embedding=mixture_embedding_for_fused, individual_encoder_d_out=config.d_enc, fusion_method=pair_fusion_method, fusion_mlp_hidden_layers=pair_fusion_mlp_layers, fusion_mlp_activation=pair_fusion_mlp_act).to(DEVICE)\n",
        "            fusion_params_count = 0\n",
        "            if hasattr(models_for_pairs[pair_name_key], 'fusion_module') and models_for_pairs[pair_name_key].fusion_module is not None: _, fusion_params_count = count_parameters(models_for_pairs[pair_name_key].fusion_module)\n",
        "            print(f\"{pair_model_prefix} PairModel instantiated. Trainable fusion module params: {fusion_params_count/1e3:.1f}K.\")\n",
        "        except Exception as e: print(f\"ERROR Instantiating PairModel for {pair_name_key}: {e}\\n{traceback.format_exc()}\")\n",
        "else:\n",
        "    print(f\"WARNING: {INSTANTIATION_PREFIX} 'train_loaders' not found/empty. No PairModels instantiated.\")\n",
        "\n",
        "# --- 5. Convenience Assignments & Sanity Checks ---\n",
        "# (Same as previous version)\n",
        "CONVENIENCE_PREFIX = f\"{CELL_INFO_PREFIX}[ConvenienceAssignments]\"; print(f\"\\n{CONVENIENCE_PREFIX} Assigning common PairModels...\")\n",
        "model_it, model_ta, model_ai = None, None, None\n",
        "if models_for_pairs:\n",
        "    assigned_it, assigned_ta, assigned_ai = False, False, False\n",
        "    for name, model_instance_check in models_for_pairs.items():\n",
        "        if not isinstance(model_instance_check, PairModel): continue\n",
        "        m1t = model_instance_check.modality1_type.lower(); m2t = model_instance_check.modality2_type.lower()\n",
        "        if not assigned_it and ((m1t == 'image' and m2t == 'text') or (m1t == 'text' and m2t == 'image')): model_it = model_instance_check; assigned_it = True; print(f\"{CONVENIENCE_PREFIX} Assigned 'model_it' from '{name}'.\")\n",
        "        elif not assigned_ta and ((m1t == 'text' and m2t == 'audio') or (m1t == 'audio' and m2t == 'text')): model_ta = model_instance_check; assigned_ta = True; print(f\"{CONVENIENCE_PREFIX} Assigned 'model_ta' from '{name}'.\")\n",
        "        elif not assigned_ai and ((m1t == 'audio' and m2t == 'image') or (m1t == 'image' and m2t == 'audio')): model_ai = model_instance_check; assigned_ai = True; print(f\"{CONVENIENCE_PREFIX} Assigned 'model_ai' from '{name}'.\")\n",
        "    if not model_it: print(f\"INFO: {CONVENIENCE_PREFIX} 'model_it' not assigned.\")\n",
        "    if not model_ta: print(f\"INFO: {CONVENIENCE_PREFIX} 'model_ta' not assigned.\")\n",
        "    if not model_ai: print(f\"INFO: {CONVENIENCE_PREFIX} 'model_ai' not assigned.\")\n",
        "else: print(f\"INFO: {CONVENIENCE_PREFIX} 'models_for_pairs' empty.\")\n",
        "\n",
        "CURRENT_TIME_END_CELL5 = time.time()\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL5))}] {CELL_INFO_PREFIX} All definitions and instantiations complete. Duration: {CURRENT_TIME_END_CELL5 - CURRENT_TIME_START_CELL5:.2f}s\")\n",
        "\n",
        "if DEBUG_CELL5:\n",
        "    print(f\"\\n{CELL_INFO_PREFIX}[SanityChecks] Final check on models...\")\n",
        "    for name, model_to_check in models_for_pairs.items():\n",
        "        if isinstance(model_to_check, nn.Module):\n",
        "             print(f\"    {CELL_INFO_PREFIX}[SanityChecks] PairModel '{name}': On device {next(model_to_check.parameters()).device if list(model_to_check.parameters()) else 'N/A (No params or Identity)'}\")"
      ],
      "metadata": {
        "id": "1xhXbUJHMgUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6"
      ],
      "metadata": {
        "id": "gAuOVoI3RlYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 6: Loss Functions (Corrected and Robust) ---\n",
        "# Defines a robust SinkhornOTLoss wrapper and a corrected MainCriterion.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, Any, List\n",
        "import traceback\n",
        "\n",
        "try:\n",
        "    import geomloss\n",
        "    GEOMOSS_AVAILABLE_CELL6 = True\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] [Cell 6] Geomloss imported successfully (Version: {getattr(geomloss, '__version__', 'N/A')}).\")\n",
        "except ImportError:\n",
        "    GEOMOSS_AVAILABLE_CELL6 = False\n",
        "    print(f\"WARNING: [{time.strftime('%Y-%m-%d %H:%M:%S')}] [Cell 6] Geomloss library not found. SinkhornOTLoss will be disabled.\")\n",
        "\n",
        "if 'config' not in globals(): raise NameError(\"FATAL: [Cell 6] 'config' not defined. Run Cell 3.\")\n",
        "if 'DEVICE' not in globals(): raise NameError(\"FATAL: [Cell 6] 'DEVICE' not defined. Run Cell 1.\")\n",
        "if 'MixtureEmbedding' not in globals():\n",
        "    print(\"WARNING: [Cell 6] MixtureEmbedding class not found. Defining a placeholder.\")\n",
        "    class MixtureEmbedding(nn.Module):\n",
        "        def compute_entropy(self, measures:Optional[Dict]=None, gamma:Optional[torch.Tensor]=None) -> torch.Tensor: return torch.tensor(0.0)\n",
        "        def compute_pairwise_distances(self, m1, g1, m2, g2, metric=None): return torch.zeros(g1.shape[0] if g1 is not None else 0, g2.shape[0] if g2 is not None else 0, device=DEVICE)\n",
        "        temperature = nn.Parameter(torch.tensor(0.1))\n",
        "        geom_keys: List[str] = []\n",
        "        # Add d_enc attribute for __init__ if MainCriterion tries to access it from placeholder for device\n",
        "        d_enc: int = 0\n",
        "\n",
        "\n",
        "CELL6_INFO_PREFIX = \"[Cell 6]\"\n",
        "DEBUG_CELL6 = getattr(config, 'debug_mode', True) if 'config' in globals() else True\n",
        "\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL6_INFO_PREFIX} Defining Loss Functions...\")\n",
        "print(f\"    {CELL6_INFO_PREFIX} Device: {DEVICE}, Debug Mode: {DEBUG_CELL6}\")\n",
        "\n",
        "class SinkhornOTLoss(nn.Module):\n",
        "    \"\"\"Robust wrapper for geomloss.SamplesLoss to compute Sinkhorn (Optimal Transport) distance.\"\"\"\n",
        "    def __init__(self,\n",
        "                 p_norm: int = 2,\n",
        "                 blur: float = 0.05,\n",
        "                 scaling: float = 0.9,\n",
        "                 debias: bool = True,\n",
        "                 backend: str = \"tensorized\",\n",
        "                 potentials: bool = False\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.class_prefix = f\"{CELL6_INFO_PREFIX}[SinkhornOTLoss]\"\n",
        "        self.geomloss_is_available = GEOMOSS_AVAILABLE_CELL6\n",
        "        self.sinkhorn_loss_calculator = None\n",
        "\n",
        "        if self.geomloss_is_available:\n",
        "            try:\n",
        "                self.sinkhorn_loss_calculator = geomloss.SamplesLoss(\n",
        "                    loss=\"sinkhorn\", p=p_norm, blur=blur, scaling=scaling,\n",
        "                    debias=debias, backend=backend, potentials=potentials\n",
        "                )\n",
        "                print(f\"    {self.class_prefix} Initialized geomloss.SamplesLoss: p={p_norm}, blur={blur}, scaling={scaling}, debias={debias}, backend='{backend}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: {self.class_prefix} Failed to initialize geomloss.SamplesLoss: {e}\\n{traceback.format_exc()}\"); self.geomloss_is_available = False\n",
        "        else: print(f\"INFO: {self.class_prefix} Geomloss not available or failed. SinkhornOTLoss returns 0.0.\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        output_device = x.device if isinstance(x, torch.Tensor) else (y.device if isinstance(y, torch.Tensor) else DEVICE)\n",
        "        if not self.geomloss_is_available or self.sinkhorn_loss_calculator is None: return torch.tensor(0.0, device=output_device)\n",
        "        if not (isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor) and x.ndim >= 2 and y.ndim >= 2 and x.shape[0] > 0 and y.shape[0] > 0 and x.shape[-1] == y.shape[-1]):\n",
        "            if DEBUG_CELL6: print(f\"DEBUG: {self.class_prefix} Invalid inputs. x:{x.shape if isinstance(x,torch.Tensor) else type(x)}, y:{y.shape if isinstance(y,torch.Tensor) else type(y)}. Ret 0 loss.\")\n",
        "            return torch.tensor(0.0, device=output_device)\n",
        "        if not torch.isfinite(x).all() or not torch.isfinite(y).all():\n",
        "            if DEBUG_CELL6: print(f\"DEBUG: {self.class_prefix} Inputs NaN/Inf. x_finite: {torch.isfinite(x).all()}, y_finite: {torch.isfinite(y).all()}. Ret 0 loss.\")\n",
        "            return torch.tensor(0.0, device=output_device)\n",
        "        try:\n",
        "            raw_sinkhorn_loss = self.sinkhorn_loss_calculator(x, y)\n",
        "            if raw_sinkhorn_loss is None: print(f\"WARNING: {self.class_prefix} raw_sinkhorn_loss is None. Ret 0.\"); return torch.tensor(0.0, device=output_device)\n",
        "            final_loss_val = raw_sinkhorn_loss\n",
        "            if final_loss_val.ndim > 0 and final_loss_val.numel() > 1 : final_loss_val = final_loss_val.mean()\n",
        "            elif final_loss_val.numel() > 1 and final_loss_val.ndim == 0 : final_loss_val = final_loss_val.squeeze()\n",
        "            if not torch.isfinite(final_loss_val): print(f\"WARNING: {self.class_prefix} Sinkhorn loss NaN/Inf ({final_loss_val.item() if final_loss_val.numel()==1 else 'Non-scalar problematic'}). Ret 0.0.\"); return torch.tensor(0.0, device=output_device)\n",
        "            if DEBUG_CELL6: print(f\"DEBUG: {self.class_prefix} Computed Sinkhorn loss: {final_loss_val.item():.4f}\")\n",
        "            return final_loss_val\n",
        "        except RuntimeError as re:\n",
        "            print(f\"ERROR: {self.class_prefix} RuntimeError in geomloss: {re}\")\n",
        "            if DEBUG_CELL6:\n",
        "                print(f\"    Inputs x: {x.shape}, y: {y.shape}\\n{traceback.format_exc()}\")\n",
        "            return torch.tensor(0.0, device=output_device)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {self.class_prefix} Unexpected error in geomloss: {e}\\n{traceback.format_exc()}\")\n",
        "            return torch.tensor(0.0, device=output_device)\n",
        "\n",
        "print(f\"    {CELL6_INFO_PREFIX} SinkhornOTLoss class defined.\")\n",
        "\n",
        "class MainCriterion(nn.Module):\n",
        "    \"\"\"Main loss: Contrastive (InfoNCE-like) + Entropy + Regularization for raw modality embeddings.\"\"\"\n",
        "    def __init__(self,\n",
        "                 mixture_embedding_raw_instance: MixtureEmbedding, # Corrected parameter name\n",
        "                 lambda_kl_contrastive: float = 1.0,\n",
        "                 lambda_ent: float = 0.1,\n",
        "                 lambda_reg: float = 0.01,\n",
        "                 reg_type: str = 'mean_norm',\n",
        "                 device: Optional[torch.device] = None):\n",
        "        super().__init__()\n",
        "        self.class_prefix = f\"{CELL6_INFO_PREFIX}[MainCriterion]\"\n",
        "        self.mixture_model_for_terms = mixture_embedding_raw_instance\n",
        "        self.lambda_kl_contrastive = lambda_kl_contrastive\n",
        "        self.lambda_ent = lambda_ent\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.reg_type = reg_type.lower()\n",
        "        self.device = device if device else (self.mixture_model_for_terms.temperature.device if self.mixture_model_for_terms and hasattr(self.mixture_model_for_terms, 'temperature') else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        print(f\"    {self.class_prefix} Initialized: L_kl_cont={self.lambda_kl_contrastive}, L_ent={self.lambda_ent}, L_reg={self.lambda_reg} (type: '{self.reg_type}')\")\n",
        "        if self.mixture_model_for_terms is None: print(f\"WARNING: {self.class_prefix} mixture_model_for_terms is None. Loss terms might be zero.\")\n",
        "\n",
        "    def forward(self,\n",
        "                measures1: Dict[str, Tuple[torch.Tensor, ...]],\n",
        "                gamma1: torch.Tensor,\n",
        "                measures2: Dict[str, Tuple[torch.Tensor, ...]],\n",
        "                gamma2: torch.Tensor,\n",
        "                current_batch_size: int\n",
        "               ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "\n",
        "        loss_components = {'kl_contrastive': 0.0, 'entropy_avg': 0.0, 'reg_avg': 0.0}\n",
        "        total_main_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        if self.mixture_model_for_terms is None:\n",
        "             print(f\"ERROR: {self.class_prefix} mixture_model_for_terms is None in forward pass. Returning zero loss.\")\n",
        "             return total_main_loss, loss_components\n",
        "\n",
        "        # 1. Contrastive Loss (InfoNCE-like from pairwise distances of mu_proj)\n",
        "        if self.lambda_kl_contrastive > 0 and current_batch_size > 1 :\n",
        "            try:\n",
        "                pairwise_dist = self.mixture_model_for_terms.compute_pairwise_distances(measures1, gamma1, measures2, gamma2)\n",
        "                current_temp_raw = self.mixture_model_for_terms.temperature.data.clamp(min=1e-6, max=1.0)\n",
        "                logits = -pairwise_dist / current_temp_raw\n",
        "                labels = torch.arange(current_batch_size, device=self.device)\n",
        "                loss_i = F.cross_entropy(logits, labels, reduction='mean')\n",
        "                loss_j = F.cross_entropy(logits.t(), labels, reduction='mean')\n",
        "                kl_contrastive_loss = (loss_i + loss_j) / 2.0\n",
        "                if torch.isfinite(kl_contrastive_loss):\n",
        "                    total_main_loss += self.lambda_kl_contrastive * kl_contrastive_loss\n",
        "                    loss_components['kl_contrastive'] = kl_contrastive_loss.item()\n",
        "                else:\n",
        "                    print(f\"WARNING: {self.class_prefix} KL Contrastive loss NaN/Inf. Skipping. Logits range: {logits.min().item() if logits.numel()>0 else 'N/A'} to {logits.max().item() if logits.numel()>0 else 'N/A'}\")\n",
        "            except Exception as e_kl:\n",
        "                print(f\"ERROR: {self.class_prefix} KL Contrastive loss: {e_kl}\")\n",
        "                if DEBUG_CELL6:\n",
        "                    print(traceback.format_exc())\n",
        "\n",
        "        # 2. Entropy Terms for Gamma1 and Gamma2\n",
        "        if self.lambda_ent > 0:\n",
        "            try:\n",
        "                entropy_term1 = self.mixture_model_for_terms.compute_entropy(measures=measures1, gamma=gamma1)\n",
        "                entropy_term2 = self.mixture_model_for_terms.compute_entropy(measures=measures2, gamma=gamma2)\n",
        "                avg_entropy_term = (entropy_term1 + entropy_term2) / 2.0\n",
        "                if torch.isfinite(avg_entropy_term):\n",
        "                    total_main_loss -= self.lambda_ent * avg_entropy_term\n",
        "                    loss_components['entropy_avg'] = avg_entropy_term.item()\n",
        "                else: print(f\"WARNING: {self.class_prefix} Entropy term NaN/Inf. Skipping.\")\n",
        "            except Exception as e_ent: print(f\"ERROR: {self.class_prefix} Entropy loss: {e_ent}\")\n",
        "\n",
        "        # 3. Regularization Terms on mu_proj from measures1 and measures2\n",
        "        if self.lambda_reg > 0 and self.reg_type != 'none':\n",
        "            try:\n",
        "                accumulated_reg = torch.tensor(0.0, device=self.device); num_valid_reg_terms = 0\n",
        "                for measures_set_for_reg in [measures1, measures2]:\n",
        "                    if measures_set_for_reg:\n",
        "                        for g_key in self.mixture_model_for_terms.geom_keys:\n",
        "                            if g_key in measures_set_for_reg and isinstance(measures_set_for_reg[g_key], tuple) and len(measures_set_for_reg[g_key]) == 3:\n",
        "                                _mu, _sigma, mu_proj = measures_set_for_reg[g_key]\n",
        "                                if mu_proj is not None and isinstance(mu_proj, torch.Tensor) and torch.isfinite(mu_proj).all() and mu_proj.numel() > 0:\n",
        "                                    term_val = torch.tensor(0.0, device=self.device)\n",
        "                                    if self.reg_type == 'mean_norm': term_val = torch.norm(mu_proj, p=2, dim=-1).mean()\n",
        "                                    elif self.reg_type == 'cov_diag_sum' and _sigma is not None and isinstance(_sigma, torch.Tensor) and torch.isfinite(_sigma).all() and _sigma.numel() > 0:\n",
        "                                        if _sigma.ndim >=2 and _sigma.shape[-1] == _sigma.shape[-2]: term_val = torch.diagonal(_sigma, dim1=-2, dim2=-1).sum(dim=-1).mean()\n",
        "                                    if torch.isfinite(term_val): accumulated_reg += term_val; num_valid_reg_terms +=1\n",
        "                if num_valid_reg_terms > 0:\n",
        "                    reg_term = accumulated_reg / num_valid_reg_terms\n",
        "                    if torch.isfinite(reg_term): total_main_loss += self.lambda_reg * reg_term; loss_components['reg_avg'] = reg_term.item()\n",
        "                    else: print(f\"WARNING: {self.class_prefix} Reg term NaN/Inf after averaging. Skipping.\")\n",
        "            except Exception as e_reg: print(f\"ERROR: {self.class_prefix} Regularization loss: {e_reg}\")\n",
        "\n",
        "        if not torch.isfinite(total_main_loss):\n",
        "            print(f\"CRITICAL WARNING: {self.class_prefix} Total main_loss NaN/Inf! Forcing to 0. Comps: {loss_components}\")\n",
        "            total_main_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        # Standardize output dict keys to match Cell 9's aggregation keys\n",
        "        final_log_components_main = {\n",
        "            'kl': loss_components.get('kl_contrastive', float('nan')), # Cell 9 used 'kl'\n",
        "            'ent': loss_components.get('entropy_avg', float('nan')),   # Cell 9 used 'ent'\n",
        "            'reg': loss_components.get('reg_avg', float('nan')),     # Cell 9 used 'reg'\n",
        "        }\n",
        "        return total_main_loss, final_log_components_main # Return only components from this criterion\n",
        "\n",
        "print(f\"    {CELL6_INFO_PREFIX} MainCriterion class defined.\")\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))}] {CELL6_INFO_PREFIX} Loss function definitions complete.\")"
      ],
      "metadata": {
        "id": "kyz_biKCMeHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7"
      ],
      "metadata": {
        "id": "Yqb3GHayRrIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 7: Training Utilities ---\n",
        "# Defines LR scheduler, logging (WandB/TensorBoard), and checkpointing helpers.\n",
        "# Enhanced for robustness, granularity, and debugging.\n",
        "\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.optim # For torch.optim.Optimizer\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "from typing import Dict, Optional, Any, List # List was already there\n",
        "from dataclasses import asdict\n",
        "import numpy as np # For np.isfinite\n",
        "import traceback # For detailed error logging\n",
        "\n",
        "# --- Configuration for this Cell ---\n",
        "CELL_INFO_PREFIX = \"[Cell 7]\"\n",
        "# Set to True for verbose debugging printouts, mirroring Cell 5's approach\n",
        "DEBUG_CELL7 = True # Or ideally, set this from a global config: config.get(\"debug_mode\", False)\n",
        "\n",
        "# --- Pre-run Checks & Placeholder Definitions ---\n",
        "print(f\"{CELL_INFO_PREFIX} Performing pre-run checks and placeholder definitions...\")\n",
        "\n",
        "# Configuration object check\n",
        "if 'config' not in globals():\n",
        "    raise NameError(f\"FATAL: {CELL_INFO_PREFIX} 'config' not defined. Ensure Cell 3 (ModelConfig) has been run successfully.\")\n",
        "\n",
        "# DEVICE check\n",
        "if 'DEVICE' not in globals():\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"INFO: {CELL_INFO_PREFIX} 'DEVICE' not found in globals. Auto-defined DEVICE: {DEVICE}\")\n",
        "else:\n",
        "    # Ensure DEVICE is a torch.device object if it exists\n",
        "    if not isinstance(DEVICE, torch.device):\n",
        "        print(f\"WARNING: {CELL_INFO_PREFIX} 'DEVICE' was in globals but not a torch.device. Re-initializing.\")\n",
        "        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"INFO: {CELL_INFO_PREFIX} Auto-defined DEVICE: {DEVICE}\")\n",
        "\n",
        "# Placeholder for ModelConfig if not defined (for type hints and basic structure)\n",
        "# This allows the cell to be parsed and functions defined,\n",
        "# but runtime execution of functions requiring config will need a proper config object.\n",
        "if 'ModelConfig' not in globals():\n",
        "    print(f\"INFO: {CELL_INFO_PREFIX} 'ModelConfig' class not found in globals. Defining a basic placeholder for type hinting and attribute access.\")\n",
        "    class ModelConfig:\n",
        "        # Define attributes that are expected by Cell 7 to prevent AttributeError if a placeholder is used\n",
        "        # These should ideally match the actual ModelConfig from Cell 3 or be comprehensive enough for Cell 7\n",
        "        use_lr_scheduler: bool = False\n",
        "        scheduler_type: Optional[str] = None\n",
        "        num_epochs: int = 20\n",
        "        scheduler_warmup_epochs: int = 0\n",
        "        scheduler_num_cycles: float = 0.5\n",
        "        lr: float = 1e-3\n",
        "        checkpoint_dir: str = \"./checkpoints\"\n",
        "        use_wandb: bool = False\n",
        "        wandb_project: Optional[str] = \"my_project\"\n",
        "        wandb_entity: Optional[str] = \"my_entity\"\n",
        "        wandb_run_name: Optional[str] = None\n",
        "        # Add any other config attributes directly accessed in this cell\n",
        "\n",
        "# Placeholder for MixtureEmbedding\n",
        "if 'MixtureEmbedding' not in globals():\n",
        "    print(f\"INFO: {CELL_INFO_PREFIX} 'MixtureEmbedding' class not found in globals. Defining a basic placeholder.\")\n",
        "    class MixtureEmbedding(torch.nn.Module): pass # Inherit from nn.Module for state_dict\n",
        "\n",
        "# Ensure torch.nn is available for nn.Module type hint\n",
        "if 'nn' not in globals():\n",
        "    import torch.nn as nn\n",
        "    print(f\"INFO: {CELL_INFO_PREFIX} Imported torch.nn as nn for type hinting.\")\n",
        "\n",
        "print(f\"{CELL_INFO_PREFIX} Pre-run checks and placeholders complete.\")\n",
        "\n",
        "CURRENT_TIME_START_CELL7 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL7))}] {CELL_INFO_PREFIX} Initializing: Defining Training Utilities...\")\n",
        "print(f\"{CELL_INFO_PREFIX} Using DEVICE: {DEVICE}\")\n",
        "print(f\"{CELL_INFO_PREFIX} Debug Mode: {'ON' if DEBUG_CELL7 else 'OFF'}\")\n",
        "\n",
        "# --- 1. Learning Rate Scheduler ---\n",
        "SCHEDULER_PREFIX = f\"{CELL_INFO_PREFIX}[Scheduler]\"\n",
        "\n",
        "def get_lr_scheduler(optimizer: torch.optim.Optimizer,\n",
        "                       cfg: ModelConfig,\n",
        "                       steps_per_epoch: int) -> Optional[torch.optim.lr_scheduler._LRScheduler]:\n",
        "    \"\"\"Initializes and returns a learning rate scheduler based on the configuration.\"\"\"\n",
        "    func_prefix = f\"{SCHEDULER_PREFIX}[get_lr_scheduler]\"\n",
        "    print(f\"{func_prefix} Attempting to initialize LR scheduler...\")\n",
        "\n",
        "    if not hasattr(cfg, 'use_lr_scheduler') or not cfg.use_lr_scheduler:\n",
        "        print(f\"{func_prefix} LR Scheduler disabled via cfg.use_lr_scheduler=False.\")\n",
        "        return None\n",
        "    if not hasattr(cfg, 'scheduler_type') or not cfg.scheduler_type:\n",
        "        print(f\"{func_prefix} LR Scheduler disabled due to missing cfg.scheduler_type.\")\n",
        "        return None\n",
        "    if not isinstance(steps_per_epoch, int) or steps_per_epoch <= 0:\n",
        "        print(f\"WARNING: {func_prefix} Invalid steps_per_epoch ({steps_per_epoch}). Scheduler might not behave as expected or be disabled.\")\n",
        "        # Depending on scheduler type, this might be critical. For now, proceed with caution.\n",
        "\n",
        "    # Ensure necessary config attributes exist\n",
        "    num_epochs = getattr(cfg, 'num_epochs', 1)\n",
        "    warmup_epochs = getattr(cfg, 'scheduler_warmup_epochs', 0)\n",
        "    lr_for_fallback = getattr(cfg, 'lr', 1e-3) # For CosineAnnealingLR eta_min\n",
        "    num_cycles = getattr(cfg, 'scheduler_num_cycles', 0.5) # For Hugging Face cosine\n",
        "\n",
        "    total_steps = num_epochs * steps_per_epoch\n",
        "    warmup_steps = warmup_epochs * steps_per_epoch\n",
        "\n",
        "    print(f\"{func_prefix} Config: Type='{cfg.scheduler_type}', TotalSteps={total_steps}, WarmupSteps={warmup_steps}, Epochs={num_epochs}, StepsPerEp={steps_per_epoch}\")\n",
        "\n",
        "    scheduler_type_lower = cfg.scheduler_type.lower()\n",
        "    scheduler_instance = None\n",
        "\n",
        "    if scheduler_type_lower == \"cosine_with_warmup\":\n",
        "        try:\n",
        "            from transformers import get_cosine_schedule_with_warmup\n",
        "            scheduler_instance = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, num_cycles)\n",
        "            print(f\"{func_prefix} Using HuggingFace get_cosine_schedule_with_warmup.\")\n",
        "        except ImportError:\n",
        "            print(f\"WARNING: {func_prefix} 'transformers.get_cosine_schedule_with_warmup' not found. Falling back to PyTorch CosineAnnealingLR.\")\n",
        "            t_max_cosine = total_steps - warmup_steps if total_steps > warmup_steps else total_steps\n",
        "            if t_max_cosine <= 0 : t_max_cosine = 1 # Ensure T_max is positive\n",
        "            scheduler_instance = lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max_cosine, eta_min=lr_for_fallback * 0.01)\n",
        "            print(f\"{func_prefix} Using PyTorch CosineAnnealingLR (T_max={t_max_cosine}, eta_min={lr_for_fallback * 0.01}). Warmup not directly handled by this scheduler.\")\n",
        "    elif scheduler_type_lower == \"linear_with_warmup\":\n",
        "        try:\n",
        "            from transformers import get_linear_schedule_with_warmup\n",
        "            scheduler_instance = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "            print(f\"{func_prefix} Using HuggingFace get_linear_schedule_with_warmup.\")\n",
        "        except ImportError:\n",
        "            print(f\"WARNING: {func_prefix} 'transformers.get_linear_schedule_with_warmup' not found. Scheduler will be disabled.\")\n",
        "            scheduler_instance = None\n",
        "    elif scheduler_type_lower == \"reduce_on_plateau\":\n",
        "        patience = getattr(cfg, 'scheduler_patience', 3)\n",
        "        factor = getattr(cfg, 'scheduler_factor', 0.1)\n",
        "        scheduler_instance = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True)\n",
        "        print(f\"{func_prefix} Using PyTorch ReduceLROnPlateau (Factor={factor}, Patience={patience}).\")\n",
        "    else:\n",
        "        print(f\"WARNING: {func_prefix} Unsupported scheduler type: '{cfg.scheduler_type}'. Scheduler will be disabled.\")\n",
        "        scheduler_instance = None\n",
        "\n",
        "    if scheduler_instance:\n",
        "        print(f\"{func_prefix} LR Scheduler '{cfg.scheduler_type}' initialized successfully.\")\n",
        "    else:\n",
        "        print(f\"{func_prefix} LR Scheduler initialization failed or disabled.\")\n",
        "    return scheduler_instance\n",
        "\n",
        "print(f\"{SCHEDULER_PREFIX} get_lr_scheduler function defined.\")\n",
        "\n",
        "# --- 2. Logging Utilities (TensorBoard & WandB) ---\n",
        "LOGGING_PREFIX = f\"{CELL_INFO_PREFIX}[Logging]\"\n",
        "\n",
        "# TensorBoard Writer Initialization\n",
        "tb_writer: Optional[SummaryWriter] = None\n",
        "try:\n",
        "    # Ensure checkpoint_dir attribute exists on config\n",
        "    tb_log_dir_base = getattr(config, 'checkpoint_dir', './checkpoints_fallback')\n",
        "    if tb_log_dir_base == './checkpoints_fallback':\n",
        "        print(f\"WARNING: {LOGGING_PREFIX} config.checkpoint_dir not found, using default './checkpoints_fallback' for TensorBoard.\")\n",
        "\n",
        "    log_dir_tb = os.path.join(tb_log_dir_base, \"tensorboard_logs\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    os.makedirs(log_dir_tb, exist_ok=True)\n",
        "    tb_writer = SummaryWriter(log_dir=log_dir_tb)\n",
        "    print(f\"{LOGGING_PREFIX} TensorBoard SummaryWriter initialized. Log directory: {tb_writer.log_dir}\")\n",
        "except Exception as e_tb:\n",
        "    print(f\"ERROR: {LOGGING_PREFIX} TensorBoard SummaryWriter initialization failed: {e_tb}\")\n",
        "    print(traceback.format_exc())\n",
        "    tb_writer = None\n",
        "\n",
        "def setup_wandb_logging(cfg: ModelConfig, job_type: str = \"training\") -> bool:\n",
        "    \"\"\"Initializes Weights & Biases logging if configured.\"\"\"\n",
        "    func_prefix = f\"{LOGGING_PREFIX}[WandBSetup]\"\n",
        "    print(f\"{func_prefix} Attempting to initialize WandB logging...\")\n",
        "\n",
        "    if not hasattr(cfg, 'use_wandb') or not cfg.use_wandb:\n",
        "        print(f\"{func_prefix} WandB logging disabled via cfg.use_wandb=False.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        project_name = getattr(cfg, 'wandb_project', None)\n",
        "        entity_name = getattr(cfg, 'wandb_entity', None) # Optional\n",
        "        run_name_cfg = getattr(cfg, 'wandb_run_name', None)\n",
        "\n",
        "        if not project_name:\n",
        "            print(f\"ERROR: {func_prefix} cfg.wandb_project is not set. WandB cannot be initialized.\")\n",
        "            return False\n",
        "\n",
        "        run_name = run_name_cfg if run_name_cfg else f\"pairwise_run_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        # Filter config for wandb (convert ModelConfig to dict, handle non-serializable if any)\n",
        "        wandb_config = {}\n",
        "        if isinstance(cfg, ModelConfig) or hasattr(cfg, '__dict__') or hasattr(cfg, '_asdict'): # handles dataclasses and regular classes\n",
        "            try:\n",
        "                wandb_config = asdict(cfg) # Best for dataclasses\n",
        "            except TypeError: # Fallback for regular classes\n",
        "                wandb_config = {k: v for k, v in cfg.__dict__.items() if not k.startswith('_')}\n",
        "\n",
        "        # Sanitize config for wandb (e.g. remove complex objects if any)\n",
        "        for key, value in list(wandb_config.items()):\n",
        "            if not isinstance(value, (str, int, float, bool, list, dict, tuple, type(None))):\n",
        "                 wandb_config[key] = str(value)\n",
        "\n",
        "\n",
        "        wandb.init(\n",
        "            project=project_name,\n",
        "            entity=entity_name, # Can be None\n",
        "            name=run_name,\n",
        "            config=wandb_config,\n",
        "            job_type=job_type,\n",
        "            reinit=True, # Allow reinitialization in notebooks\n",
        "            settings=wandb.Settings(start_method=\"thread\") # Good for notebooks\n",
        "        )\n",
        "        print(f\"{func_prefix} WandB logging initialized: Project='{project_name}', Run='{run_name}'.\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"ERROR: {func_prefix} 'wandb' library not found. Please install it. Disabling WandB.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {func_prefix} WandB initialization failed: {e}. Disabling WandB.\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    # Ensure use_wandb is False if setup failed\n",
        "    if hasattr(cfg, 'use_wandb'): cfg.use_wandb = False\n",
        "    return False\n",
        "\n",
        "print(f\"{LOGGING_PREFIX} setup_wandb_logging function defined.\")\n",
        "\n",
        "def log_metrics_to_wandb(metrics: Dict[str, Any], step: int, epoch: int, cfg: ModelConfig, prefix: str = \"\"):\n",
        "    \"\"\"Logs metrics to WandB if enabled and initialized.\"\"\"\n",
        "    if hasattr(cfg, 'use_wandb') and cfg.use_wandb and wandb.run is not None:\n",
        "        payload = {}\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, (int, float, torch.Tensor)):\n",
        "                if isinstance(value, torch.Tensor):\n",
        "                    if value.numel() == 1:\n",
        "                        value = value.item() # Convert single-element tensor to scalar\n",
        "                    else:\n",
        "                        if DEBUG_CELL7: print(f\"INFO: {LOGGING_PREFIX}[WandB] Metric '{prefix}{key}' is a multi-element tensor, cannot log directly. Skipping.\")\n",
        "                        continue # WandB typically expects scalar values for time-series metrics\n",
        "\n",
        "                # np.isfinite expects float/int, not tensor\n",
        "                if isinstance(value, (int, float)) and np.isfinite(value):\n",
        "                    payload[f\"{prefix}{key.replace('.', '_')}\"] = value\n",
        "                elif DEBUG_CELL7:\n",
        "                     print(f\"INFO: {LOGGING_PREFIX}[WandB] Metric '{prefix}{key}' value {value} is not finite. Skipping.\")\n",
        "            elif DEBUG_CELL7:\n",
        "                print(f\"INFO: {LOGGING_PREFIX}[WandB] Metric '{prefix}{key}' type {type(value)} not directly loggable. Skipping.\")\n",
        "\n",
        "        if payload:\n",
        "            payload[f\"{prefix}epoch_num\"] = epoch # Consistent naming\n",
        "            try:\n",
        "                wandb.log(payload, step=step)\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: {LOGGING_PREFIX}[WandB] Failed to log metrics: {e}\")\n",
        "\n",
        "def log_metrics_to_tensorboard(metrics: Dict[str, Any], step: int, epoch: int, prefix: str = \"\"):\n",
        "    \"\"\"Logs metrics to TensorBoard if enabled and initialized.\"\"\"\n",
        "    # epoch is not directly used by add_scalar, global_step (step) is the x-axis\n",
        "    if tb_writer is not None:\n",
        "        for key, value in metrics.items():\n",
        "            try:\n",
        "                if isinstance(value, (int, float, torch.Tensor)):\n",
        "                    if isinstance(value, torch.Tensor):\n",
        "                        if value.numel() == 1:\n",
        "                            value = value.item()\n",
        "                        else:\n",
        "                            if DEBUG_CELL7: print(f\"INFO: {LOGGING_PREFIX}[TB] Metric '{prefix}{key}' is a multi-element tensor, cannot log as scalar. Skipping.\")\n",
        "                            continue\n",
        "\n",
        "                    if isinstance(value, (int,float)) and np.isfinite(value):\n",
        "                         tb_writer.add_scalar(f\"{prefix}{key.replace('.', '_')}\", value, global_step=step)\n",
        "                    elif DEBUG_CELL7:\n",
        "                        print(f\"INFO: {LOGGING_PREFIX}[TB] Metric '{prefix}{key}' value {value} is not finite for TensorBoard. Skipping.\")\n",
        "                elif DEBUG_CELL7:\n",
        "                    print(f\"INFO: {LOGGING_PREFIX}[TB] Metric '{prefix}{key}' type {type(value)} not directly loggable as scalar. Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: {LOGGING_PREFIX}[TB] Failed to log metric '{prefix}{key}': {e}\")\n",
        "        try:\n",
        "            tb_writer.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {LOGGING_PREFIX}[TB] Failed to flush TensorBoard writer: {e}\")\n",
        "\n",
        "print(f\"{LOGGING_PREFIX} WandB & TensorBoard logging helper functions defined.\")\n",
        "\n",
        "# --- 3. Checkpoint Utilities ---\n",
        "CHECKPOINT_PREFIX = f\"{CELL_INFO_PREFIX}[Checkpoint]\"\n",
        "\n",
        "def save_checkpoint(epoch: int,\n",
        "                    models_for_pairs: Dict[str, nn.Module],\n",
        "                    optimizers_for_pairs: Dict[str, torch.optim.Optimizer],\n",
        "                    schedulers_for_pairs: Dict[str, Optional[torch.optim.lr_scheduler._LRScheduler]],\n",
        "                    mixture_embedding_for_raw: MixtureEmbedding,\n",
        "                    mixture_embedding_for_fused: MixtureEmbedding,\n",
        "                    cfg: ModelConfig,\n",
        "                    metrics: Optional[Dict[str, Any]] = None,\n",
        "                    is_best: bool = False,\n",
        "                    prefix_filename: str = \"ckpt\"):\n",
        "    \"\"\"Saves a training checkpoint.\"\"\"\n",
        "    func_prefix = f\"{CHECKPOINT_PREFIX}[save]\"\n",
        "    print(f\"{func_prefix} Attempting to save checkpoint for epoch {epoch}...\")\n",
        "\n",
        "    checkpoint_dir = getattr(cfg, 'checkpoint_dir', './checkpoints_fallback')\n",
        "    if checkpoint_dir == './checkpoints_fallback':\n",
        "        print(f\"WARNING: {func_prefix} config.checkpoint_dir not found, using default './checkpoints_fallback'.\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            print(f\"{func_prefix} Created checkpoint directory: {checkpoint_dir}\")\n",
        "    except OSError as e:\n",
        "        print(f\"ERROR: {func_prefix} Could not create checkpoint directory '{checkpoint_dir}': {e}. Checkpoint will not be saved.\")\n",
        "        return\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base_filename = f\"{prefix_filename}_epoch_{epoch + 1}_{timestamp}.pth\"\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, base_filename)\n",
        "\n",
        "    model_states = {name: model.state_dict() for name, model in models_for_pairs.items()}\n",
        "    optimizer_states = {name: opt.state_dict() for name, opt in optimizers_for_pairs.items()}\n",
        "    scheduler_states = {name: sch.state_dict() for name, sch in schedulers_for_pairs.items() if sch is not None}\n",
        "\n",
        "    # Ensure config is serializable\n",
        "    serializable_cfg = {}\n",
        "    if isinstance(cfg, ModelConfig) or hasattr(cfg, '__dict__') or hasattr(cfg, '_asdict'):\n",
        "        try: serializable_cfg = asdict(cfg)\n",
        "        except TypeError: serializable_cfg = {k: v for k, v in cfg.__dict__.items() if not k.startswith('_')}\n",
        "\n",
        "    for key, value in list(serializable_cfg.items()): # Sanitize\n",
        "        if not isinstance(value, (str, int, float, bool, list, dict, tuple, type(None))):\n",
        "             serializable_cfg[key] = str(value)\n",
        "\n",
        "    save_object = {\n",
        "        'epoch': epoch + 1, # Save as next epoch to resume from\n",
        "        'config_snapshot': serializable_cfg,\n",
        "        'models_state_dict': model_states,\n",
        "        'optimizers_state_dict': optimizer_states,\n",
        "        'mixture_embedding_for_raw_state_dict': mixture_embedding_for_raw.state_dict(),\n",
        "        'mixture_embedding_for_fused_state_dict': mixture_embedding_for_fused.state_dict(),\n",
        "        'metrics': metrics if metrics else {}\n",
        "    }\n",
        "    if scheduler_states:\n",
        "        save_object['schedulers_state_dict'] = scheduler_states\n",
        "\n",
        "    if DEBUG_CELL7:\n",
        "        print(f\"{func_prefix} Saving following components: {list(save_object.keys())}\")\n",
        "        print(f\"{func_prefix} Model keys: {list(model_states.keys())}\")\n",
        "\n",
        "    try:\n",
        "        torch.save(save_object, checkpoint_path)\n",
        "        print(f\"{func_prefix} Checkpoint saved successfully: '{checkpoint_path}'\")\n",
        "        if is_best:\n",
        "            best_filename = f\"{prefix_filename}_best.pth\"\n",
        "            best_path = os.path.join(checkpoint_dir, best_filename)\n",
        "            torch.save(save_object, best_path) # Overwrite previous best\n",
        "            print(f\"{func_prefix} Saved current checkpoint as BEST model: '{best_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {func_prefix} Failed to save checkpoint to '{checkpoint_path}': {e}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "print(f\"{CHECKPOINT_PREFIX} save_checkpoint function defined.\")\n",
        "\n",
        "def load_checkpoint(cfg: ModelConfig,\n",
        "                    models_for_pairs: Dict[str, nn.Module],\n",
        "                    optimizers_for_pairs: Dict[str, torch.optim.Optimizer],\n",
        "                    schedulers_for_pairs: Dict[str, Optional[torch.optim.lr_scheduler._LRScheduler]],\n",
        "                    mixture_embedding_for_raw: MixtureEmbedding,\n",
        "                    mixture_embedding_for_fused: MixtureEmbedding,\n",
        "                    filename: Optional[str] = None,\n",
        "                    load_strict: bool = False) -> int: # Start from epoch 0 if no checkpoint\n",
        "    \"\"\"Loads a training checkpoint. Returns the starting epoch.\"\"\"\n",
        "    func_prefix = f\"{CHECKPOINT_PREFIX}[load]\"\n",
        "    starting_epoch = 0 # Default if no checkpoint or load fails\n",
        "\n",
        "    checkpoint_dir = getattr(cfg, 'checkpoint_dir', './checkpoints_fallback')\n",
        "    if checkpoint_dir == './checkpoints_fallback' and not filename:\n",
        "         print(f\"WARNING: {func_prefix} config.checkpoint_dir not found and no specific filename given. Using './checkpoints_fallback'.\")\n",
        "\n",
        "\n",
        "    checkpoint_path_to_load = None\n",
        "    if filename: # Specific file provided\n",
        "        checkpoint_path_to_load = filename if os.path.isabs(filename) else os.path.join(checkpoint_dir, filename)\n",
        "        print(f\"{func_prefix} Attempting to load specified checkpoint: '{checkpoint_path_to_load}'\")\n",
        "    else: # Find latest checkpoint in dir\n",
        "        print(f\"{func_prefix} No specific filename. Searching for latest checkpoint in '{checkpoint_dir}'...\")\n",
        "        if not os.path.isdir(checkpoint_dir):\n",
        "            print(f\"INFO: {func_prefix} Checkpoint directory '{checkpoint_dir}' not found. Starting fresh (epoch 0).\")\n",
        "            return starting_epoch\n",
        "\n",
        "        # Filter for primary checkpoint files (not 'best')\n",
        "        ckpt_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"ckpt_epoch_\") and f.endswith(\".pth\") and \"_best\" not in f]\n",
        "        if not ckpt_files:\n",
        "            print(f\"INFO: {func_prefix} No 'ckpt_epoch_*.pth' files found in '{checkpoint_dir}'. Starting fresh (epoch 0).\")\n",
        "            return starting_epoch\n",
        "\n",
        "        # Sort by epoch number (assuming format \"ckpt_epoch_NUM_TIMESTAMP.pth\")\n",
        "        def get_epoch_from_filename(fname):\n",
        "            try: return int(fname.split('_epoch_')[1].split('_')[0])\n",
        "            except (IndexError, ValueError): return -1 # Invalid format, sort to beginning\n",
        "\n",
        "        ckpt_files.sort(key=get_epoch_from_filename, reverse=True)\n",
        "        if get_epoch_from_filename(ckpt_files[0]) == -1: # Check if sorting failed due to bad names\n",
        "             print(f\"WARNING: {func_prefix} Could not reliably sort checkpoint files by epoch due to naming convention. Using the first valid-looking file if any.\")\n",
        "\n",
        "        checkpoint_path_to_load = os.path.join(checkpoint_dir, ckpt_files[0])\n",
        "        print(f\"{func_prefix} Found latest checkpoint candidate: '{checkpoint_path_to_load}'\")\n",
        "\n",
        "    if not checkpoint_path_to_load or not os.path.exists(checkpoint_path_to_load):\n",
        "        print(f\"WARNING: {func_prefix} Checkpoint file not found: '{checkpoint_path_to_load}'. Starting fresh (epoch 0).\")\n",
        "        return starting_epoch\n",
        "\n",
        "    print(f\"{func_prefix} Loading checkpoint from '{checkpoint_path_to_load}' to DEVICE: {DEVICE}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path_to_load, map_location=DEVICE)\n",
        "\n",
        "        # Load model states\n",
        "        ckpt_model_states = checkpoint.get('models_state_dict', {})\n",
        "        for name, model_instance in models_for_pairs.items():\n",
        "            if name in ckpt_model_states:\n",
        "                try:\n",
        "                    model_instance.load_state_dict(ckpt_model_states[name], strict=load_strict)\n",
        "                    print(f\"    SUCCESS: Loaded state for model '{name}' (Strict: {load_strict}).\")\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"    WARNING: RuntimeError loading state for model '{name}' (Strict: {load_strict}). Error: {e}. Model may be partially loaded or not at all.\")\n",
        "                    if DEBUG_CELL7: print(traceback.format_exc())\n",
        "            else:\n",
        "                print(f\"    INFO: No state found for model '{name}' in checkpoint.\")\n",
        "\n",
        "        # Load mixture embedding states\n",
        "        if 'mixture_embedding_for_raw_state_dict' in checkpoint:\n",
        "            mixture_embedding_for_raw.load_state_dict(checkpoint['mixture_embedding_for_raw_state_dict'], strict=load_strict)\n",
        "            print(f\"    SUCCESS: Loaded state for 'mixture_embedding_for_raw' (Strict: {load_strict}).\")\n",
        "        if 'mixture_embedding_for_fused_state_dict' in checkpoint:\n",
        "            mixture_embedding_for_fused.load_state_dict(checkpoint['mixture_embedding_for_fused_state_dict'], strict=load_strict)\n",
        "            print(f\"    SUCCESS: Loaded state for 'mixture_embedding_for_fused' (Strict: {load_strict}).\")\n",
        "\n",
        "        # Load optimizer states (be cautious if model architecture changed)\n",
        "        ckpt_optimizer_states = checkpoint.get('optimizers_state_dict', {})\n",
        "        for name, optimizer_instance in optimizers_for_pairs.items():\n",
        "            if name in ckpt_optimizer_states:\n",
        "                try:\n",
        "                    optimizer_instance.load_state_dict(ckpt_optimizer_states[name])\n",
        "                    print(f\"    SUCCESS: Loaded state for optimizer '{name}'.\")\n",
        "                except Exception as e_opt: # Catching general Exception as optimizer loading can be tricky\n",
        "                    print(f\"    WARNING: Failed to load state for optimizer '{name}'. Optimizer state might be reset. Error: {e_opt}\")\n",
        "            else:\n",
        "                print(f\"    INFO: No state found for optimizer '{name}' in checkpoint.\")\n",
        "\n",
        "        # Load scheduler states\n",
        "        ckpt_scheduler_states = checkpoint.get('schedulers_state_dict', {})\n",
        "        if ckpt_scheduler_states: # Only try if key exists\n",
        "            for name, scheduler_instance in schedulers_for_pairs.items():\n",
        "                if scheduler_instance and name in ckpt_scheduler_states: # Check if scheduler exists and has state in ckpt\n",
        "                    try:\n",
        "                        scheduler_instance.load_state_dict(ckpt_scheduler_states[name])\n",
        "                        print(f\"    SUCCESS: Loaded state for scheduler '{name}'.\")\n",
        "                    except Exception as e_sch:\n",
        "                        print(f\"    WARNING: Failed to load state for scheduler '{name}'. Scheduler state might be reset. Error: {e_sch}\")\n",
        "                elif scheduler_instance: # Scheduler exists but no state in ckpt\n",
        "                    print(f\"    INFO: No state found for scheduler '{name}' in checkpoint.\")\n",
        "\n",
        "        starting_epoch = checkpoint.get('epoch', 0) # epoch in ckpt is the one _after_ completion\n",
        "        print(f\"{func_prefix} Checkpoint loaded successfully. Resuming from epoch {starting_epoch}.\")\n",
        "\n",
        "        if DEBUG_CELL7 and 'config_snapshot' in checkpoint:\n",
        "            print(f\"{func_prefix} Config snapshot from checkpoint:\")\n",
        "            # for k, v in checkpoint['config_snapshot'].items(): print(f\"        {k}: {v}\") # Can be very verbose\n",
        "            print(f\"        Snapshot keys: {list(checkpoint['config_snapshot'].keys())}\")\n",
        "\n",
        "\n",
        "        return starting_epoch\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {func_prefix} Critical error loading checkpoint from '{checkpoint_path_to_load}': {e}. Starting fresh (epoch 0).\")\n",
        "        print(traceback.format_exc())\n",
        "        return 0 # Ensure returning 0 on critical failure\n",
        "\n",
        "print(f\"{CHECKPOINT_PREFIX} load_checkpoint function defined.\")\n",
        "\n",
        "# --- Cell Completion ---\n",
        "CURRENT_TIME_END_CELL7 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL7))}] {CELL_INFO_PREFIX} Training Utilities defined. Duration: {CURRENT_TIME_END_CELL7 - CURRENT_TIME_START_CELL7:.2f}s\")"
      ],
      "metadata": {
        "id": "IqBpHoDUMbGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8"
      ],
      "metadata": {
        "id": "2Cd96d7ARskn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 8: Model & Training Setup (Corrected Loss Instantiation & Full Setup) ---\n",
        "# Instantiates all components based on the configuration, using robust loss definitions\n",
        "# from Cell 6, models from Cell 5, data from Cell 4, and utilities from Cell 7.\n",
        "# Loads checkpoint if available.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import IterableDataset # For steps_per_epoch logic\n",
        "from typing import Dict, Optional, List, Any\n",
        "import traceback # For detailed error logging\n",
        "\n",
        "CELL8_INFO_PREFIX = \"[Cell 8]\"\n",
        "# Ensure config is available globally, or set a default for DEBUG_CELL8\n",
        "DEBUG_CELL8 = getattr(config, 'debug_mode', True) if 'config' in globals() else True\n",
        "\n",
        "# --- Critical Checks for necessary variables from previous cells ---\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL8_INFO_PREFIX} Performing critical component checks...\")\n",
        "critical_components_cell8 = [\n",
        "    'config', 'DEVICE', 'seed_everything',\n",
        "    'train_loaders', 'val_loaders', 'test_loaders', # Expect these from Cell 4\n",
        "    'models_for_pairs',\n",
        "    'mixture_embedding_for_fused', 'mixture_embedding_for_raw',\n",
        "    'MainCriterion', 'SinkhornOTLoss', 'GEOMOSS_AVAILABLE_CELL6', # From Cell 6\n",
        "    'get_lr_scheduler', 'setup_wandb_logging', 'load_checkpoint', 'save_checkpoint', # From Cell 7\n",
        "    'tb_writer', 'log_metrics_to_wandb', 'log_metrics_to_tensorboard', # From Cell 7\n",
        "    'DatasetPairConfig', 'MixtureEmbedding', 'PairModel' # Classes for type hints\n",
        "]\n",
        "all_crit_verified_c8 = True\n",
        "missing_c8_components = []\n",
        "for comp_c8 in critical_components_cell8:\n",
        "    if comp_c8 not in globals():\n",
        "        # sinkhorn_ot_loss_fn is allowed to be None if geomloss is not available or lambda_sb is 0\n",
        "        # It's instantiated later, so check for its class SinkhornOTLoss and GEOMOSS_AVAILABLE_CELL6\n",
        "        if comp_c8 == 'SinkhornOTLoss' and ('GEOMOSS_AVAILABLE_CELL6' not in globals() or not GEOMOSS_AVAILABLE_CELL6):\n",
        "            print(f\"    INFO: {CELL8_INFO_PREFIX} Optional component class '{comp_c8}' not found (geomloss likely unavailable).\")\n",
        "            globals()[comp_c8] = object # Placeholder to pass isinstance check if class itself is missing\n",
        "            continue\n",
        "        print(f\"FATAL ERROR: {CELL8_INFO_PREFIX} Essential component '{comp_c8}' not found in globals. Ensure Cells 1-7 ran successfully.\")\n",
        "        missing_c8_components.append(comp_c8)\n",
        "        all_crit_verified_c8 = False\n",
        "if not all_crit_verified_c8:\n",
        "    raise NameError(f\"FATAL ERROR: {CELL8_INFO_PREFIX} Prerequisite component(s) missing: {missing_c8_components}\")\n",
        "\n",
        "# Specific check for mixture_embedding_for_raw, as it's crucial for MainCriterion\n",
        "if 'mixture_embedding_for_raw' not in globals() or not isinstance(globals()['mixture_embedding_for_raw'], MixtureEmbedding):\n",
        "    raise NameError(f\"FATAL: {CELL8_INFO_PREFIX} 'mixture_embedding_for_raw' (instance of MixtureEmbedding from Cell 5) is required for MainCriterion but not found or invalid type.\")\n",
        "\n",
        "print(f\"    {CELL8_INFO_PREFIX} All critical components verified.\")\n",
        "\n",
        "CURRENT_TIME_START_CELL8 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL8))}] {CELL8_INFO_PREFIX} Setting up Model, Loss, Optimizer, DataLoaders for Training...\")\n",
        "print(f\"    {CELL8_INFO_PREFIX} Using DEVICE: {DEVICE}, Debug Mode: {DEBUG_CELL8}\")\n",
        "\n",
        "# --- 1. Apply Seed ---\n",
        "seed_everything(config.seed) # Function from Cell 1\n",
        "print(f\"    {CELL8_INFO_PREFIX} Global random seed set to {config.seed} (re-applied).\")\n",
        "\n",
        "# --- 2. Determine Steps per Epoch ---\n",
        "print(f\"{CELL8_INFO_PREFIX} Configuring steps per epoch...\")\n",
        "steps_per_epoch = 0\n",
        "_training_can_proceed_cell8 = True\n",
        "\n",
        "# Check if train_loaders itself exists and is a non-empty dictionary\n",
        "if not train_loaders or not isinstance(train_loaders, dict) or not train_loaders.keys():\n",
        "    _training_can_proceed_cell8 = False\n",
        "    print(f\"WARNING: {CELL8_INFO_PREFIX} 'train_loaders' is empty or not found. Training cannot proceed. Check Cell 4 output.\")\n",
        "    # If you want to allow running without train_loaders (e.g., for eval only), adjust this logic\n",
        "else:\n",
        "    if hasattr(config, 'max_steps_per_epoch_override') and \\\n",
        "       config.max_steps_per_epoch_override is not None and \\\n",
        "       config.max_steps_per_epoch_override > 0:\n",
        "        steps_per_epoch = config.max_steps_per_epoch_override\n",
        "        print(f\"    {CELL8_INFO_PREFIX} Using fixed steps_per_epoch from config.max_steps_per_epoch_override: {steps_per_epoch}\")\n",
        "    else:\n",
        "        # Calculate based on the longest active map-style training loader\n",
        "        active_train_loader_keys = [name for name in train_loaders.keys() if name in models_for_pairs]\n",
        "        map_style_train_loader_lengths = []\n",
        "        for name in active_train_loader_keys:\n",
        "            loader = train_loaders.get(name)\n",
        "            # Check if dataset attribute exists and has __len__ (for map-style)\n",
        "            if loader and hasattr(loader.dataset, '__len__') and not isinstance(loader.dataset, PyTorchIterableDataset):\n",
        "                map_style_train_loader_lengths.append(len(loader))\n",
        "\n",
        "        if map_style_train_loader_lengths:\n",
        "            steps_per_epoch = max(map_style_train_loader_lengths)\n",
        "            print(f\"    {CELL8_INFO_PREFIX} Calculated steps_per_epoch (from max active map-style train loader length): {steps_per_epoch}\")\n",
        "        elif active_train_loader_keys: # All active train loaders are iterable or lengths undetermined\n",
        "            steps_per_epoch = getattr(config, 'default_steps_for_iterable_epoch', 50)\n",
        "            print(f\"    {CELL8_INFO_PREFIX} All active train loaders are iterable or lengths undetermined. Using default_steps_for_iterable_epoch: {steps_per_epoch}\")\n",
        "        else: # No active train loaders with models\n",
        "            print(f\"    {CELL8_INFO_PREFIX} No active train loaders with corresponding models found to determine steps_per_epoch.\")\n",
        "            steps_per_epoch = getattr(config, 'default_steps_for_iterable_epoch', 50) # Fallback\n",
        "\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Effective steps_per_epoch for training: {steps_per_epoch}\")\n",
        "    if steps_per_epoch == 0 and train_loaders and train_loaders.keys(): # Only warn if loaders exist but steps are 0\n",
        "        print(f\"    WARNING: {CELL8_INFO_PREFIX} steps_per_epoch is 0. Schedulers may not behave as expected, and training loop might not run effectively.\")\n",
        "        _training_can_proceed_cell8 = False\n",
        "\n",
        "# --- 3. Models Check ---\n",
        "if not models_for_pairs and _training_can_proceed_cell8: # models_for_pairs from Cell 5\n",
        "    _training_can_proceed_cell8 = False\n",
        "    print(f\"WARNING: {CELL8_INFO_PREFIX} 'models_for_pairs' is empty, but train_loaders might exist. Models may not have been instantiated correctly in Cell 5.\")\n",
        "elif _training_can_proceed_cell8:\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Models for pairs ready: {list(models_for_pairs.keys())}\")\n",
        "\n",
        "# --- 4. Instantiate Loss Functions (from Cell 6) ---\n",
        "print(f\"{CELL8_INFO_PREFIX} Initializing loss functions...\")\n",
        "sinkhorn_ot_loss_fn: Optional[SinkhornOTLoss] = None # Type hint\n",
        "if GEOMOSS_AVAILABLE_CELL6 and getattr(config, 'lambda_sb', 0.0) > 0:\n",
        "    try:\n",
        "        print(f\"{CELL8_INFO_PREFIX} Attempting to initialize SinkhornOTLoss...\")\n",
        "        sinkhorn_ot_loss_fn = SinkhornOTLoss( # Class from Cell 6\n",
        "            p_norm=2,\n",
        "            blur=getattr(config, 'ot_blur', 0.05),\n",
        "            scaling=getattr(config, 'ot_scaling', 0.9),\n",
        "            debias=getattr(config, 'ot_debias', True),\n",
        "            backend=getattr(config, 'ot_backend', \"tensorized\"),\n",
        "            potentials=False\n",
        "        ).to(DEVICE)\n",
        "        print(f\"    {CELL8_INFO_PREFIX} SinkhornOTLoss instantiated and moved to {DEVICE}.\")\n",
        "    except Exception as e_sl_init:\n",
        "        print(f\"WARNING: {CELL8_INFO_PREFIX} SinkhornOTLoss instantiation failed: {e_sl_init}. Sinkhorn loss will be effectively zero.\")\n",
        "        print(traceback.format_exc()); sinkhorn_ot_loss_fn = None\n",
        "else:\n",
        "    if getattr(config, 'lambda_sb', 0.0) > 0:\n",
        "         print(f\"INFO: {CELL8_INFO_PREFIX} SinkhornOTLoss component will be disabled (Geomloss not available or lambda_sb is 0).\")\n",
        "    else:\n",
        "         print(f\"INFO: {CELL8_INFO_PREFIX} SinkhornOTLoss component disabled (lambda_sb is 0).\")\n",
        "\n",
        "# Instantiate MainCriterion\n",
        "print(f\"{CELL8_INFO_PREFIX} Initializing MainCriterion...\")\n",
        "main_criterion = MainCriterion( # Class from Cell 6\n",
        "    mixture_embedding_raw_instance=mixture_embedding_for_raw, # Correct argument name\n",
        "    lambda_kl_contrastive=getattr(config, 'lambda_kl_contrastive', getattr(config, 'lambda_KL', 1.0)),\n",
        "    lambda_ent=config.lambda_ent,\n",
        "    lambda_reg=config.lambda_reg,\n",
        "    reg_type=getattr(config, 'main_loss_reg_type', 'mean_norm'),\n",
        "    device=DEVICE\n",
        ").to(DEVICE)\n",
        "print(f\"    {CELL8_INFO_PREFIX} MainCriterion instantiated and moved to {DEVICE}.\")\n",
        "\n",
        "# --- 5. Optimizers & Schedulers ---\n",
        "all_trainable_params_for_raw_mixture: List[torch.nn.Parameter] = []\n",
        "if isinstance(mixture_embedding_for_raw, torch.nn.Module):\n",
        "    all_trainable_params_for_raw_mixture = [p for p in mixture_embedding_for_raw.parameters() if p.requires_grad]\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Collected {len(all_trainable_params_for_raw_mixture)} trainable params from 'mixture_embedding_for_raw'.\")\n",
        "\n",
        "optimizers_for_pairs:Dict[str,torch.optim.Optimizer]={}\n",
        "schedulers_for_pairs:Dict[str,Optional[torch.optim.lr_scheduler._LRScheduler]]={}\n",
        "\n",
        "if _training_can_proceed_cell8 and models_for_pairs: # Ensure models_for_pairs is not empty\n",
        "    print(f\"{CELL8_INFO_PREFIX} Setting up optimizers and schedulers for each active training pair...\")\n",
        "    # Iterate through active_pair_names from train_loaders that also have models\n",
        "    active_pairs_for_opt = [name for name in train_loaders.keys() if name in models_for_pairs]\n",
        "    if not active_pairs_for_opt:\n",
        "         print(f\"    WARNING: {CELL8_INFO_PREFIX} No pairs found in train_loaders that have corresponding models. Optimizers not created.\")\n",
        "\n",
        "    for pair_name in active_pairs_for_opt:\n",
        "        model_inst = models_for_pairs.get(pair_name) # Should exist due to filter\n",
        "        if model_inst and isinstance(model_inst, torch.nn.Module):\n",
        "            current_pair_model_params = [p for p in model_inst.parameters() if p.requires_grad]\n",
        "\n",
        "            # Combine PairModel params + shared mixture_embedding_for_raw params\n",
        "            params_for_this_optimizer = list(dict.fromkeys(current_pair_model_params + all_trainable_params_for_raw_mixture))\n",
        "\n",
        "            if not params_for_this_optimizer:\n",
        "                print(f\"    WARNING: No trainable parameters found for model '{pair_name}' (after combining). Optimizer not created.\")\n",
        "                continue\n",
        "\n",
        "            opt_type = getattr(config, 'optimizer_type', 'AdamW').lower()\n",
        "            lr_val = getattr(config, 'lr', 2e-4)\n",
        "            wd_val = getattr(config, 'weight_decay', 1e-4)\n",
        "            optimizer_instance: Optional[torch.optim.Optimizer] = None\n",
        "\n",
        "            if opt_type == \"adamw\": optimizer_instance = optim.AdamW(params_for_this_optimizer, lr=lr_val, weight_decay=wd_val)\n",
        "            elif opt_type == \"adam\": optimizer_instance = optim.Adam(params_for_this_optimizer, lr=lr_val, weight_decay=wd_val) # Adam usually doesn't handle WD \"correctly\" like AdamW\n",
        "            # Add other optimizers here if needed\n",
        "            else:\n",
        "                print(f\"    WARNING: Optimizer type '{opt_type}' for '{pair_name}' not recognized. Defaulting to AdamW.\")\n",
        "                optimizer_instance = optim.AdamW(params_for_this_optimizer, lr=lr_val, weight_decay=wd_val)\n",
        "\n",
        "            optimizers_for_pairs[pair_name] = optimizer_instance\n",
        "            print(f\"    Optimizer '{type(optimizer_instance).__name__}' created for model '{pair_name}' (LR={lr_val}, WD={wd_val}). Num params: {sum(p.numel() for p in params_for_this_optimizer)}\")\n",
        "\n",
        "            if steps_per_epoch > 0 and getattr(config, 'use_lr_scheduler', True):\n",
        "                scheduler_instance = get_lr_scheduler(optimizer_instance, config, steps_per_epoch) # get_lr_scheduler from Cell 7\n",
        "                schedulers_for_pairs[pair_name] = scheduler_instance\n",
        "                if scheduler_instance: print(f\"        LR Scheduler '{type(scheduler_instance).__name__}' configured for optimizer '{pair_name}'.\")\n",
        "            else:\n",
        "                schedulers_for_pairs[pair_name] = None\n",
        "                if steps_per_epoch == 0: print(f\"        LR Scheduler skipped for '{pair_name}' (steps_per_epoch is 0).\")\n",
        "                else: print(f\"        LR Scheduler disabled for '{pair_name}' by config.\")\n",
        "else:\n",
        "     print(f\"    INFO: {CELL8_INFO_PREFIX} Skipping optimizer/scheduler setup as training cannot proceed or no models to optimize.\")\n",
        "\n",
        "\n",
        "# --- 6. AMP GradScaler ---\n",
        "use_amp_cfg = getattr(config, 'use_amp', False)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp_cfg and torch.cuda.is_available())\n",
        "print(f\"    {CELL8_INFO_PREFIX} AMP GradScaler enabled: {scaler.is_enabled()} (Config.use_amp: {use_amp_cfg}, CUDA available: {torch.cuda.is_available()})\")\n",
        "\n",
        "# --- 7. Load Checkpoint ---\n",
        "start_epoch = 0 # Default if not loading\n",
        "load_ckpt_cfg = getattr(config, 'load_from_checkpoint', False)\n",
        "if _training_can_proceed_cell8 and models_for_pairs and optimizers_for_pairs and \\\n",
        "   (load_ckpt_cfg is True or (isinstance(load_ckpt_cfg, str) and load_ckpt_cfg.strip() != \"\")):\n",
        "    checkpoint_filename = load_ckpt_cfg if isinstance(load_ckpt_cfg, str) else None\n",
        "    print(f\"{CELL8_INFO_PREFIX} Attempting to load checkpoint (File: {'Latest in dir' if checkpoint_filename is None else checkpoint_filename})...\")\n",
        "\n",
        "    # Ensure all components to be loaded are passed\n",
        "    # Note: load_checkpoint from Cell 7 should handle missing keys gracefully\n",
        "    start_epoch = load_checkpoint( # Function from Cell 7\n",
        "        cfg=config,\n",
        "        models_for_pairs=models_for_pairs,\n",
        "        optimizers_for_pairs=optimizers_for_pairs,\n",
        "        schedulers_for_pairs=schedulers_for_pairs,\n",
        "        mixture_embedding_for_raw=mixture_embedding_for_raw,\n",
        "        mixture_embedding_for_fused=mixture_embedding_for_fused,\n",
        "        filename=checkpoint_filename,\n",
        "        load_strict=getattr(config, 'checkpoint_load_strict', False)\n",
        "    )\n",
        "else:\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Not loading from checkpoint (Condition not met: CanProceed={_training_can_proceed_cell8}, ModelsExist={bool(models_for_pairs)}, OptimsExist={bool(optimizers_for_pairs)}, LoadCfg='{load_ckpt_cfg}'). Starting from epoch 0.\")\n",
        "\n",
        "# --- 8. Setup Logging (WandB) ---\n",
        "logging_active = False # From Cell 7\n",
        "if _training_can_proceed_cell8:\n",
        "    print(f\"{CELL8_INFO_PREFIX} Setting up external logging (WandB)...\")\n",
        "    logging_active = setup_wandb_logging(config, job_type=\"multimodal_training_final\") # Pass a job_type\n",
        "    if logging_active: print(f\"    {CELL8_INFO_PREFIX} WandB logging is active.\")\n",
        "    else: print(f\"INFO: {CELL8_INFO_PREFIX} WandB logging not active (either disabled in config or setup failed).\")\n",
        "\n",
        "    if tb_writer: print(f\"    {CELL8_INFO_PREFIX} TensorBoard logging active. Log dir: {tb_writer.log_dir}\") # tb_writer from Cell 7\n",
        "    else: print(f\"INFO: {CELL8_INFO_PREFIX} TensorBoard writer not available (initialization in Cell 7 might have failed).\")\n",
        "\n",
        "# --- Final Readiness Check ---\n",
        "if not _training_can_proceed_cell8:\n",
        "    print(f\"\\nCRITICAL: {CELL8_INFO_PREFIX} Training setup is incomplete. Training cannot proceed. Review warnings above.\")\n",
        "else:\n",
        "    print(f\"\\n{CELL8_INFO_PREFIX} All pre-training checks passed. Setup is complete.\")\n",
        "\n",
        "CURRENT_TIME_END_CELL8 = time.time()\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL8))}] {CELL8_INFO_PREFIX} Model & Training Setup complete. Duration: {CURRENT_TIME_END_CELL8 - CURRENT_TIME_START_CELL8:.2f}s\")\n",
        "\n",
        "if _training_can_proceed_cell8:\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Ready to start training from epoch {start_epoch}.\")\n",
        "else:\n",
        "    print(f\"    {CELL8_INFO_PREFIX} Training cannot start due to setup issues. Please review logs from this and previous cells.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ci4G9LZXMW3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9"
      ],
      "metadata": {
        "id": "vVpjXRaXRt1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 9: Training Loop (Pairwise Training - Corrected Loss Usage & Robust Sinkhorn) ---\n",
        "# Executes the training process using the corrected MainCriterion and robust SinkhornOTLoss from Cell 6.\n",
        "# Assumes Cell 8 has successfully set up all components.\n",
        "# Ensures correct indentation for optimizer step logic.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "import numpy as np\n",
        "import traceback\n",
        "# from tqdm.auto import tqdm # Optional: for progress bars, uncomment and install if needed\n",
        "\n",
        "CELL9_INFO_PREFIX = \"[Cell 9]\"\n",
        "DEBUG_CELL9 = getattr(config, 'debug_mode', True) if 'config' in globals() else True\n",
        "\n",
        "# --- Critical Checks ---\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL9_INFO_PREFIX} Verifying required components for training loop...\")\n",
        "critical_components_cell9_check = [\n",
        "    'config', 'DEVICE', 'models_for_pairs', 'train_loaders', 'val_loaders',\n",
        "    'optimizers_for_pairs', 'schedulers_for_pairs',\n",
        "    'main_criterion', 'scaler', 'start_epoch', 'tb_writer', 'logging_active',\n",
        "    'mixture_embedding_for_raw', 'mixture_embedding_for_fused',\n",
        "    'save_checkpoint', 'log_metrics_to_wandb', 'log_metrics_to_tensorboard',\n",
        "    'MixtureEmbedding', 'DatasetPairConfig', 'PairModel', 'SinkhornOTLoss', 'MainCriterion',\n",
        "    'steps_per_epoch', # This should be globally available from Cell 8\n",
        "    'run_validation_epoch' # From Cell 10 (if defined and to be used)\n",
        "]\n",
        "all_verified_c9 = True; missing_comps_c9 = []\n",
        "for comp_c9 in critical_components_cell9_check:\n",
        "    if comp_c9 not in globals():\n",
        "        if comp_c9 == 'sinkhorn_ot_loss_fn' and ('GEOMOSS_AVAILABLE_CELL6' not in globals() or not GEOMOSS_AVAILABLE_CELL6 or getattr(config, 'lambda_sb', 0.0) == 0):\n",
        "            globals()[comp_c9] = None\n",
        "            print(f\"    INFO: {CELL9_INFO_PREFIX} Optional component '{comp_c9}' not found (expected as geomloss N/A or lambda_sb=0).\")\n",
        "            continue\n",
        "        # Allow run_validation_epoch to be missing if val_loaders is empty\n",
        "        if comp_c9 == 'run_validation_epoch' and ('val_loaders' not in globals() or not val_loaders):\n",
        "            print(f\"    INFO: {CELL9_INFO_PREFIX} Optional component '{comp_c9}' not found (validation loaders are empty).\")\n",
        "            globals()[comp_c9] = None # Define as None so later checks don't fail\n",
        "            continue\n",
        "        print(f\"FATAL ERROR: {CELL9_INFO_PREFIX} Missing '{comp_c9}'. Run previous cells.\"); missing_comps_c9.append(comp_c9); all_verified_c9 = False\n",
        "if not all_verified_c9: raise NameError(f\"FATAL ERROR: {CELL9_INFO_PREFIX} Prerequisite(s) missing for Cell 9: {missing_comps_c9}\")\n",
        "\n",
        "if not isinstance(main_criterion, MainCriterion): raise TypeError(f\"FATAL ERROR: 'main_criterion' not MainCriterion instance. Got: {type(main_criterion)}.\")\n",
        "if sinkhorn_ot_loss_fn is not None and not isinstance(sinkhorn_ot_loss_fn, SinkhornOTLoss): raise TypeError(f\"FATAL ERROR: 'sinkhorn_ot_loss_fn' not SinkhornOTLoss instance. Got: {type(sinkhorn_ot_loss_fn)}.\")\n",
        "\n",
        "# _training_can_proceed_cell8 should be defined in Cell 8. If not, assume True if we reached here.\n",
        "_training_can_proceed_now = _training_can_proceed_cell8 if '_training_can_proceed_cell8' in globals() else True\n",
        "if 'steps_per_epoch' in globals() and steps_per_epoch == 0 and train_loaders and train_loaders.keys():\n",
        "     print(f\"CRITICAL WARNING: {CELL9_INFO_PREFIX} 'steps_per_epoch' is 0. Training loop cannot run effectively.\");\n",
        "     _training_can_proceed_now = False\n",
        "print(f\"    {CELL9_INFO_PREFIX} All critical components verified. Training can proceed: {_training_can_proceed_now}\")\n",
        "\n",
        "\n",
        "CURRENT_TIME_START_CELL9_exec = time.time()\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_START_CELL9_exec))}] {CELL9_INFO_PREFIX} ===== Starting Pairwise Training =====\")\n",
        "\n",
        "# --- Training Configuration (from `config` object) ---\n",
        "num_epochs_cfg = getattr(config, 'num_epochs', 10)\n",
        "grad_accum_steps_cfg = getattr(config, 'gradient_accumulation_steps', 1)\n",
        "grad_clip_val_cfg = getattr(config, 'gradient_clip_val', None)\n",
        "log_steps_cfg = getattr(config, 'log_steps', 10)\n",
        "save_every_n_epochs_cfg = getattr(config, 'save_every_n_epochs', 1)\n",
        "val_every_n_epochs_cfg = getattr(config, 'val_check_interval_epochs', 1)\n",
        "experiment_name_cfg = getattr(config, 'experiment_name', 'default_exp')\n",
        "lambda_sb_cfg = getattr(config, 'lambda_sb', 0.0)\n",
        "\n",
        "print(f\"    {CELL9_INFO_PREFIX} Target Epochs: {num_epochs_cfg}, Starting from Epoch: {start_epoch + 1}\")\n",
        "print(f\"    {CELL9_INFO_PREFIX} Effective Batch Size (per optimizer step): ~{config.batch_size * grad_accum_steps_cfg}\")\n",
        "if grad_clip_val_cfg is not None: print(f\"    {CELL9_INFO_PREFIX} Gradient Clipping: {grad_clip_val_cfg}\")\n",
        "print(f\"    {CELL9_INFO_PREFIX} Device: {DEVICE}, AMP Enabled: {scaler.is_enabled()}\")\n",
        "print(f\"    {CELL9_INFO_PREFIX} Logging every {log_steps_cfg} super-steps.\")\n",
        "\n",
        "\n",
        "if not _training_can_proceed_now:\n",
        "    print(f\"ERROR: [{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL9_INFO_PREFIX} Prerequisites not met (e.g. steps_per_epoch=0 or no loaders). Skipping training loop.\")\n",
        "else:\n",
        "    active_pair_names = [name for name in train_loaders.keys() if name in models_for_pairs and name in optimizers_for_pairs]\n",
        "    if not active_pair_names:\n",
        "        print(f\"ERROR: {CELL9_INFO_PREFIX} No active pairs with models & optimizers. Skipping training loop.\");\n",
        "        _training_can_proceed_now = False # Update flag\n",
        "    else:\n",
        "        print(f\"    {CELL9_INFO_PREFIX} Confirmed active pairs for training: {active_pair_names}\")\n",
        "        max_steps_for_epoch = steps_per_epoch # Use global steps_per_epoch from Cell 8\n",
        "        print(f\"    {CELL9_INFO_PREFIX} Using steps_per_epoch from Cell 8: {max_steps_for_epoch}\")\n",
        "\n",
        "if _training_can_proceed_now:\n",
        "    pair_data_iters = {name: iter(train_loaders[name]) for name in active_pair_names}\n",
        "\n",
        "    # This calculation of global_optimizer_step_count might need refinement based on how accumulation is handled.\n",
        "    # If optimizers step together after grad_accum_steps_cfg *super-steps*, then it's simpler.\n",
        "    # If each pair's optimizer accumulates independently, it's more complex.\n",
        "    # The current loop structure implies optimizers step together at super-step boundaries if grad_accum > 1.\n",
        "    optimizer_steps_per_epoch_approx = (max_steps_for_epoch // grad_accum_steps_cfg) if grad_accum_steps_cfg > 0 else max_steps_for_epoch\n",
        "    global_optimizer_step_count = start_epoch * optimizer_steps_per_epoch_approx\n",
        "\n",
        "    best_validation_metric = float('inf')\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs_cfg):\n",
        "        # Set models to train mode\n",
        "        for model_instance in models_for_pairs.values():\n",
        "            if model_instance and isinstance(model_instance, nn.Module): model_instance.train()\n",
        "        if mixture_embedding_for_raw and isinstance(mixture_embedding_for_raw, nn.Module): mixture_embedding_for_raw.train()\n",
        "        if mixture_embedding_for_fused and isinstance(mixture_embedding_for_fused, nn.Module): mixture_embedding_for_fused.train()\n",
        "\n",
        "        epoch_loss_sum_total = 0.0\n",
        "        epoch_loss_components_sum = {comp: 0.0 for comp in ['kl', 'ent', 'reg', 'sb_loss', 'main_loss_total']}\n",
        "        successful_fwd_bwd_passes_this_epoch = 0\n",
        "        epoch_start_time = time.time(); batch_processing_times_log_interval = []\n",
        "        print(f\"\\n--- Epoch {epoch + 1}/{num_epochs_cfg} ---\")\n",
        "\n",
        "        pbar_steps = range(max_steps_for_epoch) # Use max_steps_for_epoch from Cell 8\n",
        "\n",
        "        for step_in_epoch_idx in pbar_steps: # This is a \"super-step\"\n",
        "            # Define is_global_accumulation_boundary INSIDE this loop's scope\n",
        "            is_global_accumulation_boundary = ((step_in_epoch_idx + 1) % grad_accum_steps_cfg == 0) or \\\n",
        "                                              ((step_in_epoch_idx + 1) == max_steps_for_epoch)\n",
        "\n",
        "            # Zero gradients for all optimizers at the start of an accumulation cycle\n",
        "            # if this super-step is an optimizer update boundary.\n",
        "            if is_global_accumulation_boundary:\n",
        "                for opt_to_zero in optimizers_for_pairs.values():\n",
        "                    if opt_to_zero: opt_to_zero.zero_grad(set_to_none=True)\n",
        "                if DEBUG_CELL9 and grad_accum_steps_cfg > 1:\n",
        "                    print(f\"DEBUG: {CELL9_INFO_PREFIX} Zeroed all optimizers at super-step {step_in_epoch_idx+1} (global accum boundary).\")\n",
        "\n",
        "            # Inner loop processes each active pair once per \"super-step\"\n",
        "            for current_pair_name in active_pair_names:\n",
        "                batch_start_time = time.time()\n",
        "                current_model = models_for_pairs[current_pair_name]\n",
        "                current_optimizer = optimizers_for_pairs[current_pair_name]\n",
        "                current_scheduler = schedulers_for_pairs.get(current_pair_name)\n",
        "\n",
        "                # If not using global accumulation boundary (i.e., grad_accum_steps_cfg == 1),\n",
        "                # zero grad for each pair's optimizer before its turn.\n",
        "                if grad_accum_steps_cfg == 1:\n",
        "                     current_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                try: batch_data = next(pair_data_iters[current_pair_name])\n",
        "                except StopIteration:\n",
        "                    pair_data_iters[current_pair_name] = iter(train_loaders[current_pair_name])\n",
        "                    try: batch_data = next(pair_data_iters[current_pair_name])\n",
        "                    except StopIteration: print(f\"WARNING: {CELL9_INFO_PREFIX} Loader '{current_pair_name}' empty after re-init. Skipping.\"); continue\n",
        "                if batch_data is None: continue\n",
        "                try: mod1_cpu, mod2_cpu = batch_data\n",
        "                except ValueError: print(f\"ERROR: {CELL9_INFO_PREFIX} Unpack batch for '{current_pair_name}' failed. Skipping.\"); continue\n",
        "                if mod1_cpu is None or mod2_cpu is None: continue\n",
        "\n",
        "                current_batch_size = (mod1_cpu.shape[0] if isinstance(mod1_cpu, torch.Tensor) else len(mod1_cpu) if isinstance(mod1_cpu, list) else 0)\n",
        "                if current_batch_size == 0: continue\n",
        "\n",
        "                mod1_device = mod1_cpu.to(DEVICE, non_blocking=True) if isinstance(mod1_cpu, torch.Tensor) else mod1_cpu\n",
        "                mod2_device = mod2_cpu.to(DEVICE, non_blocking=True) if isinstance(mod2_cpu, torch.Tensor) else mod2_cpu\n",
        "\n",
        "                loss_this_micro_batch = torch.tensor(0.0, device=DEVICE)\n",
        "                current_log_loss_components = {k: float('nan') for k in epoch_loss_components_sum.keys()} # Initialize for current step\n",
        "\n",
        "                try:\n",
        "                    with torch.autocast(device_type=DEVICE.type, dtype=torch.bfloat16 if DEVICE.type == 'cuda' and torch.cuda.is_bf16_supported() else torch.float16, enabled=scaler.is_enabled()):\n",
        "                        _m_fused, _g_fused, raw_feat1, raw_feat2 = current_model(mod1_device, mod2_device)\n",
        "                        if raw_feat1 is None or raw_feat2 is None or raw_feat1.shape[0] != current_batch_size or raw_feat2.shape[0] != current_batch_size :\n",
        "                            if DEBUG_CELL9: print(f\"DEBUG: {CELL9_INFO_PREFIX} Invalid raw_feat from PairModel for '{current_pair_name}'. Skipping.\"); continue\n",
        "\n",
        "                        measures1, gamma1 = mixture_embedding_for_raw(raw_feat1)\n",
        "                        measures2, gamma2 = mixture_embedding_for_raw(raw_feat2)\n",
        "                        if measures1 is None or measures2 is None or gamma1 is None or gamma2 is None or \\\n",
        "                           gamma1.shape[0] != current_batch_size or gamma2.shape[0] != current_batch_size:\n",
        "                            if DEBUG_CELL9: print(f\"DEBUG: {CELL9_INFO_PREFIX} Invalid measures/gamma for '{current_pair_name}'. Skipping.\"); continue\n",
        "\n",
        "                        # Call MainCriterion (from Cell 6)\n",
        "                        main_loss_value_total, loss_components_dict_main = main_criterion(\n",
        "                            measures1=measures1, gamma1=gamma1,\n",
        "                            measures2=measures2, gamma2=gamma2,\n",
        "                            current_batch_size=current_batch_size\n",
        "                        )\n",
        "                        current_log_loss_components.update(loss_components_dict_main)\n",
        "                        # Ensure 'main_loss_total' from criterion's output is captured for logging\n",
        "                        if torch.isfinite(main_loss_value_total):\n",
        "                             current_log_loss_components['main_loss_total'] = main_loss_value_total.item()\n",
        "\n",
        "                        sb_loss_val = torch.tensor(0.0, device=DEVICE)\n",
        "                        current_log_loss_components['sb_loss'] = 0.0\n",
        "                        if sinkhorn_ot_loss_fn is not None and lambda_sb_cfg > 0:\n",
        "                            s1_ot_cand, s2_ot_cand = None, None\n",
        "                            if 'E' in measures1 and isinstance(measures1['E'], tuple) and len(measures1['E']) > 2 and isinstance(measures1['E'][2], torch.Tensor): s1_ot_cand = measures1['E'][2]\n",
        "                            if 'E' in measures2 and isinstance(measures2['E'], tuple) and len(measures2['E']) > 2 and isinstance(measures2['E'][2], torch.Tensor): s2_ot_cand = measures2['E'][2]\n",
        "\n",
        "                            s1_final, s2_final = None, None; valid_sinkhorn_inputs = True\n",
        "                            if DEBUG_CELL9: print(f\"    DEBUG: {CELL9_INFO_PREFIX} PRE-SINKHORN for '{current_pair_name}':\")\n",
        "                            for i_s, (s_cand, s_n) in enumerate([(s1_ot_cand, \"s1_ot\"), (s2_ot_cand, \"s2_ot\")]):\n",
        "                                if s_cand is None or not isinstance(s_cand, torch.Tensor) or s_cand.numel()==0 or s_cand.shape[0]!=current_batch_size:\n",
        "                                    valid_sinkhorn_inputs = False\n",
        "                                    if DEBUG_CELL9: print(f\"        {s_n}_candidate is None or invalid. Sinkhorn check failed.\")\n",
        "                                    break\n",
        "\n",
        "                                if DEBUG_CELL9: print(f\"        {s_n}_candidate: Shape={s_cand.shape}, Dtype={s_cand.dtype}, Finite={torch.isfinite(s_cand).all().item()}\")\n",
        "\n",
        "                                if not torch.isfinite(s_cand).all():\n",
        "                                    print(f\"        WARNING: {s_n}_candidate contains NaN/Inf!\")\n",
        "                                    valid_sinkhorn_inputs = False\n",
        "                                    break\n",
        "\n",
        "                                if i_s == 0:\n",
        "                                    s1_final = s_cand.contiguous().float()\n",
        "                                else:\n",
        "                                    s2_final = s_cand.contiguous().float()\n",
        "                                if not valid_sinkhorn_inputs: break # Stop if first candidate fails\n",
        "\n",
        "                            if valid_sinkhorn_inputs and s1_final is not None and s2_final is not None and \\\n",
        "                               (s1_final.ndim == 2 and s2_final.ndim == 2 and s1_final.shape == s2_final.shape and s1_final.shape[0] > 0):\n",
        "                                try:\n",
        "                                    if DEBUG_CELL9: print(f\"    DEBUG: {CELL9_INFO_PREFIX} Calling SinkhornOTLoss s1:{s1_final.shape}, s2:{s2_final.shape}\")\n",
        "                                    sb_loss_val = sinkhorn_ot_loss_fn(s1_final, s2_final)\n",
        "                                    if not torch.isfinite(sb_loss_val): print(f\"WARNING: SB loss NaN/Inf. Default 0.\"); sb_loss_val = torch.tensor(0.0, device=DEVICE)\n",
        "                                except Exception as e_sink: print(f\"ERROR: {CELL9_INFO_PREFIX} Sinkhorn call: {e_sink}\\n{traceback.format_exc()}\"); sb_loss_val = torch.tensor(0.0, device=DEVICE)\n",
        "                            elif DEBUG_CELL9: print(f\"    INFO: {CELL9_INFO_PREFIX} Skipping Sinkhorn for '{current_pair_name}' due to invalid inputs or shape mismatch.\")\n",
        "                        current_log_loss_components['sb_loss'] = sb_loss_val.item() if isinstance(sb_loss_val, torch.Tensor) else float(sb_loss_val)\n",
        "\n",
        "                        loss_this_micro_batch = main_loss_value_total + lambda_sb_cfg * sb_loss_val\n",
        "                        if not torch.isfinite(loss_this_micro_batch): print(f\"WARNING: {CELL9_INFO_PREFIX} Total micro_batch loss NaN/Inf for '{current_pair_name}'. Skipping.\"); continue\n",
        "\n",
        "                except TypeError as te_main_crit: print(f\"ERROR: {CELL9_INFO_PREFIX} TypeError calling main_criterion for '{current_pair_name}': {te_main_crit}\\n{traceback.format_exc()}\"); continue\n",
        "                except Exception as e_fwd: print(f\"ERROR: {CELL9_INFO_PREFIX} FWD/Loss for '{current_pair_name}': {e_fwd}\\n{traceback.format_exc()}\"); continue\n",
        "\n",
        "                normalized_loss = loss_this_micro_batch / grad_accum_steps_cfg\n",
        "                try: scaler.scale(normalized_loss).backward()\n",
        "                except Exception as e_bwd: print(f\"ERROR: {CELL9_INFO_PREFIX} backward() for '{current_pair_name}': {e_bwd}\\n{traceback.format_exc()}\"); continue\n",
        "\n",
        "                if torch.isfinite(loss_this_micro_batch):\n",
        "                    epoch_loss_sum_total += loss_this_micro_batch.item()\n",
        "                    for k_log, v_log in current_log_loss_components.items():\n",
        "                        if isinstance(v_log, (int, float)) and np.isfinite(v_log): epoch_loss_components_sum[k_log] = epoch_loss_components_sum.get(k_log, 0.0) + v_log\n",
        "                    successful_fwd_bwd_passes_this_epoch += 1\n",
        "                batch_processing_times_log_interval.append(time.time() - batch_start_time)\n",
        "\n",
        "            # ** CORRECTED INDENTATION FOR OPTIMIZER STEP BLOCK **\n",
        "            # This block is now INSIDE the `for step_in_epoch_idx in pbar_steps:` loop\n",
        "            if is_global_accumulation_boundary: # Check the flag defined at the start of the super-step\n",
        "                for pair_name_opt in active_pair_names:\n",
        "                    optimizer_to_step = optimizers_for_pairs.get(pair_name_opt)\n",
        "                    scheduler_to_step = schedulers_for_pairs.get(pair_name_opt)\n",
        "                    if optimizer_to_step:\n",
        "                        if grad_clip_val_cfg is not None and scaler.is_enabled(): # Unscale only if using scaler\n",
        "                            scaler.unscale_(optimizer_to_step)\n",
        "\n",
        "                        # Clip gradients for parameters associated with this specific optimizer\n",
        "                        params_to_clip = []\n",
        "                        for group in optimizer_to_step.param_groups:\n",
        "                            params_to_clip.extend([p for p in group['params'] if p.grad is not None])\n",
        "                        if params_to_clip and grad_clip_val_cfg is not None:\n",
        "                             torch.nn.utils.clip_grad_norm_(params_to_clip, grad_clip_val_cfg)\n",
        "\n",
        "                        scaler.step(optimizer_to_step) # scaler.step() checks scaler.is_enabled() internally\n",
        "\n",
        "                        if scheduler_to_step and not isinstance(scheduler_to_step, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                            scheduler_to_step.step() # Step LR for those that step per optimizer update\n",
        "                scaler.update() # Single scaler update after all optimizers have stepped\n",
        "                global_optimizer_step_count += 1 # Increment for each actual optimizer update cycle\n",
        "\n",
        "            # Step Logging (based on super-step_in_epoch_idx)\n",
        "            if (step_in_epoch_idx + 1) % log_steps_cfg == 0 or (step_in_epoch_idx + 1) == max_steps_for_epoch:\n",
        "                avg_bt = np.mean(batch_processing_times_log_interval) if batch_processing_times_log_interval else 0\n",
        "                # Log info for the *last processed pair* in this super-step, or average if desired\n",
        "                last_pair_processed_in_super_step = active_pair_names[-1] if active_pair_names else \"N/A_Pair\"\n",
        "                last_lr_log = optimizers_for_pairs[last_pair_processed_in_super_step].param_groups[0]['lr'] if active_pair_names and optimizers_for_pairs.get(last_pair_processed_in_super_step) else float('nan')\n",
        "                temp_raw_log_val = mixture_embedding_for_raw.temperature.item() if mixture_embedding_for_raw else float('nan')\n",
        "\n",
        "                # Use values from current_log_loss_components which corresponds to the last processed pair in the super-step\n",
        "                loss_item_log = loss_this_micro_batch.item() if 'loss_this_micro_batch' in locals() and hasattr(loss_this_micro_batch, 'item') and torch.isfinite(loss_this_micro_batch) else float('nan')\n",
        "                main_crit_total_log = current_log_loss_components.get('main_loss_total', float('nan'))\n",
        "                sb_log = current_log_loss_components.get('sb_loss', float('nan'))\n",
        "                kl_log = current_log_loss_components.get('kl', float('nan'))\n",
        "                ent_log = current_log_loss_components.get('ent', float('nan'))\n",
        "                reg_log = current_log_loss_components.get('reg', float('nan'))\n",
        "\n",
        "                print(f\"  Ep {epoch+1} SuprSt {step_in_epoch_idx+1}/{max_steps_for_epoch} (OptSt {global_optimizer_step_count}) \"\n",
        "                      f\"| LastPairLoss ({last_pair_processed_in_super_step}): {loss_item_log:.4f} \"\n",
        "                      f\"(MainTotal: {main_crit_total_log:.3f}, KL: {kl_log:.3f}, Ent: {ent_log:.3f}, Reg: {reg_log:.3f}, SB: {sb_log:.3f}) \"\n",
        "                      f\"| LR: {last_lr_log:.1e} | TempRaw:{temp_raw_log_val:.3f} | GradScale:{scaler.get_scale():.0f} | BatchTime(avg):{avg_bt:.2f}s\")\n",
        "                batch_processing_times_log_interval = []\n",
        "\n",
        "                log_payload_step_wandb = {f\"TotalLoss_step/{last_pair_processed_in_super_step}\": loss_item_log}\n",
        "                log_payload_step_wandb.update({f\"Comp_{k_comp}/{last_pair_processed_in_super_step}\": v_comp for k_comp, v_comp in current_log_loss_components.items()})\n",
        "                log_payload_step_wandb[f\"LR_step/{last_pair_processed_in_super_step}\"] = last_lr_log\n",
        "                if logging_active: log_metrics_to_wandb(log_payload_step_wandb, global_optimizer_step_count, epoch + 1, config, prefix=\"train_step_detail/\")\n",
        "                if tb_writer:\n",
        "                    for k_tb, v_tb in log_payload_step_wandb.items():\n",
        "                        if isinstance(v_tb, (int,float)) and np.isfinite(v_tb): tb_writer.add_scalar(f\"step_detail/{k_tb.replace('.','_')}\", v_tb, global_optimizer_step_count)\n",
        "\n",
        "        # --- End of Epoch Actions ---\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        avg_epoch_loss = epoch_loss_sum_total / successful_fwd_bwd_passes_this_epoch if successful_fwd_bwd_passes_this_epoch > 0 else float('nan')\n",
        "        print(f\"\\n--- Epoch {epoch + 1} Summary ---\"); print(f\"    Average Total Loss (all pairs, fwd/bwd passes): {avg_epoch_loss:.4f} ({successful_fwd_bwd_passes_this_epoch} successful passes this epoch)\")\n",
        "        if successful_fwd_bwd_passes_this_epoch > 0:\n",
        "            for comp, val_sum in epoch_loss_components_sum.items(): print(f\"        Avg Epoch Component '{comp.capitalize().replace('_',' ')}': {val_sum / successful_fwd_bwd_passes_this_epoch:.4f}\")\n",
        "        print(f\"    Epoch Duration: {epoch_duration:.2f}s\")\n",
        "\n",
        "        log_payload_epoch = {\"loss_total_epoch_avg\": avg_epoch_loss, \"duration_sec_epoch\": epoch_duration}\n",
        "        if successful_fwd_bwd_passes_this_epoch > 0 : log_payload_epoch.update({f\"loss_{c_name}_epoch_avg\": c_sum / successful_fwd_bwd_passes_this_epoch for c_name, c_sum in epoch_loss_components_sum.items()})\n",
        "        log_payload_epoch[\"lr_epoch_avg\"] = np.mean([opt.param_groups[0]['lr'] for opt in optimizers_for_pairs.values() if opt]) if optimizers_for_pairs else 0.0\n",
        "        if logging_active: log_metrics_to_wandb(log_payload_epoch, step=epoch + 1, epoch=epoch + 1, cfg=config, prefix=\"epoch_summary/\")\n",
        "        if tb_writer:\n",
        "            for k_tb_ep, v_tb_ep in log_payload_epoch.items():\n",
        "                if isinstance(v_tb_ep, (int,float)) and np.isfinite(v_tb_ep): tb_writer.add_scalar(f\"epoch_summary/{k_tb_ep.replace('.', '_')}\", v_tb_ep, epoch + 1)\n",
        "\n",
        "        for sch_name, sch_inst in schedulers_for_pairs.items():\n",
        "            if sch_inst and isinstance(sch_inst, torch.optim.lr_scheduler.ReduceLROnPlateau): sch_inst.step(avg_epoch_loss)\n",
        "\n",
        "        is_best_model_this_epoch = False\n",
        "        if (epoch + 1) % val_every_n_epochs_cfg == 0 and val_loaders and val_loaders.keys():\n",
        "            if 'run_validation_epoch' in globals() and callable(run_validation_epoch):\n",
        "                val_metrics = run_validation_epoch(\n",
        "                    current_epoch_idx=epoch, models_for_pairs_val=models_for_pairs,\n",
        "                    val_loaders_dict=val_loaders, main_criterion_val=main_criterion,\n",
        "                    sinkhorn_ot_loss_fn_val=sinkhorn_ot_loss_fn,\n",
        "                    mixture_embedding_for_raw_val=mixture_embedding_for_raw,\n",
        "                    g_config=config, device_val=DEVICE\n",
        "                )\n",
        "                current_val_metric = val_metrics.get('val_loss_total_avg', float('inf'))\n",
        "                if current_val_metric < best_validation_metric:\n",
        "                    print(f\"    INFO: {CELL9_INFO_PREFIX} New best validation metric: {current_val_metric:.4f} (prev: {best_validation_metric:.4f}).\")\n",
        "                    best_validation_metric = current_val_metric\n",
        "                    is_best_model_this_epoch = True\n",
        "            else: print(f\"WARNING: {CELL9_INFO_PREFIX} 'run_validation_epoch' (Cell 10) not defined. Skipping validation.\")\n",
        "\n",
        "        if (epoch + 1) % save_every_n_epochs_cfg == 0 or (epoch + 1) == num_epochs_cfg or is_best_model_this_epoch:\n",
        "            metrics_ckpt = {'avg_epoch_loss_total_overall': avg_epoch_loss};\n",
        "            if successful_fwd_bwd_passes_this_epoch > 0: metrics_ckpt.update({f\"avg_epoch_comp_{k}\": v/successful_fwd_bwd_passes_this_epoch for k,v in epoch_loss_components_sum.items()})\n",
        "            if 'val_metrics' in locals() and isinstance(val_metrics, dict): metrics_ckpt.update(val_metrics) # Check if val_metrics was defined\n",
        "            save_checkpoint(epoch, models_for_pairs, optimizers_for_pairs, schedulers_for_pairs, mixture_embedding_for_raw, mixture_embedding_for_fused, config, metrics=metrics_ckpt, is_best=is_best_model_this_epoch, prefix_filename=f\"ckpt_{experiment_name_cfg}\")\n",
        "\n",
        "    print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL9_INFO_PREFIX} ===== Training Finished ({num_epochs_cfg} epochs completed) =====\")\n",
        "else: print(f\"ERROR: [{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL9_INFO_PREFIX} Training loop NOT started due to setup issues.\")\n",
        "\n",
        "# --- Final Cleanup ---\n",
        "if logging_active and 'wandb' in globals() and wandb.run is not None:\n",
        "    try: wandb.finish()\n",
        "    except Exception as e: print(f\"WARNING: {CELL9_INFO_PREFIX} Error finishing WandB run: {e}\")\n",
        "if tb_writer is not None:\n",
        "    try: tb_writer.close()\n",
        "    except Exception as e: print(f\"WARNING: {CELL9_INFO_PREFIX} Error closing TB writer: {e}\")\n",
        "\n",
        "CURRENT_TIME_END_CELL9_exec = time.time()\n",
        "print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(CURRENT_TIME_END_CELL9_exec))}] {CELL9_INFO_PREFIX} Cell execution complete. Duration: {CURRENT_TIME_END_CELL9_exec - CURRENT_TIME_START_CELL9_exec:.2f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4E4aJlE6N0N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 10"
      ],
      "metadata": {
        "id": "luShlXUGNRK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 10: Validation Loop ---\n",
        "# Defines and executes the validation process for the multimodal model.\n",
        "# This cell should be run after Cell 9 (Training Loop) has defined all necessary components\n",
        "# and after Cell 4 has potentially populated 'val_loaders'.\n",
        "\n",
        "import torch\n",
        "import time\n",
        "from typing import Dict, Optional, Any, List, Tuple\n",
        "import numpy as np\n",
        "import traceback\n",
        "\n",
        "CELL10_INFO_PREFIX = \"[Cell 10]\"\n",
        "# Ensure config, DEVICE, MainCriterion, SinkhornOTLoss, MixtureEmbedding, PairModel, DataLoader are available\n",
        "DEBUG_CELL10 = getattr(config, 'debug_mode', True) if 'config' in globals() else True\n",
        "\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {CELL10_INFO_PREFIX} Defining Validation Loop logic...\")\n",
        "print(f\"    {CELL10_INFO_PREFIX} Device: {DEVICE}, Debug Mode: {DEBUG_CELL10}\")\n",
        "\n",
        "def run_validation_epoch(\n",
        "    current_epoch_idx: int,\n",
        "    models_for_pairs_val: Dict[str, PairModel], # Pass the models_for_pairs dict\n",
        "    val_loaders_dict: Dict[str, DataLoader],    # Pass the val_loaders dict\n",
        "    main_criterion_val: MainCriterion,          # Pass the instantiated main_criterion\n",
        "    sinkhorn_ot_loss_fn_val: Optional[SinkhornOTLoss], # Pass the instantiated sinkhorn_ot_loss_fn\n",
        "    mixture_embedding_for_raw_val: MixtureEmbedding, # Pass the raw mixture embedding\n",
        "    g_config: ModelConfig, # Pass the global config object\n",
        "    device_val: torch.device # Pass the target device\n",
        ") -> Dict[str, float]: # Returns a dictionary of aggregated validation metrics\n",
        "\n",
        "    func_prefix_val = f\"{CELL10_INFO_PREFIX}[run_validation_epoch:Epoch-{current_epoch_idx+1}]\"\n",
        "    print(f\"INFO: {func_prefix_val} --- Starting Validation ---\")\n",
        "\n",
        "    if not val_loaders_dict:\n",
        "        print(f\"INFO: {func_prefix_val} No validation loaders provided in 'val_loaders_dict'. Skipping validation.\")\n",
        "        return {\"val_loss_total_avg\": float('inf'), \"val_main_loss_avg\": float('inf'), \"val_sb_loss_avg\": float('inf')}\n",
        "\n",
        "    # Set all relevant models to evaluation mode\n",
        "    for model_instance in models_for_pairs_val.values():\n",
        "        if model_instance and isinstance(model_instance, torch.nn.Module): model_instance.eval()\n",
        "    if mixture_embedding_for_raw_val and isinstance(mixture_embedding_for_raw_val, torch.nn.Module): mixture_embedding_for_raw_val.eval()\n",
        "\n",
        "    # If mixture_embedding_for_fused is a separate global module and trained, set it to eval.\n",
        "    # However, it's usually part of each PairModel instance.\n",
        "    if 'mixture_embedding_for_fused' in globals() and isinstance(globals()['mixture_embedding_for_fused'], torch.nn.Module):\n",
        "        globals()['mixture_embedding_for_fused'].eval()\n",
        "\n",
        "\n",
        "    epoch_val_loss_sum_total = 0.0\n",
        "    epoch_val_loss_components_sum = {comp: 0.0 for comp in ['kl', 'ent', 'reg', 'sb_loss', 'main_loss_total']}\n",
        "    total_samples_processed_val = 0\n",
        "\n",
        "    val_pair_data_iters = {name: iter(loader) for name, loader in val_loaders_dict.items() if loader is not None} # Filter out None loaders\n",
        "    if not val_pair_data_iters:\n",
        "        print(f\"INFO: {func_prefix_val} No valid iterators from val_loaders_dict. Skipping validation.\")\n",
        "        return {\"val_loss_total_avg\": float('inf')}\n",
        "\n",
        "    # Determine max_steps for validation\n",
        "    max_val_steps = 0\n",
        "    val_steps_override = getattr(g_config, 'max_val_steps_override', None)\n",
        "    if val_steps_override is not None and val_steps_override > 0:\n",
        "        max_val_steps = val_steps_override\n",
        "    else:\n",
        "        val_map_lengths = [len(ldr) for ldr in val_loaders_dict.values() if ldr and hasattr(ldr.dataset, '__len__') and not isinstance(ldr.dataset, PyTorchIterableDataset)]\n",
        "        if val_map_lengths: max_val_steps = max(val_map_lengths)\n",
        "        elif val_loaders_dict: max_val_steps = getattr(g_config, 'default_steps_for_iterable_val_epoch', 30)\n",
        "\n",
        "    if max_val_steps == 0:\n",
        "        print(f\"INFO: {func_prefix_val} max_val_steps is 0. Validation not run effectively.\");\n",
        "        return {\"val_loss_total_avg\": float('inf')}\n",
        "    print(f\"    {func_prefix_val} Processing up to {max_val_steps} validation super-steps for each validation pair.\")\n",
        "\n",
        "    with torch.no_grad(): # IMPORTANT: Disable gradient calculations during validation\n",
        "        for val_step_idx in range(max_val_steps):\n",
        "            all_val_loaders_exhausted_for_step = True # Assume all are exhausted for this super-step\n",
        "            for pair_name_val in list(val_pair_data_iters.keys()): # Iterate over available val pairs\n",
        "                model_val = models_for_pairs_val.get(pair_name_val)\n",
        "                loader_iter_val = val_pair_data_iters.get(pair_name_val)\n",
        "                if not model_val or not loader_iter_val: continue\n",
        "\n",
        "                try: batch_data_val = next(loader_iter_val)\n",
        "                except StopIteration:\n",
        "                    if DEBUG_CELL10: print(f\"DEBUG: {func_prefix_val} Val loader for '{pair_name_val}' exhausted at super-step {val_step_idx+1}.\")\n",
        "                    val_pair_data_iters.pop(pair_name_val, None)\n",
        "                    continue # Move to next pair if this one is done for this super-step\n",
        "\n",
        "                all_val_loaders_exhausted_for_step = False # At least one loader still has data for this super-step\n",
        "\n",
        "                if batch_data_val is None: continue\n",
        "                try: mod1_cpu_val, mod2_cpu_val = batch_data_val\n",
        "                except ValueError: print(f\"ERROR: {func_prefix_val} Unpack val batch for '{pair_name_val}' failed.\"); continue\n",
        "                if mod1_cpu_val is None or mod2_cpu_val is None: continue\n",
        "\n",
        "                current_bs_val = (mod1_cpu_val.shape[0] if isinstance(mod1_cpu_val, torch.Tensor) else len(mod1_cpu_val) if isinstance(mod1_cpu_val, list) else 0)\n",
        "                if current_bs_val == 0: continue\n",
        "\n",
        "                mod1_dev_val = mod1_cpu_val.to(device_val, non_blocking=True) if isinstance(mod1_cpu_val, torch.Tensor) else mod1_cpu_val\n",
        "                mod2_dev_val = mod2_cpu_val.to(device_val, non_blocking=True) if isinstance(mod2_cpu_val, torch.Tensor) else mod2_cpu_val\n",
        "\n",
        "                current_log_loss_components_val = {k: float('nan') for k in epoch_val_loss_components_sum.keys()}\n",
        "\n",
        "                try:\n",
        "                    _m_fused_val, _g_fused_val, raw_feat1_val, raw_feat2_val = model_val(mod1_dev_val, mod2_dev_val)\n",
        "                    if raw_feat1_val is None or raw_feat2_val is None or raw_feat1_val.shape[0] != current_bs_val or raw_feat2_val.shape[0] != current_bs_val: continue\n",
        "\n",
        "                    measures1_val, gamma1_val = mixture_embedding_for_raw_val(raw_feat1_val)\n",
        "                    measures2_val, gamma2_val = mixture_embedding_for_raw_val(raw_feat2_val)\n",
        "                    if measures1_val is None or measures2_val is None or gamma1_val is None or gamma2_val is None or \\\n",
        "                       gamma1_val.shape[0] != current_bs_val or gamma2_val.shape[0] != current_bs_val: continue\n",
        "\n",
        "                    main_loss_val_total, components_dict_main_val = main_criterion_val(\n",
        "                        measures1=measures1_val, gamma1=gamma1_val,\n",
        "                        measures2=measures2_val, gamma2=gamma2_val,\n",
        "                        current_batch_size=current_bs_val\n",
        "                    )\n",
        "                    current_log_loss_components_val.update(components_dict_main_val)\n",
        "\n",
        "                    sb_loss_val_step = torch.tensor(0.0, device=device_val)\n",
        "                    if sinkhorn_ot_loss_fn_val and g_config.lambda_sb > 0:\n",
        "                        s1_ot_val_cand, s2_ot_val_cand = None, None\n",
        "                        if 'E' in measures1_val and isinstance(measures1_val['E'], tuple) and len(measures1_val['E']) > 2 and isinstance(measures1_val['E'][2], torch.Tensor): s1_ot_val_cand = measures1_val['E'][2]\n",
        "                        if 'E' in measures2_val and isinstance(measures2_val['E'], tuple) and len(measures2_val['E']) > 2 and isinstance(measures2_val['E'][2], torch.Tensor): s2_ot_val_cand = measures2_val['E'][2]\n",
        "\n",
        "                        s1_v_final, s2_v_final = None, None; valid_sink_inputs_v = True # Renamed to avoid conflict\n",
        "                        for i_sv, (s_cand_v, _) in enumerate([(s1_ot_val_cand, \"s1_ot_val\"), (s2_ot_val_cand, \"s2_ot_val\")]):\n",
        "                            if s_cand_v is None or not isinstance(s_cand_v, torch.Tensor) or s_cand_v.numel()==0 or s_cand_v.shape[0]!=current_bs_val or not torch.isfinite(s_cand_v).all(): valid_sink_inputs_v=False; break\n",
        "                            if i_sv == 0: s1_v_final = s_cand_v.contiguous().float()\n",
        "                            else: s2_v_final = s_cand_v.contiguous().float()\n",
        "\n",
        "                        if valid_sink_inputs_v and s1_v_final is not None and s2_v_final is not None and (s1_v_final.ndim == 2 and s2_v_final.ndim == 2 and s1_v_final.shape == s2_v_final.shape):\n",
        "                            try: sb_loss_val_step = sinkhorn_ot_loss_fn_val(s1_v_final, s2_v_final)\n",
        "                            except Exception: sb_loss_val_step = torch.tensor(0.0, device=device_val)\n",
        "                        if not torch.isfinite(sb_loss_val_step): sb_loss_val_step = torch.tensor(0.0, device=device_val)\n",
        "                    current_log_loss_components_val['sb_loss'] = sb_loss_val_step.item() if isinstance(sb_loss_val_step, torch.Tensor) else float(sb_loss_val_step)\n",
        "\n",
        "                    current_total_val_loss_step = main_loss_val_total + g_config.lambda_sb * sb_loss_val_step\n",
        "\n",
        "                    if torch.isfinite(current_total_val_loss_step):\n",
        "                        epoch_val_loss_sum_total += current_total_val_loss_step.item() * current_bs_val\n",
        "                        for k_log_val, v_log_val in current_log_loss_components_val.items():\n",
        "                            if isinstance(v_log_val, (int, float)) and np.isfinite(v_log_val):\n",
        "                                epoch_val_loss_components_sum[k_log_val] = epoch_val_loss_components_sum.get(k_log_val, 0.0) + (v_log_val * current_bs_val)\n",
        "                        total_samples_processed_val += current_bs_val\n",
        "                except Exception as e_val_fwd:\n",
        "                    print(f\"ERROR: {func_prefix_val} FWD/Loss for '{pair_name_val}' during validation: {e_val_fwd}\")\n",
        "                    if DEBUG_CELL10: print(traceback.format_exc()); continue\n",
        "\n",
        "            if all_val_loaders_exhausted_for_step and not val_pair_data_iters: # All val loaders are fully exhausted\n",
        "                if DEBUG_CELL10: print(f\"DEBUG: {func_prefix_val} All validation dataloaders fully exhausted at super-step {val_step_idx+1}. Ending validation early.\")\n",
        "                break\n",
        "\n",
        "    avg_total_val_loss = epoch_val_loss_sum_total / total_samples_processed_val if total_samples_processed_val > 0 else float('inf')\n",
        "\n",
        "    val_metrics_to_log = {\"val_loss_total_avg\": avg_total_val_loss}\n",
        "    if total_samples_processed_val > 0:\n",
        "        for comp_key, comp_sum_val in epoch_val_loss_components_sum.items():\n",
        "            val_metrics_to_log[f\"val_loss_{comp_key}_avg\"] = comp_sum_val / total_samples_processed_val\n",
        "\n",
        "    print(f\"    {func_prefix_val} --- Validation Summary ---\")\n",
        "    for metric_name, metric_val in val_metrics_to_log.items():\n",
        "        print(f\"        {metric_name}: {metric_val:.4f}\")\n",
        "    print(f\"    {func_prefix_val} Processed {total_samples_processed_val} samples across all validation pairs.\")\n",
        "\n",
        "    # Set models back to training mode (important if called mid-training epoch by Cell 9)\n",
        "    for model_instance in models_for_pairs_val.values():\n",
        "        if model_instance and isinstance(model_instance, torch.nn.Module): model_instance.train()\n",
        "    if mixture_embedding_for_raw_val and isinstance(mixture_embedding_for_raw_val, torch.nn.Module): mixture_embedding_for_raw_val.train()\n",
        "    if 'mixture_embedding_for_fused' in globals() and isinstance(globals()['mixture_embedding_for_fused'], torch.nn.Module):\n",
        "        globals()['mixture_embedding_for_fused'].train()\n",
        "\n",
        "    return val_metrics_to_log\n",
        "\n",
        "print(f\"    {CELL10_INFO_PREFIX} run_validation_epoch function defined.\")\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))}] {CELL10_INFO_PREFIX} Validation loop definition complete.\")\n",
        "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] ==> To use validation: \\n\"\n",
        "      f\"    1. Ensure 'val_loaders' dict is populated by Cell 4 (define val splits in Cell 3's 'active_pairs_definitions').\\n\"\n",
        "      f\"    2. Cell 9 will call 'run_validation_epoch' based on 'config.val_check_interval_epochs'.\\n\"\n",
        "      f\"    3. Add 'num_val_test_samples_cap' and 'max_val_steps_override' to ModelConfig (Cell 3) for finer control if needed.\")\n"
      ],
      "metadata": {
        "id": "1v-zJvGsNU0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "l2Hhk28-R3E3",
        "nlxBSKPeRWs5",
        "VrEoxLJhRY4l",
        "uxzVQC1yRQxu",
        "nh4hX6p_Roke",
        "gAuOVoI3RlYv",
        "Yqb3GHayRrIg",
        "2Cd96d7ARskn",
        "vVpjXRaXRt1T",
        "luShlXUGNRK4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
